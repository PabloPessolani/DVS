0130515: Se implemento sys_fork en la systask y se hizo test_sysfork 
20130516: Se implemento sys_vcopy en la systask y se hizo test_sysvcopy1 
20130517: Se implemento sys_times en la systask y se hizo test_sysvcopy2 y test_systimes 
		Se incorporo el campo p_name_ptr al proc struct para que los procesos locales
		apunten al nombre del proceso real dado que cuando haya un EXEC p_name
		cambiaria de nombre.
20130519: Se probo el proxy.c que envió Oscar. 
20130520: Se modificó el proxy.c para get2rmt/put2lcl y funciono OK el send y receive. Hay Otros errores a corregir=> mail Oscar. 
20130521: Se probo modifico priv.h agregando priv_usr.h 
	   Se modifico test_getpriv.c
           Se incluyo do_privctl.c
20130522:Se probo SYS_PRIVCTL con test_sysprivctl
	 Se implemento SYS_GETINFO 
	 Se probo parcialmente SYS_GETINFO:  GET_MACHINE(vm) y GET_PROCINFO
20130523: se incorporo al kernel mol_getdrvsinfo con las constantes maximas para las VMs.
	 se modifico read_drvs_info del molproc.c para que tome los datos del drvs y no de las constantes
	del config. Se probo con test_getdrvsinfo.c
	Se termino test_sysgetinfo.c que incluye GET_KINFO(drvs) GET_PROCTAB y GET_PRIVTAB

20130526: Se incorporo creo kernel_sendrec que permite enviar mensajes al PM para notificar FORK y EXIT.

20130527: Se hizo getwork de PM.
	se incorporo el kill_unbind que se ejecuta al hacer EXIT
	se incorporo el fork_bind que se ejecuta cuando un proceso hace FORK

20130528: En main de PM se cambio codigo lineal por llamada a funciones sys_XXXXX en utility.c
	
20130529: Se trabajo en PM: fork, exit.  AUN no terminados ni probados
	Se trabajo en SYSTASK: setalarm	 AUN no terminados ni probados

20130529: Se modifico todo lo relacionado a SETALARM y CLOCK de la SYSTASK

20130531: se crearon las librerias SYSLIB y TIMERS. Se incorporaro algunas funcione sys_xxxxx a syslib.
	
20130601: Se removieron de pm/utility.c  aquellas funciones sys_xxxx para ponerlas como parte de syslib.

20130602: Se probó hacer un fork de un pseudo DS para que este a su vez hiciera un fork de otro proceso (system("/bin/sh")
	Se cuelga.
	Se probó el proxy q envió Oscar, el send y receive por separado andan. El sendrec() y receive/send da error.

20130603: Se modifico kernel_bind para el binding automatico. Se creo la syscall FREEPROC para obtener del PM el child_nr asi el kernel puede hacer el bind automatico.

20130604: Se hizo MOLPROCBIND en PM y SYSTASK que permite que un proceso del sistema haga bind sobre el kernel por si mismo (solo procesos del sistema), para luego hacercelo al PM y este al SYSTASK y asi quede en sus tablas,de tal modo se registrar el proceso solamente. Se creo el server INIT para hacer las pruebas.

20130607:
20130608: Se trabajo en molproc.c vm_procs_read para que imprima TODOS los proceso de la VM. 
	

20130608: Se modifico la semantica del SENDREC remoto.
	ANTES:
		SENDREC----------------->RECEIVE 
			<--------------- SEND_ACK
			<--------------- SEND
		SEND_ACK --------------->
	AHORA:
		SENDREC----------------->RECEIVE 
			<--------------- SEND
		SEND_ACK --------------->
			
	EL PROXY FUNCIONA PERFECTAMENTE PARA:
		- send
		- sendrec
		- notify

20130611: Hacer se do_getsysinfo para obtener informacion del PM/SYSTASK
	se hizo mollib para facilitar

20130611: funciones GETINFO IS->PM->SYSTASK para obtener info		
		- INFO DRVS: LISTO
		- INFO MACHINE: LISTO
		- INFO PROC KERNEL : LISTO
		- INFO PRIV KERNEL: LISTO
		- INFO PROC PM: LISTO

20130613: Diferenciar si el proxy esta BINDEADO respecto al estado CONNECTED 
		- BINDEADO: Está activo pero no puede transmitir mensajes
		- CONNECTED:listo para transmitir mensajes.
	Se modificaron las IPCs para que contemple el CONNECTED
	En todos los tipos de SEND, se verifico la disponibilidad del SPROXY
	En todos los tipos de RECEIVE donde se especifica emiosr se verifico la disponibilidad del RPROXY
	Se modificaron algunas cosas relacionadas a nodos y proxies en /proc

20130615: Test de la info de los proxies en cuanto a transferencia de mensajes y flag misc para ver si esta CONNECTED.

20130618:  Test de proxies en loops

20130619:   Test de proxies en loop
		SERVER			CLIENT				RESULT
		receive/send		sendrec				OK
		receive/send		send/receive			OK 
		receive/send		notify/receive			OK 
		rcvrqst/reply		sendrec				OK
		
20130619: Modificacion de mol_vcopy (sin probar) y auto-rmtbind y auto-rmtunbind en put2lcl (probado OK)
	Esto hace que no sea necesario hacer el bind y unbind de procesos remotos.

20130620:  supongamos que un proceso remoto envio su 1er mensaje a un proceso local e hizo un AUTO-RMTBIND
	   que sucede si un proceso local ahora le envia un mensaje al proceso remoto pero el proceso remoto 
	   no existe mas. 
	   El put2lcl hace:
	   		if( lcl_ptr->p_usr.p_endpoint != h_ptr->c_dst) {ret = EMOLENDPOINT; break;}
	   por lo que solo retorna pero no informa al proceso local de que ese ENDPOINT no existe mas.
	   Quizas se puede poner una bandera reply = NO | YES, segun se quiera retornar un error al origen.
	   if( lcl_ptr->p_usr.p_endpoint != h_ptr->c_dst) {ret = EMOLENDPOINT; reply= YES ; break;}
       Al final
	   
	   QUIZAS HASTA NO HACE FALTA TRABAJAR CON reply
	   	if (ret) {
			if(reply) {
				switch(cmd) {
					case CMD_SEND_MSG:
					case CMD_REPLY_MSG:
					case CMD_SNDREC_MSG:
					case CMD_NTFY_MSG:
						ack = CMD_SEND_ACK;
						break;
					case CMD_COPYIN_DATA:
						ack = CMD_COPYIN_ACK;
						break;
					case CMD_COPYOUT_RQST:
						ack = CMD_COPYOUT_DATA;
						break;
						break;
					default
						ack = CMD_NONE;
						break;
				}
				if(ack) error_lcl2rmt(h_ptr->c_dst, ack, ret); /* PROBLEMA: Que descriptor de proceso se encola en el proxy SENDER ??*/
																/* El del proceso remoto, porque está libre */
			}
			ERROR_UNLOCK_TASK(task_ptr,ret);
		}
		
20130621: Cuando un proceso envio un mensaje de peticion (SEND, SENDREC, REPLY, etc) y el nodo remoto le retorna un error EMOLDSTDEAD
			El put2lcl, ademas de entregarle el error al proceso emisor del msg original hace un do_unbind() del proceso destino (remoto) no localizado.
			Se reemplazaron todas las constantes por drvs.nr_vms, etc	NR_VMS, NR_NODES, NR_PROCS, NR_TASKS, NR_SYS_PROCS

20130622:   Se provo VCOPY remoto:  requester->client y requester<-client
			
20130623:   Se provo VCOPY remoto: 
					(requester:cliente1)->(client2)
					(requester:cliente1)<-(client2)
					(requester)   (client1->client2)


20130627:   Se reprogramaron los proxies para que tomen el <rmt_nodeid> de la linea de comandos.
		El port destino para el PROXY SENDER => 3000 + lcl_nodeid
		El port local para el PROXY RECEIVER => 3000 + rmt_nodeid
		La IP Address de ambos PROXIES => BASE = "192.168.214."+ (100+lcl_nodeid)
		La info del lcl_nodeid se obtiene del mnx_getdrvsinfo(drvs)


20130628:  Se corrigieron errores en SPROXY 
		NODO0---> CMD_COPYRMT_RQST---->NODE1 : OK
		NODE1 put2lcl OK ====== linea 820
		NODE1 ---> CMD_COPYIN_RQST---->NODE2 : ERROR en get2rmt  => copy_usr2usr  => SE CUELGA

	
20130628: Se probo la copia remota rqtr->source->destination
		(requester)   (client1)->( client2)  requester, client1 y client2 en diferentes nodos
		(requester)->CMD_COPYRMT_RQST->(sender)-CMD_COPYIN_RQST->(receiver)
		requester<-------------CMD_COPYRMT_ACK-------------------(receiver)
        ON ERROR requester<----CMD_COPYRMT_ACK (sender)
			 				
		NODE0)	Requester --> sproxy_enqueue(requester_ptr, CMD_COPYRMT_RQST) ONCOPY(rqtr, src, dst)
		NODE0) SPROXY   ----> CMD_COPYRMT_RQST;
		NODE1)  RPROXY -->put2cll -> CMD_COPYRMT_RQST: 	copyrmt_rqst_rmt2lcl: verifica procesos
				copyrmt_data_lcl2rmt: encola en proxy de sender CMD_COPYIN_RQST al proceso 
												local enviando datos al destino
			si hay error envia mensaje al requester
		NODE1) SPROXY-----> CMD_COPYIN_RQST		
		3)  Destination-> 	CMD_COPYIN_RQST: 	copyin_rqst_rmt2lcl: verifica procesos y copia datos desde el Receiver 
													proxy al proceso destino
										copyrmt_ack_lcl2rmt:  envia un ack desde el destino al requester.

20130630:  Se integraron funciones en proxy server
		Se dividio en mol-sproxy.c y mol-rproxy.c-
	
	
20130703:	Se sumarizaron funciones(copylcl_ack_rmt2lcl, copyin_ack_rmt2lcl, copyrmt_ack_rmt2lcl) en copygen_ack_rmt2lcl
		POWERPOINT: Se realizo diagrama de flujo de proxies

20130703: 	Se incorporaron las versiones (via macros) de IPC con timeout (ms) Ej: mnx_send_T(dest, &mesg, timeout_ms);
		Para ello se modifico sleep_proc(proc, timeout)

20130704:	Se incorporo tratamiento para TIMEOUT_NOWAIT=0
		Se modificaron proxy/tests/loop_server.c y loop_client.c
	
20130706:	Se soluciono COLGADA al hacer DRVS_END y kill a los proxies
		Se hicieron tests sencillos de IPC y se documentaron

20130708:   Se corrigieron errores en proxy.c y se hicieron pruebas de transferencias remotas sencillas hasta 65530

20130708: 	Se incoporó copia de NR_BUF ciclos de MAXBUFSIZE a todas las copias remotas.
			Se probaron transferencias simples client<->server de 262144 y 524288 bytes.

20130710:	PROXY: Se introdujo <rmt_nodename> como  parámetro del proxy para que se resuelva por /etc/hosts.
		Se hizo la prueba de transferencia de REMOTO<->LOCAL donde LOCAL->nodeid == REQUESTER->nodeid
		Se realizaron los Scripts de benchmarks (IPC y VCOPY entre 2 nodos)
		Se puso NONE en lugar de PM_PROC_NR como s_warn (proceso para notificar autobind/unbind), 
		con lo que se puede usar libremente p_nr = 0 = PM_PROC_NR.

		Al finalizar el RPROXY, uno de los procesos bloqueados intervinientes en un VCOPY podría estar esperando
		un ACK de un proceso remoto. Se verifica si el proceso es: LOCAL->ONCOPY->REQUESTER

		Se cambio vcopy() de tal modo que SIEMPRE se encola en el SPROXY al descriptor del REQUESTER, dado que 
		la regla es que en las PETICIONES se encola el proceso LOCAL.

****************************************
HITO: Hasta aqui funcionaba todo bien
Pero podria haber problemas potenciales.
****************************************

20130711:	Se cambiaron mucho los proxies y sus funciones
		Se sumarizaron funciones de ACKs en genericas 
		Como los proxies cada vez q ejecuta get2rmt o put2lcl es equivalente a que hagan un BIND temporal sobre la VM
		incrementan el contador de la VM y al retornarlo decrementan.
		Se creo el archivo mol-acks donde se concentran funciones de ACK tanto LCL-RMT como RMT-LCL
		TEST LOCALES = OK

20130714:	Se cambiaron mucho los SPROXY y RPROXY en cuanto a LOCKS				
		Se ejecutaron Scripts de benchmarks (IPC y VCOPY entre 2 y 3 nodos)
****************************************
HITO: Hasta aqui funcionaba todo bien
Pero podria haber problemas potenciales.
****************************************

20130715:	COPYRMT: El REQUESTER solicita al SENDER un CMD_COPYRMT_RQST 
			Luego el SENDER solicita un CMD_COPYIN_RQST al DESTINATION
			Si algo en el nodo del DESTINATION anda mal, debería responderle al REQUESTER y no al SENDER !!
		Se solucionó en mol-rproxy.c put2lcl() y modifiando error_lcl2rmt.

		Se creo do_autobind() usada por RPROXy 
		Se creo la macro LOCK_ALL_PROCS
		Se modifico do_vm_end()

20130715:
		Se depuro el uso de LOCKs
		Se modifico send_rmt2lcl y mol_receive, mol_rcvrqst, mol_sendrec  de tal modo
		que si el proceso receptor está esperando el mensaje, directamente copiarlo desde el espacio de 
		usuario del sender hacia el espacio de usuario del receiver. Esto se extendio al rproxy para
		que se copie desde el espacio de usuario del rproxy (payload_ptr) al espacio del receptor mptr.
		
20130715:	Se uso LOCKING de NODOS en algunas funciones.
		Se seteo SOREUSEADDR en el socket del RPROXY para que permita reiniciar rapidamente el socket despues 
		finalizada la conexion.
		Se cambio el esquema de DEBUG, seteando en el DRVS una variable que controla el nivel de debugging.
	
	
20130726	se Modifico GETEP para que solo lo puedan usar aplicaciones registradas de tal modo que sean de la misma VM.
		Se creo la funcion check_lock_caller para reducir codigo 
		se ordendo codigo de ingreso a ipc y proxies en control de errores
		se agrego version y subversion a la estructura del drvs

20130727: 	Se modifico SYSTASK, PM , INIT para arrancar procesos
		La selección de ENDPOINT LIBRE quedo a cargo de SYSTASK en lugar de PM
		SYSTASK: Se cuentan los los slotfree para disparar el algoritmo de DONACION DE RANURAS


20130728: 	PROBLEMA: que pasa si un proceso LOCAL muere, hace el unbind en el kernel pero no hace el unbind en PM/FS/SYSTASK.
			Como notificarlos?
		SOLUCION: En lugar de dar una solucion a nivel de kernel, darla a nivel de usuario.
			El padre de un proceso hace 
			while((child_pid = wait(&child_sts)) >= 0)
				PM_EXIT(child_pid)  /* envia un mensaje al PM diciendo que el child pid EXIT */
		PROBLEMA de la SOLUCION: Cuando el padre muere y no hizo wait() los children pasan a INIT
		SOLUCION:  Incorporar a INIT de LINUX la finalizacion de los procesos guachos.

20130730:   Se creo la imagen de booteo y el DVD para cluster de UTN
			Se realizaron los benchmarks nuevamente -TODO OK
		
20130731:	Se comenzo con SPREAD en SYSTASK
			Se instalo /usr/src/spread_4.3.0
			./configure
			make 
			make install
			cd libspread-util
			make install
			
		
20130802:	Se implemento la comunicacion con SPREAD entre systask de diferentes nodos
			Cuando el primer nodo inicia es el owner de todas las ranuras
			Cuando otro nodo inicia hace un broadcast solicitando ranuras
			Cuando un nodo muere , el primer nodo se queda con sus ranuras.
			El fork controla si le quedan pocas ranuras libres. Si quedan pocas solicita donacion
			Si los donadores tienen le otorgan ranuras.
	
20130803:	Se implemento el rmtbind via SPREAD entre SYSTASKs	de tal modo que cuando PM se registra en NODO0
			PM->SYSTASK0--(MOLBIND)->SYSTASK1->KERNEL1
			El proceso RINIT del NODO1 se registra al PM de NODO0 y a la SYSTASK del NODO1
			
...............................................................................................
REGLA:		Las SYSTASKs se registran en (SYSTEM + local_nodeid)
			Las Tareas y Servidores se registran ante su SYSTASK Local y la SYSTASK lo difunde por SPREAD
			Los Procesos de usuario se registran ante el PM (Local o Remoto), la SYSTASK no difunde por SPREAD 
			El PM registrará el proceso de usuario (Local o Remoto) ante su SYSTASK
			El PM registrará el proceso de usuario (Local o Remoto) ante el FS (Local o Remoto)
...............................................................................................
					
	
20130810:   Como la systask está compuesta por 2 threads y se hace un BIND del THREAD SYSTASK y del SPREAD 
			por lo que ahora la SYSTASK ocupa 2 slots.
			Por otro lado, para que las SYSTASK puedan conformar un GRUPO comunicado por SPREAD 
			deben tener su propio endpoint. El p_nr de cada systask se calcula de la siguiente manera:
				sys_nr = SYSTEM - (2*local_nodeid) 		para el thread principal SYSTASK
				sp_nr  = SYSTEM - (2*local_nodeid) - 1	para el thread secundario SPREAD
			CUESTION: Porque tener varias SYSTASKs??
				- Para tener rendimiento ideal en operaciones LOCALES
				- Para reducir el tráfico de red
			Para ello se debieron cambiar la libreria syslib, particularmente taskcall()

			Al incrementarse el número de SYSTASK/SPREAD por VM, debieron cambiarse las constantes máximas
			El numero de systask = 2*nr_nodes.
			
			#define NR_NODES 	32
			#define NR_SYSTASKS	(2*NR_NODES)		/* 2 Threads por SYSTASK (64) */
			#define NR_TASKS	(3+NR_SYSTASKS)		/* HARDWARE, CLOCK, IDLE + 32xSYSTASKS (64+3) */
			#define NR_SERVERS	(32-3)			/* (32-3) = 29*/
			#define NR_SYS_PROCS	(NR_SERVERS+NR_TASKS)   /* 64+3+32-3 = 64+32 = 96 */
			#define NR_USR_PROCS    (256-NR_SYS_PROCS)	/* (256-96) = 160 */
			#define NR_PROCS	(NR_SERVERS+NR_USR_PROCS) /* 29 +  160 =  189 => (NR_PROCS+NR_TASKS) = (189+ 64+3) = 256 !!!! */

			/* Kernel tasks. These all run in the same address space. */
			#define KERNEL           -1			/* pseudo-process for IPC and scheduling */
			#define HARDWARE     KERNEL			/* for hardware interrupt handlers */
			#define SYSTEM           -2			/* request system functionality */
			#define CLOCK  		 (SYSTEM-NR_SYSTASKS)	/* process number until NR_NODES System tasks */
			#define IDLE             (CLOCK-1)		/* runs when no one else can run */

			/* User-space processes, that is, device drivers, servers, and INIT. */
			#define PM_PROC_NR	  0			/* process manager */
			#define FS_PROC_NR 	  (PM_PROC_NR+1)	/* file system */
			#define RS_PROC_NR 	  (FS_PROC_NR+1)	/* reincarnation server */
			#define MEM_PROC_NR 	  (RS_PROC_NR+1)	/* memory driver (RAM disk, null, etc.) */
			#define LOG_PROC_NR	  (MEM_PROC_NR+1)	/* log device driver */
			#define WEB_PROC_NR	  (LOG_PROC_NR+1)	/* terminal (TTY) driver */
			#define DS_PROC_NR	  (WEB_PROC_NR+1)	/* data store server */
			#define IS_PROC_NR	  (DS_PROC_NR+1) 	/* Information Server*/
			#define RINIT_PROC_NR	  (IS_PROC_NR+1)    	
			#define INIT_PROC_NR	  (RINIT_PROC_NR+1)  	/* init -- goes multiuser */

			#define FIRST_USER_NR	   (NR_SYS_PROCS)

20130811:	Supongamos:
			NODO0:  SYSTASK0, PM, INIT
			NODO1:  SYSTASK1, RINIT, USER	
			RINIT1 hace un BIND a KERNEL1  (LISTO)
			RINIT1 hace un BIND a SYSTASK1 (LISTO)
			SYSTASK1 le informa por multicast a SYSTASK0 de la existencia de RINIT1 (LISTO)
			RINIT1 hace FORK al SYSTASK1 para registrar a USER1.(LISTO)
			RINIT1 hace FORK al PM0 para registrar a USER1.
			El PM0 verifica si el padre es LOCAL o REMOTO
				Si es LOCAL hace SYS_FORK
				Si es REMOTO hace SYS_RFORK -> SYSTASK0 (=>rmtbind)
			PM0 le envía un mensaje que desbloquea a USER1.
			Cuando un server hace MOLEXIT hay que hacer el broadcast via SPREAD (LISTO).

20130812:	Funciona el EXIT remoto 
			Ejemplo: el RINIT1 hace EXIT al PM0. 


20130813:	Cada vez que se suma un nuevo nodo:
			1) La SYSTASK y SPREAD del nuevo nodo debe reflejarse en los nodos activos.
				* SYSTASK nuevo	Multicast(MOLBINDPROC) 	
				* Todos los nodos reciben el MOLBINDPROC del resto de los nodos y hacen rmtbind
			2) Los procesos existentes en los nodos activos deben reflejarse en el nuevo nodo.
				* SYSTASK nuevo	Multicast(SYS_GET_PROCS)
				* Todos los otros nodos hacen Multicast(MOLBINDPROC) de todos los procesos del sistema de SU nodo.
				* Todos los nodos reciben el MOLBINDPROC del resto de los nodos y hacen rmtbind 
		Se incorporo semaforo sys_mutex para proteger region critica entre SYSTASK y SPREAD
			

ATENCION:	Que hacer cuando un server muere por un KILL o FAULT ?? el unico que está enterado es el kernel.
			SOLUCION: Controlar :
				EMOLSRCDIED
				EMOLDSTDIED
				EMOLNOPROXY
				EMOLNOTCONN
				EMOLDEADSRCDST
			PM: Si ocurre algun error de este tipo, hacer el UNBIND al PM y SYSTASK.
			SYSTASK: Si el proceso es LOCAL y del sistema, hacer BROADCAST UNBIND
			Si el PM le solicita algo a SYSTASK acerca de un proceso MUERTO, la SYSTASK le reporta alguno de los errores al PM.
			Reemplazar la salida de errores:
		  	if((rcode =sys_procinfo(who_p)) != OK) 
				ERROR_EXIT(rcode);
			
			#define ERROR_EXIT(endpoint, rcode) \
 do { \
     	printf("ERROR: %s:%u:%d: rcode=%d\n",__FUNCTION__ ,__LINE__,endpoint, rcode); \
	fflush(stderr);\
	error_exit(endpoint, rcode); \ 
	exit(rcode); \
 }while(0);
 
error_exit(int endpoint, int rcode) 
{
	switch(rcode){
			case	EMOLSRCDIED:
			case	EMOLDSTDIED:
			case	EMOLNOPROXY:
			case	EMOLNOTCONN:
			case	EMOLDEADSRCDST:
				/* PM UNBIND */
				/* SYSTASK UNBIND */
				break;
			default:
				break;
	}
}			


20130814: 		Se controla en drvs_init() y vm_init() la relación entre los parametros entre si:
			Parametros BASICOS: NR_NODES, NR_TASKS, NR_SYS_PROCS, NR_PROCS
			Parametros DERIVADOS:
			NR_SYSTASKS = (2*NR_NODES) <= (NR_TASKS-3)
			NR_SERVERS  = (NR_SYS_PROCS-NR_TASKS)
			NR_USR_PROCS= (NR_PROCS+NR_TASKS)-(NR_SYS_PROCS)

			Se creo en mollib la llamada mol_bindproc() y se cambiaron init/rinit/is para que la usen.
			Se unificaron INIT y RINIT. 
			ATENCION: Tiene que haber tantos INIT como NODOS, por lo tanto cada init tendra p_nr = (INIT_PROC_NR+localnodeid)
			El INIT distingue si es local o remoto segun si el PM está en local_nodeid.
			Se realizo USER1 que haga getpid() en el PM0 (TODO OK!)

REGLA:		Todo proceso DE SISTEMA LOCAL, SYSTASK lo difunde por SPREAD para que los otros nodos hagan BINDPROC
			Para todo proceso de usuario, su padre le hace sys_fork a la SYSTASK quien hace BIND al KERNEL
			Los procesos de sistemas son conocidos en todos los nodos. Tanto BINDPROC como EXIT.
			Los procesos de usuario son solo conocidos en el nodo donde residen y en el nodo donde está el PM y FS.
			Los procesos de usuario solo usan slots propiedad del nodo donde residen.
			Para replicas de procesos del sistema (caso SYSTASK, INIT), establecen slots segun el local_nodeid.
					
PARA ARRANQUE REMOTO:	Todo proceso de usuario USER1 se comunicará tanto con el FS y PM.
			Supongamos FS0 y PM0
			Dado USER1 levanta a través de INIT1 (mol_fork), automaticamente USER1 se registra en:
				 * SYSTASK1->KERNEL1
				 * PM0->FS0->SYSTASK0->KERNEL0
			En el caso que PM0, FS1 y USER2
			Dado USER2 levanta a través de INIT2 (mol_fork), automaticamente USER2 se registra en:
				 * SYSTASK2->KERNEL2
				 * PM0->SYSTASK0->KERNEL0
				 * PM0->FS1->SYSTASK1->KERNEL1			

ORDER TO LOCK:
		1- DRVS
		2- VM
	   2A- LINUX TASK
		3- PROCS
			-BY PROCESS NUMBER
		4- PROXIES
		5- RPROXY
		6- SPROXY
		7- NODES


20130816:		Se creo wait4bind(); Es para que el CHILD espere que su padre haga BIND durante el fork().


20130819:		Se implemento mol_exit() 


20130820:		Se implemento mol_time() , mol_times(), mol_stime()


20130827:		getpnbr y derivadas


20130827:		sE incorporó el flag BIT_NOTIFY en p_misc_flags que señala si el receptor está esperando un mensaje
			(receive, rcvrqst) que copie desde su buffer de kernel a su buffer de usuario el mensaje creado 
			por el notify antes de retornar.
			Se cambió receive, rcvrqst, notify, y rproxy (notify remoto).

20130902:		Se implementaron correctamente las alarmas en SYSTASK
			El signal handler del CLOCK se dispara cada 1 segundo, pero cuando la proxima alarma tiene un vencimiento < 1seg
			se progrograma el interval timer en ONE SHOT con un tiempo de la diferencia entre (next_timeout-realtime)
			luego prosigue con 1 Segundo o con lo que reste de la proxima alarma.
			

20130903:		Se termino con la gestion de timers del PM.
			Se implemento sys_memset para setear memoria.

20130911:		Se implementó el comando mol_migrate que soporta la migracion de procesos.
			Para ello se crea una nueva cola en los descriptores de procesos en la que se encolan los 
			procesos emisores durante el tiempo en el que el proceso migrante está en estado MIGRATE
			Finalizada Exitosamente o Fallidamente la migración, los procesos son despertados y 
			se ejecutan las primitivas de forma normal, seleccionandose el nodo destino correcto.
			Aquellos descriptores de procesos YA encolados en las colas de envios de proceso migrante o de
			los proxies salientes se retorna error EMOLMIGRATE dentro de la propia primitiva lo que hace que
			se reejecute la primitiva, esta vez encaminando el mensaje al nodo correcto a donde termino de
			instalarse el proceso migrante.

		TESTS DE MIGRACION:
			TEST 1: Proceso local que intenta migrar y falla
			* Arrancar rtest0.sh y rtest1.sh
			* nodo0:  test/test_receive 0 10 &
			* nodo0:  test/test_migrate 0 7 0 <pid> 10 1
			* nodo0:  ver el estado /proc/drvs/VMO/procs (bandera BIT_MIGRATE)
			* nodo0:  test/test_send 0 9 10 &
			* nodo0:  ver el estado /proc/drvs/VMO/procs (bandera BIT_MIGRATE)
			* nodo0:  test/test_migrate 0 7 2 <pid> 10 1 
			=> Deberia continuar el SEND ====================> OK!!!

			TEST 2: Proceso remoto que intenta migrar y falla 
			* Arrancar rtest0.sh y rtest1.sh
			* nodo0:  test/test_rmtbind 0 10 1 
			* nodo0:  test/test_migrate 0 7 0 0 10 0
			* nodo0:  ver el estado /proc/drvs/VMO/procs (bandera BIT_MIGRATE)
			* nodo0:  test/test_send 0 9 10 &
			* nodo0:  ver el estado /proc/drvs/VMO/procs (bandera BIT_MIGRATE)

			* nodo1:  test/test_rmtbind 0 9 0
			* nodo1:  test/test_receive 0 10 &
			 
			* nodo0:  test/test_migrate 0 7 2 <pid> 10 1 
			=> Deberia continuar el SEND ====================> OK!!!

			TEST 3: Proceso local que intenta migrar y es exitoso
			* Arrancar rtest0.sh y rtest1.sh
			* nodo0:  test/test_receive 0 10 &
			* nodo0:  test/test_migrate 0 7 0 <pid> 10 1
			* nodo0:  ver el estado /proc/drvs/VMO/procs (bandera BIT_MIGRATE)
			* nodo0:  test/test_send 0 9 10 &
			* nodo0:  ver el estado /proc/drvs/VMO/procs (bandera BIT_MIGRATE)

			* nodo1:  test/test_rmtbind 0 9 0
			* nodo1:  test/test_receive 0 10 &

			* nodo0:  test/test_migrate 0 7 1 <pid> 10 1 
			=> Deberia continuar el SEND ====================> OK!!!


20130912:		Se implemento el concepto de proceso BACKUP. Es decir,ahora se pueden tener procesos:
				- Locales
				- Remotos
				- Backup:  El descriptor tiene marca REMOTE, pero tambien tiene marca MIS_RMTBACKUP		
					Esto permite que a atraves del mol_migrate, un proceso BACKUP se convierta
					en un proceso LOCAL en el nodo donde reside, sea porque es un proceso migrado
					o por que realmente es un proceso backup que está PASIVO hasta activarse
					cuando muere el ACTIVO REMOTO de otro nodo.
		
20130915:		Se modifico do_unbind() para que contemple la muerte de los procesos BACKUP.
				La muerte de un proceso BACKUP debe transformar el descriptor del proceso en REMOTE.
				Mientras el proceso BACKUP está vivo su comportamiento debe ser identico a REMOTE.
				Despues de ejecutar migrate() que convierte al BACKUP en PRIMARIO, el proceso 
				BACKUP se transforma en LOCAL.

20130915:		Se incorporo el campo p_proxy que cuando el bit BIT_RMTOPER está seteado cuando se trata de un proceso local
			indica el ID del nodo del proxy sender en el que está encolado el descriptor tratando de hacer una operacion 
			remota.
			De esta forma, cuando se hace el unbind() es mas sencillo detectar si el proceso está encolado o no en 
			estado SENDING, RECEIVING (local) o BIT_RMTOPER (remoto) encolado y poder desencolarlo para terminar.


20130918:		Se agrego el campo p_nodemap que es un BIT MAP de los nodos donde hay réplicas del proceso.
			p_nodeid es donde se está ejecutando la réplica PRIMARIA.
			En el caso de los PROXIES el campo p_nodemap indicará a que nodos representa el proxy.

20130919:		Se implemento mol_node_up y mol_node_down que conectan al par de proxies que lo representa.


20131001:		Se hicieron los benchmarks remotos en LAB5

20131002:		Se hicieron los PROXIES UDP
			Se optimizaron los tests remotos y locales reduciendo el valor de los sleep()
			Se instalo IPERF para pruebas de bandwidth

20131003:		Se optimizaron los PROXIES UDP para que envíe el header+payload en un solo sendto()
			Se cambio test_drvs_init para que se puedan ingresar parametros desde modo usuario.
			Se cambio MAXCOPYBUF(32768) MAXCOPYLEN (En UDP el máximo datagrama es 65400)

20131004: 	Se eliminaron errores de PROXIES

20131005:	Se realizaron los siguientes tests en LAB5.			
				Realizar los tests REMOTOS con TCP 1 CPU
				Realizar los tests LOCALES con 4 CPU

				Realizar los tests REMOTOS con UDP 1 CPU
				Realizar los tests REMOTOS con UDP 4 CPU

				Realizar iperf REMOTO con TCP 1 CPU
				Realizar iperf REMOTO con TCP 4 CPU
			HAY PROBLEMAS DE PERFORMANCE EN TCP !!!!!!!
				Esto se debe a que en la version modificada de TCP;
					* se solicita memoria alineada para el header y el payload juntos
					* se envian header y payload en el mismo send() de TCP

20131005:	SOLUCIONADO:
				* se solicita memoria alineada por separado para el header y el payload 
				* se envian header y payload en diferentes send() de TCP

20131005:	Se 	genero directorio proxies
				con archivo info
				y archivos procs
			Se hizo primitiva getproxyinfo

20131008:	Probar getproxinfo
		Se incorporaron LOCKS  para proxies.
		pruebas de PROXY TCP 

20131009:       Se creo imagen DVD con RWLOCKS 
		Se corrigieron errores de LOCKs de proxies, nodos y procs
		Se modifico PROXY SENDER
		Se distinguen los PROCESOS ENCOLADOS EN PROXY SENDER
			p_proxy: nro de proxy
			p_rts_flags: BIT_RMTOPER
		Por eso en el do_proxy_unbind se pueden detectar mas rápido.

20131012:	Se incorporo a la VM y PROCS el campo cpumask
		Cuando un proceso hace un bind, su cpumask se setea en la cpumask de la VM
		Los proxies cuando se bindean tienen todas las CPUs a su disposición.
		Cuando un proceso hace un UP de otro (incluso un proxy) le setea 
		el cpuid propio como el unico CPU de la mascara de afinidad del proceso dormido.
		Cuando el proceso dormido se despierta vuelve a setear su afinidad al valor de su p_cpumask original.
		Esto se hace para para reducir ping-pong cache.
		Cada proceso debería bindearse con su cpu_affinity = p_nr/max_cpu_in_system		


20131016:	Atencion se anularon los LOCK_TASKS de todas las funciones diferentes de bind() y unbind()
	
20131017: 	MIGRACION: Que hacer con un proceso con THREADS ?????
		Cuando desencola de la lista SENDING del proceso migrante debe:
			-retornar EMOLMIGRATE  a los procesos LOCALES
			- retornar EMOLMIGRATE a los procesos REMOTOS - VERIFICAR QUE ESTA ENVIANDO
		MIGRACION: Que pasa cuando el proceso estaba RUNNING, con NEEDMIGR seteado pero MUERE?
		

		TRATAMIENTO DE HILOS:
			- SOLO sobre el HILO PRINCIPAL se puede inicial la migracion (MIGR_START)
			- Cuando se hace MIGR_START en el nodo LOCAL ORIGEN, este debe SETEAR a TODOS los hilos del PROCESO
				con el BIT_NEEDMIGR o BIT_MIGRATE.
			- Cuando se hace MIGR_ROLLBACK en el nodo LOCAL ORIGEN, este debe RESETEAR a TODOS los hilos del PROCESO
				el BIT_MIGRATE.
			- Cuando se hace MIGR_COMMIT en el nodo LOCAL ORIGEN se hace solo sobre el HILO PRINCIPAL.
				Luego se cambia el estado de LOCAL a REMOTO de cada hilo BINDEADO 
				ATENCION: Los hilos NO BINDEADOS se ignoran.
			- Cuando se hace MIGR_COMMIT en el nodo LOCAL DESTINO, solo se hace sobre el proceso cuyo PID se indica
				en el comando.  Es decir, debe haber un comando mol_migrate(MIGR_COMMIT) por cada HILO.

20131021:	Se debuggearon los LOCKS. 	
			Cuando se probo loop_mol_mperf.sh hubo una colgada pero no se a que atribuirla.
			
20131224:	 Se intento instalar el paquete rpcbind pero fallo.
			Se tomo la maquina virtual MOL-IPC2 y se actualizo
				1- se genero nuevo archivo de sources para squeeze
				2- se hizo apt-get update, upt-get upgrade
				3- luego se genero nuevo archivo de sources para wheeze
				4- se hizo apt-get update, apt-get dist-upgrade
				5- se instalo correctamente rpcbind
			Se uso el programa de trasfernecia de RPC que hizo OSCAR 
		
20140322:	Se modifico system.c y spread.c con el nuevo algoritmo basado en Finite State Machine (FSM)

20140323:	Estas variables son constantes durante la ejecucion
 				total_slots = (vm_ptr->vm_nr_tasks + vm_ptr->vm_nr_procs) - vm_ptr->vm_nr_sysprocs;
				min_slots	= (total_slots/vm_ptr->vm_nr_nodes);
				free_slots_low  = min_slots/2 
			
			Estas son las variables globales que hay que proteger y que cambian cuando hay JOIN/DISCONNECT (active_nodes)
				active_nodes
				max_slots	= (total_slots - (min_slots*active_nodes));	
			Estas son las variables globales que hay que proteger	
				free_slots
				owned_slots

				
20140324:	Para probar en NODE0
			/usr/local/sbin/spread &			/* Arranca spread*/
			cd /home/jara/mol-ipc/proxy/	
			./tproxy node1  1 &					/* Arranca el proxy TCP */
			cd /home/jara/mol-ipc/tests
			./test_vm_init						/* Arranca la VM */
			./test_add_node 0 1					/* Asocia el nodo(proxy) a la VM */
			cd /home/jara/mol-ipc/tasks
			./systask							/* Arranca la Systask */
			
Cuando se arranca SOLO NODO0
			
20140406:				local_host			first_mbr			other
			JOIN		STS_WAIT_VM_INFO	SYS_PUT_VMINFO		ignore
						STS_WAIT_TABLE		SYS_PUT_TABLE		ignore
						SYS_REQ_SLOTS

20140414: Se modifico spread, do_fork, system para el algoritmo de SLOTS.
	
20140419:	Hay problemas en la VM NODE0 pero no en la VM NODE1 
			Comparar fuentes de kernels de ambos para ver que fue lo que se cambio.
			Da error/dump en el copy_usr2usr. 
	
20140503:	Se modificaron spread, systask y do_fork para ejecucion sincronizada de nodo0 y nodo1.
			Quedan algunos errores por solucionar
			Se modifico minix.sh
			Se  anulo de la systask la inicializacion de la VM porque reiniciaba los nodos que ya habian
			sido dados de alta por el script.
			

20140504:	Se implemento tipcproxy.
			Se probaron las transferencias entre nodos 0 y 1
			funciono todo OK excepto NOTIFY (IPC3) que se bloqueo, pero parece ser un problema interno de MOL o del test.
		
20140524:	SE MODIFICO SPREAD, SYSTEM Y DO_FORK
			Se trata que SPREAD no hace ninguna peticion de SLOTS solo las hacen
				- SYSTASK: Al momento del JOIN
				- FORK: Cuando necesita
			Los procesos inicializados estan registrados en el bitmap bm_init
			Los procesos de los que se esperan donaciones estan en el bitmap bm_donors
			
20140525:	SE MODIFICO SPREAD, SYSTEM
			Ahora para los miembros despues del primero que se agregan, una vez que terminaro de transferirle el estado
			ya quedan en STS_RUNNING, y el fork forzará la peticion de ranuras, lo hará a demanda.
			Los bind de las SYSTASK remotas ahora las hace bind_rmt_systass()
			EL problema se presenta con SYSTASK que debe esperar que un numero de donors le donen ranuras
			Ahora bien, alguno de esos donors puede morir, por lo tanto no se sabe si ese donor muerto YA DONO o NO DONO ranuras aun
			por lo que hay que llevar el estado y eso se hace con bm_donors.

20140531:	SE MODIFICO SPREAD
			Cambios de errores menores
			se cambio mol_mhyper.c unbind() porque faltaba un desbloqueo.
			Se cambio do_fork de tal modo que si owned_slots = 0 (es decir, arranco) solicita TANTOS SLOTS como le corresponden!!!	no solo 1.

			 

20140609:		SE MODIFICO SPREAD		 
			
PROBLEMA:	Cuando se hace un HALT de un miembro SPREAD lo reporta como NETWORK change !!!
 spread_loop:697:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 curr_first=0 service_type=8192
 get_nodeid:24:member=VM0 nodeid=0
 spread_loop:697:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 curr_first=0 service_type=6144
 get_nodeid:24:member=#1 nodeid=1
 spread_loop:772:Received REGULAR membership for group VM0 with 1 members, where I am member 0:
 spread_loop:808:Due to NETWORK change with 1 VS sets
 spread_loop:818:LOCAL VS set 0 has 1 members:
 spread_loop:827:	#1.0#node1
SOLUCION:
			Se dio tratamiento a NETWORK CHANGE haciendo que se recalcule toda la configuracion de miembros
			y se de por caidos aquellos que no integran la nueva configuracion/vista.
			 
PROBLEMA:
			en SYSTASK do_exit se supone que hace mol_unbind() quien deberia limpiar el descriptor del kernel pero no lo hace!!
SYSTASK:
 do_exit:30:proc_ep=35767
 do_exit:34:before nr=165 endp=35767 vmid=0 flags=0 misc=20 lpid=2414 nodeid=0 nodemap=1 name=init 
 tmrs_clrtimer:17:prev_time=0
 do_exit:52:proc_nr=165 free_slots=17
 do_exit:68:UNBIND proc_ep=35767
 do_exit:76:after nr=165 endp=35767 vmid=0 flags=8 misc=20 lpid=2414 nodeid=0 nodemap=1 name=init 			

KERNEL:

[  284.668667] DEBUG 2319:mol_unbind:1251: vmid=0 proc_ep=35767
[  284.668671] DEBUG 2319:mol_unbind:1260: RLOCK_VM vm=0 count=0
[  284.668674] DEBUG 2319:mol_unbind:1270: RLOCK_PROC ep=35767 count=0
[  284.668677] DEBUG 2319:mol_unbind:1271: RUNLOCK_VM vm=0 count=0
[  284.668680] DEBUG 2319:mol_unbind:1294: RUNLOCK_PROC ep=35767 count=0
[  284.668683] DEBUG 2319:mol_unbind:1301: RLOCK_VM vm=0 count=0
[  284.668685] DEBUG 2319:mol_unbind:1302: WLOCK_PROC ep=35767 count=0
[  284.668689] DEBUG 2319:do_unbind:1353: vmid=0 endpoint=35767 lpid=2414 nodeid=0 flags=8 pseudosem=-1
[  284.668693] DEBUG 2319:do_unbind:1378: Caller endpoint=-2 lpid=2319
[  284.668696] DEBUG 2319:do_unbind:1389: Sending SIGPIPE to pid=2414
[  284.668706] DEBUG 2319:mol_unbind:1304: WUNLOCK_PROC ep=35767 count=0
[  284.668709] DEBUG 2319:mol_unbind:1305: RUNLOCK_VM vm=0 count=0
 
PROBLEMA!! ESTO SUCEDE porque el mol_unbind ejecutado por un 3er proceso (PM) que trata de matar a uno de los 
hijos de INIT, le envia un SIGPIPE, pero mientras esto termina el do_exit entrega el valor ANTERIOR a kill.

20140614:
SOLUCION: Se modifico do_unbind para que acuando se hace un unbind() de un 3er proceso enviandole un SIGPIPE, entonces 
se espera en un event_queue por que se de la condicion proc_ptr->p_rts_flags == SLOT_FREE

	if( caller_ptr != proc_ptr){
		if( IT_IS_LOCAL(proc_ptr)) {
			MOLDEBUG(INTERNAL,"Sending SIGPIPE to pid=%d\n", proc_ptr->p_usr.p_lpid);
			rcode = send_sig_info(SIGPIPE, SEND_SIG_NOINFO, proc_ptr->p_task);
			if(rcode) ERROR_RETURN(rcode);
			/* Waits until the target process unbinds */
			old_pid = proc_ptr->p_usr.p_lpid;
			WUNLOCK_PROC(proc_ptr);
			ret = wait_event_interruptible(caller_ptr->p_wqhead, (proc_ptr->p_usr.p_lpid!=old_pid));
			WLOCK_PROC(proc_ptr);
			return(OK);
		}
	}

	Se modifico do_proxies_unbind() y do_vm_end() haciendo mas o menos lo mismo

20140615:	Se modifico exit_unbind(void)
			Se modifico do_vm_end()
			Se modifico do_proxies_unbind()
	
20140616: 	Se modifico init() con doble fork() para que el INIT de LINUX se haga cargo
			de los procesos ZOMBIES. 

	
MEJORA:		Hacer un GETSLOTS para que muestre la tabla de slots el IS.
	
ERROR :		
name=$noname endp=71237 mp_pid=547 mp_parent=8 mp_flags=5 mp_nice=-10
name=$noname endp=249248 mp_pid=548 mp_parent=8 mp_flags=5 mp_nice=-10
name=$noname endp=71239 mp_pid=549 mp_parent=8 mp_flags=5 mp_nice=-10
name=$noname endp=320454 mp_pid=550 mp_parent=8 mp_flags=5 mp_nice=-10
name=$noname endp=71241 mp_pid=551 mp_parent=8 mp_flags=5 mp_nice=-10
name=$noname endp=284854 mp_pid=552 mp_parent=8 mp_flags=5 mp_nice=-10

#define IN_USE          0x0001	/* set when 'mproc' slot In use */
#define ZOMBIE          0x0004	/* set by EXIT, cleared by WAIT *

Esta indicando que al proceso FIRST PARENT nadie le hizo WAIT!!

20140617:	Se soluciono cambiando init() con simple fork()
			Se agrego GET_SLOTINFO para que el IS imprima
			MEJORA: El PUT_VMINFO y PUT_TABLE pueden hacerse con FIFO.
			SLOTS FUNCION OK !!! 

20140630:	Se modifico mol-hyper.c el mol_bind() para que considere que un thread 
			solo puede hacer un bind si su THREAD LEADER esta bindeado en la misma VM!!!
				
20140816:	Manejo de NETWORK PARTITION Y MERGE.

			Cuando se produce una partición, cada partición trabaja en forma independiente 
			con los slots de los miembros activos de la partición  y mantiene a los slots de 
			los miembros de las otras particiones como estaban.
	
			Definimos la particion primaria aquella que tiene el first_init_mbr del grupo
			Despues de un PARTITION cada particion tiene su propio first_init_mbr
			Cuando se hace un MERGE el first_act_mbr de la particion NO PRIMARIA sabe que el no lo es el first member de todo el conjunto.
			por lo tanto envia un broadcast SYS_MERGE_PST para que lo reciba el first_init_mbr de la particion primaria y de todo el conjunto.
			luego cuando el first_init_mbr hace un SYS_NEW_PST todos se enteran que el es el first_init_mbr
			y a traves de la PST obtiene cuales son los miembros inicializados (aquellos dueños de ranuras).

PROBLEMA: 	Podria haber algun miembro inicializado que no tenga ranuras porque recien habia ingresado antes de la PARTITION !!				

SOLUCION:	Se podria hacer lo siguiente: En lugar de enviar un SYS_INITIALIZED a todos los otros nodos
			basta conque un miembro solicite ranuras para considerarlo INICIALIZADO.
	

PROBLEMA:	Supongamos que hay una particion y luego un MERGE 

		El first_init_mbr de la particion NO PRIMARIA broadcastea un SYS_MERGE_PST
		1) Mientras tanto los miembros de todas las particiones pueden seguir operando!!
		2) Que pasa si el first_init_mbr de la particion PRIMARIA muere antes de enviar SYS_NEW_PST ?

SOLUCION1:	El miembro de la particion primaria SABE que es el first_init_mbr porque el first_init_mbr no cambio
		los miembros de la particion primaria tambien los saben y asumen su roll.

SOLUCION2:	Cuando se produce un MERGE todos los procesos cambian de estado a SYS_MERGING
		Cuando se produce un MERGE el first_init_mbr de la particion primaria hace un SYS_MERGE_PST
		y todos la hacen!!! 
		Los proceso inicializados cambian su estado a SYS_RUNNING
		Los proceso no inicializados cambian su estado a JOINING.
		El first_init_mbr broadcastea el estado.
					
SOLUCION3:	TODOS los first_init_mbr hace un broadcast de sus tablas.
		El receptor rastrea y solo actualiza los SLOTS que NO son de la particion propia 
		(contrastados contra bm_init) son actualizados
		Se arma un bm_partition que almacena los bits de esos owners
		Luego, al finalizar el merge se hace bm_init OR= bm_partition.
		luego se contrasta el first_act_mbr contra el bm_init y ese es el nuevo first_act_mbr
	PROBLEMA: El receptor, sabe cuales son slots propios, 
		pero como sabe cuales slots son propios de la particion del emisor y no de otra?? 
	SOLUCION: Deberia llenar con OWNER=LOCALNODE aquellos SLOTS que no le pertenecen a su bm_init y broadcastear
		la PST propia.



ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	Atencion con los punteros a mensajes cuando se envia la PST.

---------------------------------------------------------------------------------------------------------------
20140823:	Manejo de NETWORK PARTITION Y MERGE.
	
VM0:
----------------------------------- PARTITION -------------------------------
 spread_loop:907:sender=VM0 Private_group=#0.0#node0 vm_name=VM0 first_act_mbr=0 service_type=8192
 spread_loop:1080:received TRANSITIONAL membership for group VM0
 spread_loop:1084:received incorrecty membership message of type 0x2000
 spread_loop:907:sender=VM0 Private_group=#0.0#node0 vm_name=VM0 first_act_mbr=0 service_type=6144
 spread_loop:996:Received REGULAR membership for group VM0 with 1 members, where I am member 0:
 spread_loop:1031:Due to NETWORK change with 1 VS sets
 spread_loop:1045:LOCAL VS set 0 has 1 members:
 spread_loop:1059:	#0.0#node0
 get_nodeid:24:member=#0 nodeid=0
 spread_loop:1068:old bm_init=3 bm_active=1 first_act_mbr=0
 sp_net_partition:545:bm_init=3 bm_active=1
 sp_net_partition:576:first_init_mbr=0 bm_init=1 bm_donors=0		ESTADO DESPUES DE PARTICION
 send_status_info:103:Send the VM info to the new members
 send_status_info:123:Send PST to new members

----------------------------------- MERGE -------------------------------
spread_loop:907:sender=VM0 Private_group=#0.0#node0 vm_name=VM0 first_act_mbr=0 service_type=6144
 spread_loop:996:Received REGULAR membership for group VM0 with 2 members, where I am member 0:
 spread_loop:1031:Due to NETWORK change with 2 VS sets
 spread_loop:1045:LOCAL VS set 0 has 1 members:
 spread_loop:1059:	#0.0#node0
 get_nodeid:24:member=#0 nodeid=0
 spread_loop:1068:old bm_init=1 bm_active=1 first_act_mbr=0
 spread_loop:1045:OTHER VS set 1 has 1 members:
 spread_loop:1059:	#1.0#node1
 get_nodeid:24:member=#1 nodeid=1
 spread_loop:1068:old bm_init=1 bm_active=3 first_act_mbr=0			ESTADO DESPUES DE MERGE
 sp_net_merge:597:
 sp_net_merge:633:Send the partition's PST to all members 
 
 spread_loop:907:sender=#1.0#node1 Private_group=#0.0#node0 vm_name=VM0 first_act_mbr=0 service_type=4
 spread_loop:920:message from #1.0#node1, of type 6, (endian 0) to 1 groups (3840 bytes)
 spread_loop:976:source=27342 type=-1 m3i1=1953066601 m3i2=0 m3p1=(nil) m3ca1=[]
 get_nodeid:24:member=#1 nodeid=1
 sp_merge_PST:656:
 send_status_info:103:Send the VM info to the new members
 send_status_info:123:Send PST to new members

 
 VM1:
----------------------------------- PARTITION -------------------------------
 spread_loop:907:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=0 service_type=8192
 spread_loop:1080:received TRANSITIONAL membership for group VM0
 spread_loop:1084:received incorrecty membership message of type 0x2000
 spread_loop:907:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=0 service_type=6144
 spread_loop:996:Received REGULAR membership for group VM0 with 1 members, where I am member 0:
 spread_loop:1031:Due to NETWORK change with 1 VS sets
 spread_loop:1045:LOCAL VS set 0 has 1 members:
 spread_loop:1059:	#1.0#node1
 get_nodeid:24:member=#1 nodeid=1
 spread_loop:1068:old bm_init=3 bm_active=2 first_act_mbr=1
 sp_net_partition:545:bm_init=3 bm_active=2
 sp_net_partition:576:first_init_mbr=1 bm_init=2 bm_donors=0			ESTADO DESPUES DE PARTICION
 send_status_info:103:Send the VM info to the new members
 send_status_info:123:Send PST to new members
 
----------------------------------- MERGE -------------------------------
 spread_loop:907:sender=#1.0#node1 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=1 service_type=32
 spread_loop:920:message from #1.0#node1, of type 1, (endian 0) to 1 groups (36 bytes)
 spread_loop:926:source=1 type=1 m1i1=47 m1i2=0 m1i3=28 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
 spread_loop:907:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=1 service_type=6144
 spread_loop:996:Received REGULAR membership for group VM0 with 2 members, where I am member 1:
 spread_loop:1031:Due to NETWORK change with 2 VS sets
 spread_loop:1045:OTHER VS set 0 has 1 members:
 spread_loop:1059:	#0.0#node0
 get_nodeid:24:member=#0 nodeid=0
 spread_loop:1068:old bm_init=2 bm_active=1 first_act_mbr=0
 spread_loop:1045:LOCAL VS set 1 has 1 members:
 spread_loop:1059:	#1.0#node1
 get_nodeid:24:member=#1 nodeid=1
 spread_loop:1068:old bm_init=2 bm_active=3 first_act_mbr=0				ESTADO DESPUES DE MERGE
 sp_net_merge:597:
 sp_net_merge:633:Send the partition's PST to all members 
 spread_loop:907:sender=#1.0#node1 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=0 service_type=4
 spread_loop:920:message from #1.0#node1, of type 6, (endian 0) to 1 groups (3840 bytes)
 spread_loop:976:source=27342 type=-1 m3i1=1953066601 m3i2=0 m3p1=(nil) m3ca1=[]
 get_nodeid:24:member=#1 nodeid=1
 sp_merge_PST:656:
 spread_loop:907:sender=#0.0#node0 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=0 service_type=4
 spread_loop:920:message from #0.0#node0, of type 6, (endian 0) to 1 groups (3840 bytes)
 spread_loop:976:source=35631 type=0 m3i1=1953066601 m3i2=0 m3p1=(nil) m3ca1=[]
 get_nodeid:24:member=#0 nodeid=0
 sp_merge_PST:656:
 spread_loop:907:sender=#0.0#node0 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=0 service_type=4
 spread_loop:920:message from #0.0#node0, of type 3, (endian 0) to 1 groups (172 bytes)
 spread_loop:960:SYS_PUT_VMINFO vmid=0
 sp_put_vminfo:186:
 spread_loop:907:sender=#0.0#node0 Private_group=#1.0#node1 vm_name=VM0 first_act_mbr=0 service_type=4
 spread_loop:920:message from #0.0#node0, of type 4, (endian 0) to 1 groups (3840 bytes)
 get_nodeid:24:member=#0 nodeid=0
 spread_loop:956:SYS_PUT_PST: first_init_mbr=0 table has 160 slots
 sp_put_PST:243:

	FUNCIONO OK!!
	

---------------------------------------------------------------------------------------------------------------
20140906:	SLOTS Manejo de donaciones interrumpidas
			Cuando un donante hace el multicast, puede que se detecte un NETWORK PARTITION o un CRASH del DESTINATARIO
			Para ello  se usa el BIT_DONATING que, cuando el donante hace el broadcast el bit se setea
			y cuando el mismo recibe su propio mensaje lo borra. 
			Si esto es interrumpido por un net partition o un crash entonces el donante busca en todos los slots
			los BIT_DONATING, los recupera como propios y resetea el bit nuevamente.
			
			COMPILADO OK, pero no probado.
		
---------------------------------------------------------------------------------------------------------------
20150318:	
			mol-hyper: se generaron nuevos tipos de error cuyo valor absoluto es mayor a:
			#define EMOLERRCODE 	(_SIGN 300) 
			De esa forma no habria conflicto con los valores de endpoints retornadas por los tipos de BIND
			cuando el valor del p_nr es negativo.
		
			Por lo tanto_
				rcode = mol_bind() 
				if ( rcode < EMOLERRCODE) HAY ERROR de bind
				sino retorna el endpoint=rcode.


---------------------------------------------------------------------------------------------------------------
20150319:
			SE MODIFICO SYSTEM.C la posicion de inicializacion de la tabla de procesos y de slots.
			Ahora la inicializacion depende si va a haber 1 o mas nodos.
			
---------------------------------------------------------------------------------------------------------------
20150323:
			Saque los diff entre la VM de oscar y la mia
			se supone que hice los cambios pero me da error al ttratar de instalar el modulo
			
Cree el sitio
https://sourceforge.net/projects/m3ipc/
			
usr: ppessolani
passwd: <8digitos_renault9><2digito_año_nacimiento> total 10 caracteres


---------------------------------------------------------------------------------------------------------------
20150419:
		Incorpore el codigo de MARIE del driver de MEMORY
			- drivers/memory
			- drivers/libdriver
			
		Compila y ejecuta pero falta probar con imagenes de disco.
		
---------------------------------------------------------------------------------------------------------------
20150425:
		Incorpore el codigo de DIEGO del FS
			- servers/fs
		y varios programas de pruebas en tests y cambios en kernel/minix/*.h
		
		COMPILA BIEN.
		ANDA MAL
		
---------------------------------------------------------------------------------------------------------------
20150426:
		AHORA FUNCIONA OK
		FALTABA DEFINIR EN FS.H 
		#DEFINE MOLFS 1

---------------------------------------------------------------------------------------------------------------
20150501:		
		El  loop/loop_read_fs funcion MAL si se le define el buffer como posix_align
		En cambio funciona bien cuando se lo define como estatico.
		
Copiar 472 bytes NO ALINEADOS
[43057.950557] DEBUG 3269:mol_vcopy:1088: COPY_USR2USR_PROC copylen=472
[43057.950565] DEBUG 3269:copy_usr2usr:275: source=27342 src_pid=3269 dst_pid=3270 bytes=472
[43057.950573] DEBUG 3269:copy_usr2usr:281: src_off=1332 dst_off=3820

En el origen los datos estan en 1 pagina, pero en el destino estan en 2 !!!!! porque ???????
[43057.950580] DEBUG 3269:copy_usr2usr:286: src_npag=1 dst_npag=2 <<<<<<<<<<<<<<<<<<<<<<<<<<
[43057.950589] DEBUG 3269:copy_usr2usr:295: get_user_pages SRC OK
[43057.950598] DEBUG 3269:copy_usr2usr:302: get_user_pages DST OK
[43057.950604] DEBUG 3269:copy_usr2usr:305: kmap_atomic SRC OK
[43057.950610] DEBUG 3269:copy_usr2usr:307: kmap_atomic DST OK
[43057.950617] DEBUG 3269:copy_usr2usr:320: memcpy 276 bytes
Copia correctamente los primeros 276 bytes


[43057.950623] DEBUG 3269:copy_usr2usr:331: kunmap_atomic SRC OK
[43057.950629] DEBUG 3269:copy_usr2usr:333: kunmap_atomic DST OK
[43057.950636] DEBUG 3269:copy_usr2usr:335: set_page_dirty_lock OK
[43057.950642] DEBUG 3269:copy_usr2usr:337: put_page DST OK
[43057.950648] DEBUG 3269:copy_usr2usr:339: put_page SRC OK
[43057.950655] DEBUG 3269:copy_usr2usr:345: src_off=1608 dst_off=0
[43057.950662] DEBUG 3269:copy_usr2usr:295: get_user_pages SRC OK <<<<<<<<direccion origen OK
[43057.950938] ERROR: 3269:copy_usr2usr:301: rcode=-71            <<<<<<<<direccion destino ERROR!!!

funciono bien el loop_read_fs con posix_align
hay que cambiar en el loop de lectura principal 
bytes = MOL_read(fd, (void *) &buffer, buf_size); 
por 
bytes = MOL_read(fd, (void *) buffer, buf_size); 
fíjate que buffer NO LLEVA el &

loop/loop_read_fs
==================		
 MOL_open:121:MOL_open: file1MB.dat 2
 MOL_open:133:Request: source=-1216873754 type=5 m3i1=12 m3i2=0 m3p1=0xbff5ded8 m3ca1=[file1MB.dat]	
 main:220:fd=0

  read-->rw_chunk--->get_block--->rw_block--->dev_io---->gen_io

 ESTA ES LA SECUENCIA ABORTADA DMESG
[ 6365.451821] DEBUG FS:mol_mini_sendrec:450: srcdst_ep=3
[ 6365.456525] DEBUG FS:mol_mini_sendrec:450: srcdst_ep=3
[ 6365.456930] DEBUG FS:mol_vcopy:923: src_ep=1 dst_ep=5 bytes=4096
[ 6365.457039] DEBUG FS:mol_mini_sendrec:450: srcdst_ep=3
[ 6365.457460] fs[FS]: segfault at 1000 ip 0804ebb4 sp bf916118 error 4 in fs[8048000+9000]

EN EL SERVIDOR 
 get_block:459:rw_block(bp, READING); con bp->b_blocknr 934
 get_block:459:rw_block(bp, READING); con bp->b_blocknr 903 ????
 get_block:459:rw_block(bp, READING); con bp->b_blocknr 935
 get_block:459:rw_block(bp, READING); con bp->b_blocknr 903 ????
 get_block:459:rw_block(bp, READING); con bp->b_blocknr 936
 get_block:459:rw_block(bp, READING); con bp->b_blocknr 903 ????
<<<<<< ABORTA AQUI
rw_block:114: dev_io -> op=1027, dev=256, POS=3833856, BYTES=4096 <<< ESTO DEBERIA SEGUIR 

pero como:
SVRDEBUG(" dev_io -> op=%d, dev=%d, POS=%d, BYTES=%d\n",  op, dev, pos, block_size);   
	r = dev_io(op, dev, FS_PROC_NR, bp->b_data, pos, block_size, 0);
Quiere decir que alli no llego.

---------------------------------------------------------------------------------------------------------------
20150508:
		SE HIZO FUNCIONAR LA VERSION DE M3-IPC MODULO BAJO VIRTUAL BOX
		
---------------------------------------------------------------------------------------------------------------
20150509:
		SE MIGRARON LOS FUENTES DE M3-IPC MODULO A UNA NUEVA VM DEL NODO0 BAJO VMWARE
		BASICAMENTE FUNCIONA BIEN.
		
		QUEDAN PENDIENTES TEST REMOTOS
	
---------------------------------------------------------------------------------------------------------------
20150513:
		se modificaron Makefiles para apuntar con path absoluto 
		se modificaron scripts de pruebas remotas rtestX.sh
		
		TESTS REMOTOS OK!
		Falta solo entre 3 nodos
		
---------------------------------------------------------------------------------------------------------------
20150513 - TECO:
		Cambiando el tema de los proxies para que solo pueda soportar un proxy

TODO:
	OJO con el uso de RLOCK_XXX Y WLOCK_XXXX
 
TODO OJO
	if( px_nr < 0 || px_nr >= d_ptr->d_nr_nodes) 	ERROR_RETURN(EMOLBADRANGE);
	El numero de proxies esta limitado la numero de nodos ???


HAY 2 FUNCIONES SIMILARES
	NODE2SPROXY => proc_t *get_sproxy(int nodeid) 
	NODE2RPROXY => proc_t *get_rproxy(int nodeid) 

---------------------------------------------------------------------------------------------------------------
20150514:
		Se hicieron pruebas LOCALES Y REMOTAS
		En la remota hubo solo fallo en las IPC tipo 3 (NOTIFY, RECEIVE), pero se cuelga el NODE1 SIN MODULO
		en lugar del NODE0 con MODULO
		Cuando ejecuto el NODE0 sin modulos todo va bien.
		
TODO 	HACER EL DIFF ENTRE RPROXY CON MODULO Y SIN MODULO			
	
---------------------------------------------------------------------------------------------------------------
20150517:
	
[  888.245968] DEBUG 2645:check_lock_caller:212: caller_pid=2645 
[  888.245974] DEBUG 2645:mm_put2lcl:493: WUNLOCK_PROC ep=-1 count=0
[  888.245983] DEBUG 2645:mm_put2lcl:502: cmd=2 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
[  888.245991] DEBUG 2645:mm_put2lcl:508: vmid=0
[  888.245997] DEBUG 2645:mm_put2lcl:524: WLOCK_VM vm=0 count=0
[  888.246004] DEBUG 2645:mm_put2lcl:550: TIMESTAMP sec=1431832025 nsec=245145707
[  888.246177] DEBUG 2644:mm_get2rmt:413: WUNLOCK_PROC ep=0 count=-1 <<<<<<<< 
[  888.246207] DEBUG 2730:sleep_proc2:539: WLOCK_PROC ep=0 count=-1  <<<<<<<<<<<
[  888.246214] DEBUG 2730:sleep_proc2:539: WLOCK_PROC ep=1 count=0
[  888.246226] DEBUG 2730:sleep_proc2:565: nr=0 endp=0 vmid=0 lpid=2730 p_cpumask=FF name=loop_server3 
[  888.246234] DEBUG 2730:sleep_proc2:567: someone wakeups me: sem=0 p_rcode=0
[  888.246241] DEBUG 2730:mm_mini_notify:886: WUNLOCK_PROC ep=0 count=-1 <<<<<<<<<
[  888.246251] DEBUG 2730:mm_mini_notify:886: WUNLOCK_PROC ep=1 count=0
[  888.246263] DEBUG 2730:mm_mini_receive:268: src_ep=31438
[  888.246270] DEBUG 2730:check_lock_caller:157: caller_pid=2730 caller_tgid=2730
[  888.246277] DEBUG 2730:check_lock_caller:191: WLOCK_PROC ep=0 count=0
[  888.246284] DEBUG 2730:check_lock_caller:212: caller_pid=2730


[   97.079349] DEBUG 2329:mm_put2lcl:665: CMD_NTFY_MSG vmid=0 rmt_ep=1 rmt_nr=1 lcl_ep=0 lcl_nr=0
[   97.079353] DEBUG 2329:notify_rmt2lcl:114: vmid=0 src_ep=1 src_ptr->p_usr.p_rts_flags=1000 dst_ep=0 dst_ptr->p_usr.p_rts_flags=0
[   97.079416] ERROR: 2329:notify_rmt2lcl:136: rcode=-330 <<<<< EMOLOVERRUN	 (_SIGN 330) 
[   97.079429] DEBUG 2329:notify_rmt2lcl:139: set_sys_bit src_ptr->p_priv.p_usr.s_id=68

	if(get_sys_bit(dst_ptr->p_priv.s_notify_pending, src_ptr->p_priv.s_usr.s_id))
		ERROR_PRINT(EMOLOVERRUN); <<<<<<<<< 

		
---------------------------------------------------------------------------------------------------------------
20150523:
		Hice esqueleto funcionando de RDISK

---------------------------------------------------------------------------------------------------------------
20150607:		
		PRUEBAS DE PROXIES DE UN SOLO PROCESO 
		Cree clt_proxy y svr_proxy bajo TCP
		Hay algun problema en M3-IPC. 
		Probe proxy/test/loop_server1.c en NODO0 y
		Probe proxy/test/loop_client1.c en NODO1 
		
		Aparentemente en NODO0 funciona bien.
		En NODO1  (Client) hay algun problema en el mnx_get2rmt() 

CLT_PROXY GET2RMT (PID 2540)
[  931.084501] DEBUG 2540:mm_get2rmt:76: 
[  931.084508] DEBUG 2540:check_lock_caller:157: caller_pid=2540 caller_tgid=2540
[  931.084516] DEBUG 2540:check_lock_caller:191: WLOCK_PROC ep=-1 count=0
[  931.084523] DEBUG 2540:check_lock_caller:212: caller_pid=2540 
[  931.084529] DEBUG 2540:mm_get2rmt:104: list_for_each_entry_safe
[  931.084535] DEBUG 2540:mm_get2rmt:428: Any message was not found.
[  931.084542] DEBUG 2540:sleep_proc:444: timeout=30000 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< TIMEOUT 30 seg
[  931.084548] DEBUG 2540:sleep_proc:455: BEFORE DOWN lpid=2540 p_sem=0 timeout=30000
[  931.084556] DEBUG 2540:sleep_proc:457: WUNLOCK_PROC ep=-1 count=0
[  931.084563] DEBUG 2540:sleep_proc:458: endpoint=-1 flags=8 <<<<<<<<<<<<<<<<<<<<<<<<<< FLAG=8 o sea esta OK

CLIENT SENDREC (13 SEGUNDOS DESPUES CON UN TIMEOUT DE 30 )
[  944.048650] DEBUG 2718:mm_mini_sendrec:448: srcdst_ep=0
[  944.048661] DEBUG 2718:check_lock_caller:157: caller_pid=2718 caller_tgid=2718
[  944.048670] DEBUG 2718:check_lock_caller:191: WLOCK_PROC ep=1 count=0
[  944.048677] DEBUG 2718:check_lock_caller:212: caller_pid=2718 
[  944.048684] DEBUG 2718:mm_mini_sendrec:456: caller_nr=1 caller_ep=1 
[  944.048690] DEBUG 2718:mm_mini_sendrec:469: vmid=0
[  944.048696] DEBUG 2718:mm_mini_sendrec:472: RLOCK_VM vm=0 count=0
[  944.048703] DEBUG 2718:mm_mini_sendrec:474: RUNLOCK_VM vm=0 count=0
[  944.048710] DEBUG 2718:mm_mini_sendrec:502: WUNLOCK_PROC ep=1 count=0
[  944.048716] DEBUG 2718:mm_mini_sendrec:502: WLOCK_PROC ep=0 count=0
[  944.048723] DEBUG 2718:mm_mini_sendrec:502: WLOCK_PROC ep=1 count=0
[  944.048740] DEBUG 2718:mm_mini_sendrec:503: srcdst_nr=0 srcdst_ep=0
[  944.048748] DEBUG 2718:mm_mini_sendrec:526: vmid=0 caller_pid=2718 caller_nr=1 srcdst_ep=0 
[  944.048754] DEBUG 2718:mm_mini_sendrec:531: SENDING HALF
[  944.048766] DEBUG 2718:mm_mini_sendrec:547: RLOCK_PROC ep=-1 count=0
[  944.048773] DEBUG 2718:mm_mini_sendrec:557: RUNLOCK_PROC ep=-1 count=0
[  944.048780] DEBUG 2718:mm_mini_sendrec:564: WUNLOCK_PROC ep=0 count=0
[  944.048791] DEBUG 2718:sproxy_enqueue:31: nr=1 endp=1 vmid=0 flags=200C misc=20 lpid=2718 nodeid=1 nodemap=2 name=loop_client1 
[  944.048802] DEBUG 2718:sproxy_enqueue:33: cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=36
[  944.048809] DEBUG 2718:sproxy_enqueue:36: WLOCK_PROC ep=-1 count=0
[  944.048818] DEBUG 2718:sproxy_enqueue:45: nr=0 endp=-1 vmid=-1 >>>>>flags=0<<<<<  misc=3 >>>>>lpid=2540<<<<< nodeid=1 nodemap=1 name=clt_proxy 
[  944.048826] DEBUG 2718:sproxy_enqueue:52: WUNLOCK_PROC ep=-1 count=0		
		
PORQUE los flags se ven en 0 ?????????
Incluso en el /proc se ven en 0 !!!

PXID -Type- -lpid- -flag- -misc- -getf- -sendt -wmig- name
   0 sender   2540      0      3  27342  27342  27342 clt_proxy <<<<<<<<<<<<<<<      
   0 recver   2540      8      3  31438  27342  27342 clt_proxy      
		
AHAHAHAHAHAHHHHHHH 
    MIRAR EL recver los flags valen 8 !!!!!!!!!!!!!!!!!!!!!!!!
	
PROBLEMA: 	Hay 2 descriptores de procesos MINIX para un solo proceso LINUX !!	

			Algunas funciones pueden hacer un doble lock asi que hay que mantener los descriptores por separado.
			check_lock_caller solo referencia al proceso RECEIVER (donde lo asigna? en el proxybind seguramente)
			
SOLUCION:	

---------------------------------------------------------------------------------------------------------------
20150608:	
		Cuando hay un solo proxy, la task LINUX apunta al proc del RPROXY
		Se modifico mm_sproxy.c para que cuando es invocado verifique si el descriptor corresponde al RPROXY
		Si es asi lo cambia al del SPROXY 
		
		Las pruebas funcionan bien x q el mensaje SENDREC se envia (loop_server1 vs loop_client1)

---------------------------------------------------------------------------------------------------------------
20150610:	
			loop_client1 (SENDREC) /loop_server1 (RECEIVE/SEND) FUNCIONAN OK! 
			CLIENT				SERVER
			SENDREC	-----------> RECEIVE
			        <----------- SEND
			ACK     ------------> 
	
			loop_client2 (SEND/RECEIVE) /loop_server2 (RECEIVE/SEND) NO FUNCIONA, PERO NO INTERESA
			Hay que programar los proxies solo para el patron de uso.
			
PATRONES DE USO
PATRON 1:
			CLIENT				SERVER
			SENDREC	-----------> RECEIVE
			        <----------- SEND
			SENDACK ------------> 

PATRON 2:
			CLIENT				SERVER
			SENDREC	-----------> RECEIVE
					<----------- CMD_COPYIN_DATA
			CMD_COPYIN_ACK ----> 
					<----------- CMD_COPYIN_DATA
			CMD_COPYIN_ACK ----> 
					<----------- CMD_COPYIN_DATA
			CMD_COPYIN_ACK ----> 
			        <----------- SEND
			SENDACK ------------> 			

PATRON 2:
			CLIENT				SERVER
			SENDREC	-----------> RECEIVE
					<----------- CMD_COPYOUT_RQST
			CMD_COPYOUT_DATA ----> 
					<----------- CMD_COPYOUT_RQST
			CMD_COPYOUT_DATA ----> 
					<----------- CMD_COPYOUT_RQST
			CMD_COPYOUT_DATA ----> 
			        <----------- SEND
			SENDACK ------------> 			
				
---------------------------------------------------------------------------------------------------------------
20150612:	 
			Funcionaron perfectamente el clt_proxy y svr_proxy
			
			SERVER								CLIENT							
			proxy/test/loop_server1				proxy/test/loop_client1			PATRON1
			proxy/test/loop_svr_vcopy1a			proxy/test/loop_clt_vcopy1a		PATRON2
			proxy/test/loop_svr_vcopy1			proxy/test/loop_clt_vcopy1		PATRON3
	
Se cambio el kernel/minix/config.h
#define MAXCOPYBUF	65536 para las transferencias de mayor tamaño.

---------------------------------------------------------------------------------------------------------------
20150627:	
	El algoritmo de calculo de los nodos initializados en base a los slots poseidos puede fallar
	porque puede haber un nodo inicializado que no tiene slots todavia.
	se puede solucionar cuando el propio nodo que detecta que no tiene slots envia un mensaje INITIALIZED para que todos lo vean
	

TODO: Hacer diagrama de estados especificando fallos.
		VER QUE PASA SI ALGUN PASO SE INTERRUMPE.
		Por ejemplo: que pasa si un SYS_INITIALIZED enviado por un nuevo miembro inicializado 
			antes de que llegue a destino recibe un mensaje 
					NET_PARTITION		

---------------------------------------------------------------------------------------------------------------
20150628:

PM:	Se detiene en:
	get_work:119:Wait for the next message and extract useful information from it.
	get_work:124:Request received from who_e=35631, call_nr=20 MOLGETPID
	pm_isokendpt:41:endpoint=35631 *proc=29 kproc_ep=35631
	do_getset:25:call_nr=20

 Le estaria faltando todo esto:
	pm_isokendpt:41:endpoint=8 *proc=8 kproc_ep=8
	do_getset:47:ret=3
	main:66:call_nr=20 result=3
	setreply:276:proc_nr=29 result=3
	main:76:Send out all pending reply messages
	main:86:Replying to 35631

 ATENTI!!! :
	Murio SYSTASK y PM !!!!!
	
---------------------------------------------------------------------------------------------------------------
20150628:	

PM:	Se detiene en:
	get_work:119:Wait for the next message and extract useful information from it.
	get_work:124:Request received from who_e=35631, call_nr=20 MOLGETPID
	pm_isokendpt:41:endpoint=35631 *proc=29 kproc_ep=35631
	do_getset:25:call_nr=20

[ 1148.481713] DEBUG 2647:mm_mini_sendrec:448: srcdst_ep=0
[ 1148.481716] DEBUG 2647:check_lock_caller:157: caller_pid=2647 caller_tgid=2647
[ 1148.481719] DEBUG 2647:check_lock_caller:191: WLOCK_PROC ep=35631 count=0
[ 1148.481722] DEBUG 2647:check_lock_caller:212: caller_pid=2647 
[ 1148.481725] DEBUG 2647:mm_mini_sendrec:456: caller_nr=29 caller_ep=35631 
[ 1148.481728] DEBUG 2647:mm_mini_sendrec:469: vmid=0
[ 1148.481730] DEBUG 2647:mm_mini_sendrec:472: RLOCK_VM vm=0 count=0
[ 1148.481733] DEBUG 2647:mm_mini_sendrec:474: RUNLOCK_VM vm=0 count=0
[ 1148.481736] DEBUG 2647:mm_mini_sendrec:502: WUNLOCK_PROC ep=35631 count=0
[ 1148.481739] DEBUG 2647:mm_mini_sendrec:502: WLOCK_PROC ep=0 count=0
[ 1148.481741] DEBUG 2647:mm_mini_sendrec:502: WLOCK_PROC ep=35631 count=0
[ 1148.481744] DEBUG 2647:mm_mini_sendrec:503: srcdst_nr=0 srcdst_ep=0
[ 1148.481747] DEBUG 2647:mm_mini_sendrec:526: vmid=0 caller_pid=2647 caller_nr=29 srcdst_ep=0 
[ 1148.481750] DEBUG 2647:mm_mini_sendrec:531: SENDING HALF
[ 1148.481753] DEBUG 2647:mm_mini_sendrec:612: destination is waiting. Copy the message and wakeup destination
[ 1148.481756] DEBUG 2647:copy_usr2usr:355: source=35631 src_pid=2647 dst_pid=PM bytes=36
[ 1148.481760] DEBUG 2647:copy_usr2usr:361: src_off=1516 dst_off=2880
[ 1148.481762] DEBUG 2647:copy_usr2usr:366: src_npag=1 dst_npag=1
[ 1148.481766] DEBUG 2647:copy_usr2usr:375: get_user_pages SRC OK
[ 1148.481769] DEBUG 2647:copy_usr2usr:382: get_user_pages DST OK
[ 1148.481771] DEBUG 2647:copy_usr2usr:385: kmap_atomic SRC OK
[ 1148.481773] DEBUG 2647:copy_usr2usr:387: kmap_atomic DST OK
[ 1148.481776] DEBUG 2647:copy_usr2usr:400: memcpy 36 bytes
[ 1148.481779] DEBUG 2647:copy_usr2usr:405: source=35631 bytes=36
[ 1148.481781] DEBUG 2647:copy_usr2usr:411: kunmap_atomic SRC OK
[ 1148.481783] DEBUG 2647:copy_usr2usr:413: kunmap_atomic DST OK
[ 1148.481786] DEBUG 2647:copy_usr2usr:415: set_page_dirty_lock OK
[ 1148.481788] DEBUG 2647:copy_usr2usr:417: put_page DST OK
[ 1148.481791] DEBUG 2647:copy_usr2usr:419: put_page SRC OK
[ 1148.481793] DEBUG 2647:copy_usr2usr:425: src_off=1552 dst_off=2916
[ 1148.481796] DEBUG 2647:inherit_cpu:325: cpuid=0
[ 1148.481799] DEBUG 2647:inherit_cpu:333: nr=0 endp=0 vmid=0 lpid=PM p_cpumask=FF name=pm 
[ 1148.481803] DEBUG 2647:mm_mini_sendrec:618: BEFORE UP lpid=PM p_sem=-1 rcode=0
[ 1148.481807] DEBUG 2647:mm_mini_sendrec:626: WUNLOCK_PROC ep=0 count=0
[ 1148.481810] DEBUG 2647:sleep_proc:444: timeout=-1
[ 1148.481812] DEBUG 2647:sleep_proc:455: BEFORE DOWN lpid=2647 p_sem=0 timeout=-1
[ 1148.481815] DEBUG 2647:sleep_proc:457: WUNLOCK_PROC ep=35631 count=0
[ 1148.481818] DEBUG 2647:sleep_proc:458: endpoint=35631 flags=8
[ 1148.481828] DEBUG PM:sleep_proc:469: endpoint=0 flags=0 cpuid=0
[ 1148.481831] DEBUG PM:sleep_proc:470: WLOCK_PROC ep=0 count=0
[ 1148.481835] DEBUG PM:sleep_proc:496: nr=0 endp=0 vmid=0 lpid=PM p_cpumask=FF name=pm 
[ 1148.481838] DEBUG PM:sleep_proc:498: someone wakeups me: sem=0 p_rcode=0
[ 1148.481841] DEBUG PM:mm_mini_receive:430: WUNLOCK_PROC ep=0 count=0
[ 1148.482209] pm[PM]: segfault at 925a1c4 ip 08049ff0 sp bff2ce70 error 4 in pm[8048000+7000] <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[ 1148.482957] DEBUG PM:mm_exit_unbind:2277: LOCK_TASK pid=PM count=0
[ 1148.482999] DEBUG PM:mm_exit_unbind:2281: WLOCK_PROC ep=0 count=0
[ 1148.483065] DEBUG PM:mm_exit_unbind:2324:  Exiting endpoint=0 lpid=PM
[ 1148.483103] DEBUG PM:mm_exit_unbind:2326: WUNLOCK_PROC ep=0 count=0
[ 1148.483158] DEBUG PM:mm_exit_unbind:2327: WLOCK_VM vm=0 count=0
[ 1148.483224] DEBUG PM:mm_exit_unbind:2332: WLOCK_PROC ep=0 count=0
[ 1148.483258] DEBUG PM:mm_exit_unbind:2334:  endpoint=0 lpid=PM
[ 1148.483329] DEBUG PM:do_unbind:1409: vmid=0 endpoint=0 lpid=PM nodeid=0 flags=0 pseudosem=0
[ 1148.483412] DEBUG PM:do_unbind:1434: Caller endpoint=0 lpid=PM
[ 1148.483463] DEBUG PM:do_unbind:1488: wakeup with error all process trying to send a message to the proc
[ 1148.483504] DEBUG PM:do_unbind:1514: delete notify messages bits sent by the proc
[ 1148.483630] DEBUG PM:do_unbind:1523: Delete the bit 67 in the notify bitmap of processes 35631 
[ 1148.483697] DEBUG PM:do_unbind:1532: Process 35631 is no more waiting a message from the unbinded process 0
[ 1148.483737] DEBUG PM:do_unbind:1535: Wakeup RECEIVER with error ep=35631  pid=2647
[ 1148.483756] DEBUG PM:inherit_cpu:325: cpuid=0
[ 1148.483761] DEBUG PM:inherit_cpu:333: nr=29 endp=35631 vmid=0 lpid=2647 p_cpumask=FF name=init 
[ 1148.483832] DEBUG PM:do_unbind:1536: BEFORE UP lpid=2647 p_sem=-1 rcode=-108
[ 1148.483923] DEBUG PM:do_unbind:1542: nr=0 endp=0 vmid=0 flags=0 misc=20 lpid=PM nodeid=0 nodemap=1 name=pm 
[ 1148.483969] DEBUG PM:do_unbind:1559: decrement the reference count of the task struct=PM count=3
[ 1148.483993] DEBUG PM:init_proc_desc:228: p_name=pm vmid=0
[ 1148.484093] DEBUG PM:mm_exit_unbind:2339: WUNLOCK_VM vm=0 count=0
[ 1148.484178] DEBUG PM:mm_exit_unbind:2345: WUNLOCK_PROC ep=0 count=0
[ 1148.484197] DEBUG PM:mm_exit_unbind:2347: UNLOCK_TASK pid=PM count=0
[ 1148.484468] DEBUG 2647:sleep_proc:469: endpoint=35631 flags=8 cpuid=0	

Se modifico do_fork.c porque estaba mal el endpoint que le pasaba a isokenpt_pm()

---------------------------------------------------------------------------------------------------------------
20150630:
	Se modifico SLOT.C considerando diferentes estados con fallos.
	Se elimino la variable initialized y se la reemplazo por un bit en FSM_state

---------------------------------------------------------------------------------------------------------------
20150704:
	Modificaciones varias  a slot.c y system.c
	FUNCIONA OK!
	
	- ARRANCA node0 => node0 es el primary_mbr
	- ejecucion normal del algoritmo con 1 nodo
	- ARRANCA node1
	- ejecucion normal del algoritmo con 2 nodos
	- CAE node0 => node1 es el nuevo primary_mbr y ve un NET_PARTITION y mantiene reservados los slots de node0
	- Ejecucion del algoritmo con numero de slots reducido (por la reserva)
	- REARRANCA node0 => node1 se da cuenta que NO FUE un NET_PARTITION y quita la reserva de los slots de node0 y se apropia de ellos.
	- ejecucion normal del algoritmo con node1 como el primary_mbr y el total de los slots.

---------------------------------------------------------------------------------------------------------------
20150719:
	Se detecto problema en el BIND, cuando un thread quiere hacer bind() del main thread.
	se modifico
		mm_hyper.c
		stub_syscall.h  definiendose  mnx_tbind(vmid,p_nr)  una nueva macro que permite hacer el bind del propio hilo hijo
	
BEHAVIOUR:
	Si el bind lo hace un thread sobre su TID
		Se debe verificar que el TGID este bindeado en la misma VM
			Si el TGID no esta bindeado, entonces error.
	Si el bind lo hace un thread sobre su PID
		Se bindea al TGID.
			
	testeado con /loop/loop_sr_thread 
	funciono ok.
	
	
---------------------------------------------------------------------------------------------------------------
20150808:
		merge de M3-IPC MODULE + MOL-FS + MOL-FUSE
	
---------------------------------------------------------------------------------------------------------------
20150920:	
		Se implemento RCU para DRVS, VMS, NODES y PROXIES
		Se probo en transferencias locales
		
---------------------------------------------------------------------------------------------------------------
20151010:
		TEST REMOTOS
		Falla en el notify-receive test (test3)
		

ver http://lxr.free-electrons.com/source/drivers/block/nbd.c#L216
https://sourceforge.net/projects/nbd/files/nbd/3.11/
NBD Cliente envio IOCTL al modulo de kernel

Se instaló ksocket ksocket-0.0.2.tar.bz2 y se probo el CLIENTE Y SERVER EXITOSAMENTE

1) Desarrollar modulo proxy CLIENT/SERVER
	- Como verificar que el DRVS está activo y obtener el local_nodeid
	- Se debe desarrollar un kernel thread para CLIENT y otro para SERVER
	- Como establecer que para un nodo dado el proxy es el adecuado?
	- Se debe realizar una nueva primitiva 
		mnx_kproxies_bind(name, pxnr)
	- Modificar  mnx_proxy_conn(pxid, status) porque no se usaria para proxies de kernel
	- Modificar  mnx_node_up(name, nodeid, proxy_nr)
	- Modificar  mnx_getproxyinfo(pxid, sproc_usr, rproc_usr)
	- Definir un p_misc_flag para indicar que el proxy es un MODULO DE KERNEL
	
	
	El comando a ejecutar seria:
		insmod m3ipc_tcp_proxy px_id=<pxid> px_name=<name>
		
	http://www.makelinux.net/ldd3/chp-2-sect-8
	http://www.makelinux.net/books/lkd2/ch16lev1sec6
	http://linuxkernel51.blogspot.com.ar/2011/03/use-of-module-parameters-in-kernel.html
	
proxy_sender(){
sender_init()	
while(TRUE){
	m3_get2rmt(header, payload);
	ksend(socket, header, sizeof(header), 0);
	if(header->payload_len)
		ksend(socket, payload, header->payload_len, 0);		
}	

proxy_receiver(){
receiver_init()	
while(TRUE){
	krecv(socket, header, sizeof(header), 0);
	if(header->payload_len)
		krecv(socket, payload, header->payload_len, 0);
	m3_put2lcl(header, payload);
}	
		
	SE IMPLEMENTO EL MODULO m3ipc_tcp_proxy
	Funciona bien el k_proxies_bind() (exportado como simbolo en mol_replace.c
	
---------------------------------------------------------------------------------------------------------------
20151011:
		ARRANCANDO NODO1 se establece la conexion y se transfiere el mensaje
		
[  173.686615] m3ipc_tcp_proxy_init px_id=1
[  173.686671] m3ipc_tcp_proxy_init px_name=node1 
[  173.686692] m3ipc_tcp_proxy local_nodeid=0
[  173.686798] m3ipc_tcp_proxy_init rproxy_pid=2640
[  173.686871] m3ipc_tcp_proxy_init sproxy_pid=2641
[  173.689808] DEBUG 2640:pr_init:80: RPROXY: Initializing proxy receiver
[  173.689975] DEBUG 2640:pr_setup_connection:64: RPROXY: rproxy_fd = 0xef687100 port=3001
[  173.693177] DEBUG 2640:tcp_rproxy:193: RPROXY: Waiting for connection.
[  177.530565] DEBUG 2640:tcp_rproxy:198: RPROXY: Remote sender connected. Getting remote command.
[  177.531212] DEBUG 2640:mm_proxy_conn:1995: px_nr=1, status=2
[  177.533382] DEBUG 2640:pr_process_message:148: RPROXY: About to receive header
[  177.533475] DEBUG 2640:pr_receive_header:122: 
[  207.540472] DEBUG 2640:pr_receive_header:127: RPROXY: n:64 | received:64 | HEADER_SIZE:64
[  207.540668] DEBUG 2640:pr_receive_header:129: RPROXY: cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
[  207.540815] DEBUG 2640:pr_process_message:151: RPROXY:cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
[  207.540870] DEBUG 2640:pr_process_message:148: RPROXY: About to receive header
[  207.540877] DEBUG 2640:pr_receive_header:122: 
[  223.040802] DEBUG 2640:pr_receive_header:127: RPROXY: n:64 | received:64 | HEADER_SIZE:64
[  223.040808] DEBUG 2640:pr_receive_header:129: RPROXY: cmd=1 vmid=0 src=5 dst=0 snode=1 dnode=0 rcode=0 len=36
[  223.040813] DEBUG 2640:pr_process_message:151: RPROXY:cmd=1 vmid=0 src=5 dst=0 snode=1 dnode=0 rcode=0 len=36
[  223.040881] DEBUG 2640:pr_receive_payload:104: payload_size=36
[  223.041372] DEBUG 2640:pr_receive_payload:108: RPROXY: n:36 | received:36
[  223.041414] DEBUG 2640:pr_process_message:165: RPROXY: put2lcl
[  223.041449] DEBUG 2640:tcp_rproxy:206: RPROXY: Message succesfully processed.
[  223.041464] DEBUG 2640:pr_process_message:148: RPROXY: About to receive header
[  223.041467] DEBUG 2640:pr_receive_header:122: 

Modificar mm_put2lcl y mm_get2rmt para q soporten proxies de kernel

- Se crearon en mm_utils y las macros correspondientes en mol-macros.h.
		long copy_krn2usr(int source, char *src_addr, struct proc *dst_proc, char *dst_addr, int bytes);
		long copy_usr2krn(int source, struct proc *src_proc, char *src_addr, char *dst_addr, int bytes);


		Se crearon RPROXY y SPROXY del modulo m3ipc_tcp_proxy
		Se creo kproxy.sh para:
			- Arranque la VM0
			- Arranque el proxy
			- De de alta el nodo1
			- bind del proceso remoto 1 

PRUEBAS BASICAS:
		Cuando se ejecuto loop_ipc_node0.sh funciono todo OK excepto en el tipo de prueba 3 del notify() pero
		tambien tuvo fallos con los proxies de usuario.

		Pruebas de vcopy(). En nodo1 se cuelga en loop_clt_vcopy1 8192 	
		El nodo0 colgado completamente.
		<<< FALLA EN EL MISMO LUGAR EN TODAS LAS PRUEBAS
		===> TIENE Q SER UN PROBLEMA DE MEMORIA Hasta tamaño de pagina 4096 funciona todo OK
		Cuando son 2 paginas FALLA!!

[  258.888694] DEBUG 2634:mm_get2rmt:72: 
[  258.888696] DEBUG 2634:check_lock_caller:149: caller_pid=2634 caller_tgid=2634
[  258.888698] DEBUG 2634:check_lock_caller:183: WLOCK_PROC ep=-1 count=0
[  258.888700] DEBUG 2634:check_lock_caller:204: caller_pid=2634 
[  258.888702] DEBUG 2634:mm_get2rmt:100: RLOCK_PROXY pxid=1
[  258.888704] DEBUG 2634:mm_get2rmt:109: RUNLOCK_PROXY pxid=1
[  258.888706] DEBUG 2634:mm_get2rmt:117: list_for_each_entry_safe
[  258.888708] DEBUG 2634:mm_get2rmt:475: Any message was not found.
[  258.888710] DEBUG 2634:sleep_proc:594: timeout=30000
[  258.888712] DEBUG 2634:sleep_proc:605: BEFORE DOWN lpid=2634 p_sem=0 timeout=30000
[  258.888714] DEBUG 2634:sleep_proc:607: WUNLOCK_PROC ep=-1 count=0
[  258.888717] DEBUG 2634:sleep_proc:608: endpoint=-1 flags=8
[  258.890834] DEBUG 2633:pr_receive_header:138: RPROXY: n:64 | received:64 | HEADER_SIZE:64
[  258.890838] DEBUG 2633:pr_receive_header:140: RPROXY: cmd=8198 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=8192
[  258.890842] DEBUG 2633:pr_process_message:162: RPROXY:cmd=8198 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=8192

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[  258.891133] BUG: unable to handle kernel paging request at f098b000 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

[  258.891133] IP: [<f097a0a5>] pr_process_message+0xa5/0x210 [m3ipc_tcp_proxy]
[  258.891133] *pde = 2f805067 *pte = 00000000 
[  258.891133] Oops: 0002 [#1] SMP 
[  258.891133] last sysfs file: /sys/devices/pci0000:00/0000:00:11.0/0000:02:01.0/device
[  258.891133] Modules linked in: m3ipc_tcp_proxy ksocket mol_replace
[  258.891133] 
[  258.891133] Pid: 2633, comm: rproxy1 Not tainted (2.6.32 #827) VMware Virtual Platform
[  258.891133] EIP: 0060:[<f097a0a5>] EFLAGS: 00010216 CPU: 0
[  258.891133] EIP is at pr_process_message+0xa5/0x210 [m3ipc_tcp_proxy]
[  258.891133] EAX: 00000000 EBX: ef94d160 ECX: 00000400 EDX: 00002000
[  258.891133] ESI: ef94d160 EDI: f098b000 EBP: ef94d160 ESP: ef1d5f78
[  258.891133]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
[  258.891133] Process rproxy1 (pid: 2633, ti=ef1d4000 task=ef94d160 task.ti=ef1d4000)
[  258.891133] Stack:
[  258.891133]  f097acf0 00000a49 f097afb3 000000a2 00002006 00000000 00000001 00000000
[  258.891133] <0> 00000001 00000000 00000000 00002000 ef1d5fd0 ef94d160 ef94d160 f097a5b1
[  258.891133] <0> f097ae58 00000a49 f097afa8 000000d9 00000a49 ef94d160 00000010 f097a4f0
[  258.891133] Call Trace:
[  258.891133]  [<f097a5b1>] ? tcp_rproxy+0xc1/0x1cc [m3ipc_tcp_proxy]
[  258.891133]  [<f097a4f0>] ? tcp_rproxy+0x0/0x1cc [m3ipc_tcp_proxy]
[  258.891133]  [<c10037ef>] ? kernel_thread_helper+0x7/0x18
[  258.891133] Code: 24 e8 90 b0 fd ff 85 c0 89 c7 0f 85 32 01 00 00 83 c4 30 89 f8 5b 5e 5f c3 90 8d 74 26 00 89 d1 8b 3d b4 b1 97 f0 31 c0 c1 e9 02 <f3> ab f6 c2 02 74 02 66 ab f6 c2 01 74 01 aa 89 d0 e8 25 fe ff 
[  258.891133] EIP: [<f097a0a5>] pr_process_message+0xa5/0x210 [m3ipc_tcp_proxy] SS:ESP 0068:ef1d5f78
[  258.891133] CR2: 00000000f098b000
[  258.891133] ---[ end trace 663fcdeefca6c039 ]---
[  258.891133] DEBUG 2633:mm_exit_unbind:2466: LOCK_TASK pid=2633 count=0
[  258.891133] DEBUG 2633:mm_exit_unbind:2470: WLOCK_PROC ep=-1 count=0
[  258.891133] DEBUG 2633:mm_exit_unbind:2474:  Exiting PROXY px_nr=1 lpid=2633
[  258.891133] DEBUG 2633:mm_exit_unbind:2479: RLOCK_PROXY pxid=1
[  258.891133] DEBUG 2633:mm_exit_unbind:2486: RUNLOCK_PROXY pxid=1
[  258.891133] DEBUG 2633:mm_exit_unbind:2491: WLOCK_PROC ep=-1 count=0
[  258.891133] DEBUG 2633:do_proxies_unbind:1867: px_nr=1, pid=2633


[  112.054700] DEBUG 2637:mm_put2lcl:672: CMD_SENDREC_MSG vmid=0 rmt_ep=1 rmt_nr=1 lcl_ep=0 lcl_nr=0
[  112.054771] DEBUG 2637:send_rmt2lcl:35: cmd=3 vmid=0 src_ep=1 src_flags=1000 dst_ep=0 dst_flags=8
[  112.054835] DEBUG 2637:send_rmt2lcl:38: source=1 type=1 m1i1=0 m1i2=2 m1i3=3 m1p1=(null) m1p2=b766ebf8 m1p3=b76931e3 

---------------------------------------------------------------------------------------------------------------
20151017:		
		Se habia cambiado valloc() por kmalloc() donde funcionaba bien IPC pero fallaba el VCOPY cuando len>PAGESIZE
		Ahora con kmalloc falla todo.

El mensaje se recibe bien en el RPROXY
[  112.054700] DEBUG 2637:mm_put2lcl:672: CMD_SENDREC_MSG vmid=0 rmt_ep=1 rmt_nr=1 lcl_ep=0 lcl_nr=0
[  112.054771] DEBUG 2637:send_rmt2lcl:35: cmd=3 vmid=0 src_ep=1 src_flags=1000 dst_ep=0 dst_flags=8
[  112.054835] DEBUG 2637:send_rmt2lcl:38: source=1 type=1 m1i1=0 m1i2=2 m1i3=3 m1p1=(null) m1p2=b766ebf8 m1p3=b76931e

Pero se copia mal del RPROXY al proceso de usuario.
[  112.055894] DEBUG 2657:mm_mini_send:29: dst_ep=-1047865824 <<<<<<<<<<<
[  112.055950] DEBUG 2657:mm_mini_send:36: caller_nr=0 caller_ep=0 dst_ep=-1047865824 <<<<<<<<<<<
[  112.056012] ERROR: 2657:mm_mini_send:78: rcode=-34 <<<<<<<<<<<<<<

	Se habia volvio a cambiar kalloc() por vmalloc() y funciona OK el IPC.
 
ERROR:	Aparentemente el que esta fallando es el SPROXY TCP modo USUARIO!!
	Se recibio la siguiente peticion del NODO0
	[  197.589349] DEBUG 2630:copyout_rqst_rmt2lcl:375: src=1 dst=0 rqtr=0 saddr=504f4e4d daddr=b7474000 bytes=2048 

[  197.589493] DEBUG 2629:copy_usr2usr:358: src_npag=2 dst_npag=1
[  197.589593] ERROR: 2629:copy_usr2usr:366: rcode=-71
#define EMOLADDRNOTAVAIL (_SIGN 71)  /* Can't assign requested address */

ERROR:	Probando nuevamente falla con 8192 por primera vez, es decir que hay copia de 2 paginas!!!
	Entonces, alli esta el problema.
	 
	 
---------------------------------------------------------------------------------------------------------------
20151018:

[  604.288375] DEBUG 2673:pr_receive_header:138: RPROXY: n:64 | received:64 | HEADER_SIZE:64
[  604.288387] DEBUG 2673:pr_receive_header:140: RPROXY: cmd=8198 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=8192
[  604.288399] DEBUG 2673:pr_process_message:162: RPROXY:cmd=8198 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=8192
[  604.289055] BUG: unable to handle kernel paging request at f098c000
[  604.289055] IP: [<f097b0a5>] pr_process_message+0xa5/0x240 [m3ipc_tcp_proxy]
[  604.289055] *pde = 2f805067 *pte = 00000000 
[  604.289055] Oops: 0002 [#1] SMP 
[  604.289055] last sysfs file: /sys/devices/pci0000:00/0000:00:11.0/0000:02:01.0/device
[  604.289055] Modules linked in: m3ipc_tcp_proxy ksocket mol_replace
[  604.289055] 
[  604.289055] Pid: 2673, comm: rproxy1 Not tainted (2.6.32 #827) VMware Virtual Platform
[  604.289055] EIP: 0060:[<f097b0a5>] EFLAGS: 00010216 CPU: 0
[  604.289055] EIP is at pr_process_message+0xa5/0x240 [m3ipc_tcp_proxy]
[  604.289055] EAX: 00000000 EBX: 00002000 ECX: 00000400 EDX: f097bd20
[  604.289055] ESI: ef8d5920 EDI: f098c000 EBP: ef8d5920 ESP: ef245f78
[  604.289055]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
[  604.289055] Process rproxy1 (pid: 2673, ti=ef244000 task=ef8d5920 task.ti=ef244000)
[  604.289055] Stack:
[  604.289055]  f097bd20 00000a71 f097c01b 000000a2 00002006 00000000 00000001 00000000
[  604.289055] <0> 00000001 00000000 00000000 00002000 ef245fd0 ef8d5920 ef8d5920 f097b5e1
[  604.289055] <0> f097bec0 00000a71 f097c010 000000da 00000a71 ef8d5920 00000010 f097b520
[  604.289055] Call Trace:
[  604.289055]  [<f097b5e1>] ? tcp_rproxy+0xc1/0x1cc [m3ipc_tcp_proxy]
[  604.289055]  [<f097b520>] ? tcp_rproxy+0x0/0x1cc [m3ipc_tcp_proxy]
[  604.289055]  [<c10037ef>] ? kernel_thread_helper+0x7/0x18
[  604.289055] Code: 24 e8 b0 a0 fd ff 85 c0 89 c7 0f 85 3f 01 00 00 83 c4 30 89 f8 5b 5e 5f c3 90 8d 74 26 00 89 d9 8b 3d 34 c2 97 f0 31 c0 c1 e9 02 <f3> ab f6 c3 02 74 02 66 ab f6 c3 01 74 01 aa f6 05 9c ee 95 f0 
[  604.289055] EIP: [<f097b0a5>] pr_process_message+0xa5/0x240 [m3ipc_tcp_proxy] SS:ESP 0068:ef245f78
[  604.289055] CR2: 00000000f098c000
[  604.289055] ---[ end trace e4c89926a9b00c65 ]---
[  604.289055] DEBUG 2673:mm_exit_unbind:2472: LOCK_TASK pid=2673 count=0


FUNCIONO OK!!!!
		EL MALDITO PROBLEMA ERA que se hacia vmalloc() pero calculando mal el tamaño x q faltaba multiplicar por PAGE_SIZE
	  
---------------------------------------------------------------------------------------------------------------
20151024:
		Se modifico SLOTs
		Se probaron NODE1 y NODE0 ambos con el proxy de kernel 
		Se colgaron en loop_svr_vcopy3a 5 2048 en NODE0 

EN NODE0 están dados de alta estos ENDPOINTS
VMID p_nr -endp- -lpid- node -flag- -getf- -sndt- -wmig- -prxy- name
   0    0      0   3097    0    400  27342  27342  27342  27342 loop_svr_vcopy3
   0    1      1     -1    1   1408      0  27342  27342  27342 SoyRemoto      
   0   10     10   3099    0    408      0  27342  27342  27342 loop_svr_vcopy3
  
EN NODE1 están dados de alta estos ENDPOINTS
VMID p_nr -endp- -lpid- node -flag- -getf- -sndt- -wmig- -prxy- name
   0    0      0     -1    0   1000  27342  27342  27342      0 SoyRemoto      
   0    1      1   3099    1      8      0  27342  27342  27342 loop_clt_vcopy1  
   
   Se puede ver que aqui no està dado de alta el endpoint 10 como REMOTO.
   
 El NODE0 hace un vcopy desde el endpoint 1 al endpoint 10, pero el NODE1 desconoce al endpoint 10
[  548.672371] DEBUG 3097:mm_vcopy:918: src_ep=1 dst_ep=10 bytes=2048 

---------------------------------------------------------------------------------------------------------------
20151025/6:
	Test de particion. Al arrancar el NODO1 (Luego de la particion) no adquiere nuevos slots.
	
NODE0: 
	PARTITION 
 slots_loop:1115:sender=VM0 Private_group=#0.0#node0 vm_name=VM0 service_type=8192 SP_bytes=12 
 slots_loop:1305:received TRANSITIONAL membership for group VM0
 slots_loop:1309:received incorrecty membership message of type 0x2000
 slots_loop:1115:sender=VM0 Private_group=#0.0#node0 vm_name=VM0 service_type=6144 SP_bytes=56 
 slots_loop:1204:Received REGULAR membership for group VM0 with 1 members, where I am member 0:
 slots_loop:1245:Due to NETWORK change with 1 VS sets
 slots_loop:1267:LOCAL VS set 0 has 1 members:
 slots_loop:1280:	#0.0#node0
 get_nodeid:43:member=#0 nodeid=0
 slots_loop:1288:OLD bm_active=3 active_nodes=2
 slots_loop:1289:NEW bm_active=1 active_nodes=1
 sp_net_partition:680:
 sp_net_partition:689:bm_init=3 bm_active=1
 sp_net_partition:743:primary_mbr=0 bm_init=1 bm_donors=0
 sp_net_partition:748:total_slots=95 max_owned_slots=95 init_nodes=1 bm_init=1
 send_status_info:165:Send Global status sts_size=1456
 
	MERGE
 slots_loop:1115:sender=VM0 Private_group=#0.0#node0 vm_name=VM0 service_type=6144 SP_bytes=92 
 slots_loop:1204:Received REGULAR membership for group VM0 with 2 members, where I am member 0:
 slots_loop:1245:Due to NETWORK change with 2 VS sets
 slots_loop:1267:LOCAL VS set 0 has 1 members:
 slots_loop:1280:	#0.0#node0
 get_nodeid:43:member=#0 nodeid=0
 slots_loop:1267:OTHER VS set 1 has 1 members:
 slots_loop:1280:	#1.0#node1
 get_nodeid:43:member=#1 nodeid=1
 slots_loop:1288:OLD bm_active=1 active_nodes=1
 slots_loop:1289:NEW bm_active=3 active_nodes=2
 sp_net_merge:771:
 sp_net_merge:797:slot reserved 123 for owner=1
 .......
 sp_net_merge:805:Send the partition's PST to all initialized members 
 send_status_info:165:Send Global status sts_size=1456
 
	SYS_MERGE_STATUS
 slots_loop:1115:sender=#1.0#node1 Private_group=#0.0#node0 vm_name=VM0 service_type=32 SP_bytes=1456 
 slots_loop:1128:message from #1.0#node1, of type 5, (endian 0) to 1 groups (1456 bytes)
 slots_loop:1176:SYS_MERGE_STATUS vmid=0
 get_nodeid:43:member=#1 nodeid=1
 sp_merge_status:825:
 sp_merge_status:850: 172+4+2048=2224 bytes=1456 
 
 
NODE1  
	PARTICION
 slots_loop:1115:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 service_type=8192 SP_bytes=12 
 slots_loop:1305:received TRANSITIONAL membership for group VM0
 slots_loop:1309:received incorrecty membership message of type 0x2000
 slots_loop:1115:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 service_type=6144 SP_bytes=56 
 slots_loop:1204:Received REGULAR membership for group VM0 with 1 members, where I am member 0:
 slots_loop:1245:Due to NETWORK change with 1 VS sets
 slots_loop:1267:LOCAL VS set 0 has 1 members:
 slots_loop:1280:	#1.0#node1
 get_nodeid:43:member=#1 nodeid=1
 slots_loop:1288:OLD bm_active=3 active_nodes=2
 slots_loop:1289:NEW bm_active=2 active_nodes=1
 sp_net_partition:680:
 sp_net_partition:689:bm_init=3 bm_active=2
 get_primary_mbr:25:bm_init=2 first_mbr=1
 sp_net_partition:743:primary_mbr=1 bm_init=2 bm_donors=0
 sp_net_partition:748:total_slots=16 max_owned_slots=16 init_nodes=1 bm_init=2
 send_status_info:165:Send Global status sts_size=1456
 
	MERGE
 slots_loop:1115:sender=VM0 Private_group=#1.0#node1 vm_name=VM0 service_type=6144 SP_bytes=92 
 slots_loop:1204:Received REGULAR membership for group VM0 with 2 members, where I am member 1:
 slots_loop:1245:Due to NETWORK change with 2 VS sets
 slots_loop:1267:OTHER VS set 0 has 1 members:
 slots_loop:1280:	#0.0#node0
 get_nodeid:43:member=#0 nodeid=0
 slots_loop:1267:LOCAL VS set 1 has 1 members:
 slots_loop:1280:	#1.0#node1
 get_nodeid:43:member=#1 nodeid=1
 slots_loop:1288:OLD bm_active=2 active_nodes=1
 slots_loop:1289:NEW bm_active=3 active_nodes=2
 sp_net_merge:771:
 sp_net_merge:797:slot reserved 96 for owner=0
 ...
 sp_net_merge:805:Send the partition's PST to all initialized members 
 send_status_info:165:Send Global status sts_size=1456

 slots_loop:1115:sender=#1.0#node1 Private_group=#1.0#node1 vm_name=VM0 service_type=32 SP_bytes=1456 
 slots_loop:1128:message from #1.0#node1, of type 5, (endian 0) to 1 groups (1456 bytes)
 slots_loop:1176:SYS_MERGE_STATUS vmid=0
 get_nodeid:43:member=#1 nodeid=1
 sp_merge_status:825:

 slots_loop:1115:sender=#0.0#node0 Private_group=#1.0#node1 vm_name=VM0 service_type=32 SP_bytes=1456 
 slots_loop:1128:message from #0.0#node0, of type 5, (endian 0) to 1 groups (1456 bytes)
 slots_loop:1176:SYS_MERGE_STATUS vmid=0
 get_nodeid:43:member=#0 nodeid=0
 sp_merge_status:825:
 sp_merge_status:850: 172+4+2048=2224 bytes=1456 

PROBLEMA: NODO1 Recibio 2 mensajes SYS_MERGE_STATUS. 1 de NODE1 y el otro de NODE0
			NODO0 Debio haber recibido 2 mensajes y el propio descartarlo.
			
	Esto se debe a que SYSTASK MUERE antes de recibir su propio mensaje.
	NODE0 y NODE1 da el siguiente error:
	*** ERROR in "./systask": double free or corruption (!prev): 
	ERROR: sys_procinfo:107: rcode=-105		

SOLUCION: Error en la copia de datos de los slots desde el mensaje al buffer.

---------------------------------------------------------------------------------------------------------------
20151101/02:

PROBLEMA: Despues de llegar el mensaje MERGE de SPREAD, los miembros inicializados quedan  	
		FSM_state = STS_MERGE_STATUS;
		Todos mezclan las tablas de slots provenientes de los PRIMARIOS de las distintas particiones.
		Pero COMO SABE CUANDO TERMINAN y quedan en estado RUNNING?	
		Como sabe el PRIMARIO definitivo que:
			1) el es el primario definitivo
			2) que ya se termino todo el intercambio de tablas y la PST es la definitiva?
	
SOLUCION:	En funcion del bm_active, verifica todos los slots que estan con owner=NO_PRIMARY_MBR 
			si hay algun propietario del bm_active es porque estaba inicializado y entonces
			es que todavia esta pendiente su primario de reportar el SYS_MERGE_STATUS.
	
	
	APARENTEMENTE FUNCIONA OK
	
---------------------------------------------------------------------------------------------------------------
20151104:			
		Cuando se produce un MERGE:
		1- todos los procesos quedan en estado STS_MERGE_STATUS
		2- Solo los Primary broadcastean SYS_MERGE_STATUS
		
		Problemas:
			Que pasa si entre el MERGE Y SYS_MERGE_STATUS enviado por los primarios
				- mueren primarios
				- se produce otro merge
				- hay joins
				- hay partition
				
	APARENTEMENTE FUNCIONA OK

---------------------------------------------------------------------------------------------------------------
20151107:
			SLOT cambio de estrategia.
			Ahora se calcula en forma dinamica el Primary member 
			
			APARENTEMENTE FUNCIONA OK

---------------------------------------------------------------------------------------------------------------
20151210:
		se detecto haciendo pruebas en el simulador DAJ que cuando un nodo envia su JOIN, y hasta 
		que recibe el SYS_PUT_STATUS por parte del primary_mbr, pueden suceder que otros nodos
		envien sus SYS_INITIALIZED. El mensaje SYS_PUT_STATUS entonces no trae el bitmap completo
		de los nodos realmente inicializados (supongo que si traeria los desconectados).
		El bm_init se va completando incluso ANTES de recibir el SYS_PUT_STATUS y se hace un OR con 
		el bm_init recibido desde primary_mbr.
		
		Se creo la MACRO 
		#define CEILING(x,y)	(x/y + (x%y!= 0))	

		Se incorporo la funcion CEILING en el calculo de la peticion y la donacion de ranuras.
	
		Se incorporo la siguiente heuristica para mejorar el tiempo de respuesta:
		Cuando se recibe un SYS_REQ_SLOTS, si no hay ranuras para donar (con excepción del nodo siguiente
		al requester), se deja pendiente la respuesta en un bitmap bm_pending. 
		Cuando se recibe en nodoC una donacion de un nodoA->nodoB, el nodoC verifica si tiene 
		respuesta de donación de ZERO al nodoB. Si las tiene, envia un SYS_DON_SLOTS con ZERO slots.
		Si muere el nodoB antes, se borra del bitmap bm_pending
		Si ocurre una particion, se hace un AND con bm_init

		Otra mejora que se incorpora es que cuando hay dos (o mas) SYS_REQ_SLOTS simultaneos, estos indican
		que sus nodos origenes no tienen slots para donar, por lo tanto equivalen a SYS_DON_SLOTS con ZERO slots.
		
		
---------------------------------------------------------------------------------------------------------------
20151219:
		respecto al modulo de kernel TCP se hicieron pruebas y da errores de copia de memoria
		NODO1:

[  465.969044] DEBUG 2626:mm_put2lcl:512: WUNLOCK_PROC ep=-1 count=0
[  465.969053] DEBUG 2626:mm_put2lcl:524: cmd=5 (CMD_COPYIN_DATA) vmid=0 src=0 dst=1 snode=0 dnode=1 rcode=0 len=512
[  465.969060] DEBUG 2626:mm_put2lcl:530: vmid=0
[  465.969065] DEBUG 2626:mm_put2lcl:546: RLOCK_VM vm=0
[  465.969072] DEBUG 2626:mm_put2lcl:572: TIMESTAMP sec=1450214010 nsec=968176365
[  465.969079] DEBUG 2626:mm_put2lcl:577: WLOCK_PROC ep=0 count=0
[  465.969085] DEBUG 2626:mm_put2lcl:577: WLOCK_PROC ep=1 count=0
[  465.969091] DEBUG 2626:mm_put2lcl:596: REMOTE source OK endpoint=0
[  465.969097] DEBUG 2626:mm_put2lcl:604: LOCAL destination OK endpoint=1
[  465.969103] DEBUG 2626:mm_put2lcl:661: WLOCK_PROC ep=-1 count=0
[  465.969109] DEBUG 2626:mm_put2lcl:663: WUNLOCK_PROC ep=-1 count=0
[  465.969115] DEBUG 2626:mm_put2lcl:664: RUNLOCK_VM vm=0
[  465.969122] DEBUG 2626:mm_put2lcl:692: CMD_COPYIN_DATA vmid=0 rmt_ep=0 rmt_nr=0 lcl_ep=1 lcl_nr=1
[  465.969131] DEBUG 2626:copyin_data_rmt2lcl:305: cmd=5 vmid=0 src=0 dst=1 snode=0 dnode=1 rcode=0 len=512

AQUI HAY ALGO PARTICULAR, LA DIRECCION NO ESTA ALINEADA
[  465.969141] DEBUG 2626:copyin_data_rmt2lcl:306: src=10 dst=1 rqtr=0 saddr=b74c9000 >>>> daddr=b75cf1e3<<<<< bytes=512 
[  465.969149] DEBUG 2626:copyin_data_rmt2lcl:317: DST OK endpoint=1
[  465.969157] DEBUG 2626:copy_krn2usr:440: source=27342 dst_pid=2944 bytes=512

[  465.969163] DEBUG 2626:copy_krn2usr:446: src_off=0 dst_off=483 <<<<<<LA DIRECCION NO ESTA ALINEADA
[  465.969169] DEBUG 2626:copy_krn2usr:451: src_npag=1 dst_npag=1
[  465.969367] ERROR: 2626:copy_krn2usr:459: rcode=-71
		
		
SE MODIFICARON LOS PROGRAMAS DE PRUEBA QUE DETECTAN CUANDO LA DIRECCION NO ESTA ALINEADA

JUSTAMANTE falla cuando no esta alineada. 
NODO0: loop_svr_vcopy3 100 16384
WARNING m2.m1p1 not page aligned B759F1E3

BIND SERVER vmid=0 svr_pid=3014 svr_nr=0 svr_ep=0
source=10 type=1 m1i1=1024 m1i2=2 m1i3=3 m1p1=0xb73f8000 m1p2=0xb775b55c m1p3=(nil) 

MENSAJE ENVIADO NODO1->NODO0 sin alineacion
source=1 type=1 m1i1=1024 m1i2=2 m1i3=3 >>>>>>>>>>m1p1=0x54535251<<<<<<< m1p2=0x58575655 m1p3=0x43424159 
t_start=1450254354.06 t_stop=1450254354.06 t_total=0.00
transfer size=1024 #transfers=0 loopbysec=0.000000
Throuhput = 0.000000 [bytes/s]


Normalmente debio recibir NODO0 
source=1 type=1<<< m1i1=65536 m1i2=2<<< m1i3=3<<< m1p1=0xb7435000<<<< ALINEADO  m1p2=0xb7544bf8 m1p3=0xb75691e3 

NODO1 ->NODO0 
source=-1217091280 type=1 m1i1=65536 m1i2=2 m1i3=3 m1p1=0xb73e7000 m1p2=0xb74f6bf8 m1p3=0xb751b1e3 
NODO0 recibe
source=1 type=1 m1i1=65536 m1i2=2  m1i3=0!!!!! m1p1=0x504f4e4d!!!!!! NO ALINENDO m1p2=0x54535251 m1p3=0x58575655 

Osea, hay un problema en la transferencia de mensajes!
DEBERIAN SER ALINEADAS
		
---------------------------------------------------------------------------------------------------------------
20151220:
		Se modificaron los proxy/test para que los mensajes esten alineados (posix_malign)
		
TODO OK-. POR LO TANTO
 
	EL PROXY DE KERNEL SOLO FUNCIONA CON ESTRUCTURAS DE DATOS ALINEADAS (MENSAJES Y BUFFERS)

---------------------------------------------------------------------------------------------------------------
20160101:
		Se modifico el funcionamiento de SLOTS de tal forma de contemplar la transferencia de estado de 
		los process p < nr_sysprocs. Polìtica respecto a la recepcion de  SYS_MERGE_STATUS:
			/* MERGING POLICY
				1- Locally bound slots remain locally bound
				2- Remotely bound slots remain Remotely bound
					a- if remain the node and same endpoint  (mnx_migr_rollback)
					b- if remain the same node and different endpoint (mnx_unbind, mnx_rmtbind)
					c- if changed the node but same the endpoint (mnx_migr_commit)
					d- if changed the node and changed the endpoint mnx_unbind, mnx_rmtbind)
				3- Free slots now bound on remote node (mnx_rmtbind)
				
---------------------------------------------------------------------------------------------------------------
20160102:
		Se habilito el mcast_binds2rmt() que se ejecutan una vez q el proceso hizo el JOIN para informar
		al resto de los miembros de cuales son sus procesos de sistema que deben bindear.
		Se habilito el mcast_exit_proc() y mcast_fork_proc() que informan al resto de los miembros si hay un fork 
		de proceso de sistema.
		
		
	ERROR: 
	En NODO0:
VMID p_nr -endp- -lpid- node -flag- -getf- -sndt- -wmig- -prxy- name
   0   -5     -5     -1    1   1000  27342  27342  27342  27342 spread         
   0   -4     -4     -1    1   1000  27342  27342  27342  27342 systask        
   0   -3     -3   2641    0      0  27342  27342  27342  27342 systask        
   0   -2     -2   2640    0      8  31438  27342  27342  27342 systask        
   0    0      0   RPROXY    0      4  27342     40  27342  27342 pm      <<< PM: receive(40)       
   0    8      8   2645    0      C      0      0  27342  27342 init    <<< INIT: sendrec(PM)        
   0    9      9     -1    1   100C      0      0  27342  27342 init           
   0   29     29   2646    0      C      0      0  27342  27342 init           
   0   30     30   2647    0      C      0      0  27342  27342 init           
   0   31     31   2648    0      C      0      0  27342  27342 init           
   0   32     32   2650    0      C      0      0  27342  27342 init           
   0   33     33   2651    0      C      0      0  27342  27342 init           
   0   34     34   2652    0      C      0      0  27342  27342 init           
   0   35     35   2653    0      C      0      0  27342  27342 init           
   0   36     36   2654    0      C      0      0  27342  27342 init           
   0   37     37   2655    0      C      0      0  27342  27342 init           
   0   38     38   2656    0      C      0      0  27342  27342 init           
   0   39     39   2657    0      C      0      0  27342  27342 init           
   0   40     40     -1    1   1008      0  27342  27342  27342 init   <<<< 40 PROCESO REMOTO!!! ???        
   0   41     41     -1    1   100C      0      0  27342  27342 remote         
   0   44     44   2658    0      C      0      0  27342  27342 init           
   0   45     45   2659    0      C      0      0  27342  27342 init    
   
	En NODO1:
VMID p_nr -endp- -lpid- node -flag- -getf- -sndt- -wmig- -prxy- name
   0   -5     -5   2640    1      0  27342  27342  27342  27342 systask        
   0   -4     -4   2639    1      8  31438  27342  27342  27342 systask        
   0   -3     -3     -1    0   1000  27342  27342  27342  27342 remote         
   0   -2     -2     -1    0   1000  27342  27342  27342  27342 remote         
   0    0      0     -1    0   1000  27342  27342  27342  27342 remote <<<<< PM REMOTO ??????         
   0    8      8     -1    0   1000  27342  27342  27342  27342 remote         
   0    9      9   2644    1      8      0  27342  27342  27342 init           
   0   40     40   2645    1      8      0  27342  27342  27342 init           
   0   41     41   2646    1      8      0  27342  27342  27342 init           

---------------------------------------------------------------------------------------------------------------
20160109:
		
		FUNCIONO el MERGE STATUS tanto para slots del sistema como para slots de usuario.
---------------------------------------------------------------------------------------------------------------
20160110:
		Se implemento sys_migr_proc() para indicarle a todos los nodos que ha migrado un proceso 
		o cambiado de primary a backup

---------------------------------------------------------------------------------------------------------------
20160116:		
	
ERRORES VARIOS EN BIND:
	1- Retorna un error con numero negativo y sin embargo se admiten p_nr < 0 o sea habria incompatibilidad
		SOLUCION: Cambiar el formato del bind - NO SE PUEDE!! no hay mas lugar para parametros.
		Se agrego un control en sys_config.h 
			#if (NR_TASKS >= 300)  
			#error "NR_TASKS must be lower than -EMOLERRCODE (300)"
			#endif
			
	2- Se asigna automaticamente una estructura de privilegios si el proceso es  nr_sysprocs
		if( i < vm_ptr->vm_usr.vm_nr_sysprocs) {
			proc_ptr->p_priv.s_usr.s_id = i;
		}else{
			proc_ptr->p_priv.s_usr.s_id = 0;
		}
		Ahora bien, i = p_nr+vm_ptr->vm_usr.vm_nr_tasks; y como p_nr puede ser negativo, 
		i puede ser 0 => ERROR porque los procesos de usuario se le asigna el priv 0.-
		SOLUCION: No hay problemas porque cada proceso tiene su estructura de privilegios
		SOLUCION: para los procesos de usuario, el s_id 
				proc_ptr->p_priv.s_usr.s_id = vm_ptr->vm_usr.vm_nr_sysprocs;

	3-  Los endpoints de p < nr_sysprocs no cambia la generacion. 	
		if( i < vm_ptr->vm_usr.vm_nr_sysprocs) {
			proc_ptr->p_usr.p_endpoint = _ENDPOINT(0,proc_ptr->p_usr.p_nr);
			proc_ptr->p_priv.s_usr.s_id = i;
		}else{
			g = _ENDPOINT_G(proc_ptr->p_usr.p_endpoint);	/* Update endpoint 	*/
			if(++g >= _ENDPOINT_MAX_GENERATION)		/* increase generation */
				g = 1;					/* generation number wraparound */
			proc_ptr->p_usr.p_endpoint = _ENDPOINT(g,proc_ptr->p_usr.p_nr);
			proc_ptr->p_priv.s_usr.s_id = vm_ptr->vm_usr.vm_nr_sysprocs; <<<<  CAMBIADO
		}
		
---------------------------------------------------------------------------------------------------------------
20160117:		
	Se implemento  mnx_wait4bind TEMPORIZADO mnx_wait4bind_T()	
	
---------------------------------------------------------------------------------------------------------------
20160127:
	Se modifico el kernel de M3-IPC para soportar endpoints REPLICADOS
	Ademàs ahora el bind no se hace mas con el P_NR como parametro sino con el ENDPOINT.
	El que tiene que otorgar el endpoint ( en caso de procesos NO servidores) es la SYSTASK que tiene 
	que leer el descriptor del kernel (que se supone vacio), incrementar la GENERACION y generar
	un nuevo endpoint para hacer el BIND.

---------------------------------------------------------------------------------------------------------------
20160128:	
		Se otorga a SLOTS un unico endpoint en todos los nodos, al igual que HARDWARE. Es un endpoint REPLICADO!
		De la misma forma se modificaron PM e INIT para bindear como REPLICADOS.
		Se modifico mm_procfs para que se precesnte p_misc_flags en la tabla de procesos.
		
---------------------------------------------------------------------------------------------------------------
20160129:
		Se modifico mm_mini_notify() para soportar 
			mnx_ntfy_value(src_ep, dst_ep, value);	
		ATENCION: value solo puede ser 0 < value < sizeof(update_t) siendo
				typedef long unsigned int update_t;
		Es decir, no funcionara para notificar mas de 32 procesos 
		Este valor es el endpoint del proceso que necesita ser actualizado por el PM y el unico emisor es SLOTS.
		
		Se modifico tanto mm_ipc.c como mm_rproxy.c para soportar este tipo de notify REMOTO.
		Antes habia errores en la conformación del mensaje y además el mensaje remoto no soportaba el valor
		el valor ahora se transfiere en el rcode del CMD_NTFY_MSG.
		
		El PM de nodo ahora se entera de los procesos servidores (< nr_sysprocs) REMOTOS que son (BINDED/EXIT) que no son REPLICADOS
		El PM es notificado mnx_ntfy_value(src_ep, dst_ep, proc_ep) por SLOTS y entonces nace sys_procinfo() del proceso notificado.
		FUNCIONA OK!!
		
---------------------------------------------------------------------------------------------------------------
20160130:
		Se creo el comando "demonize" para como convertir un proceso de usuario (via fork()/exec()) 
		en un proceso servidor (< NR_SYS_PROCS)

---------------------------------------------------------------------------------------------------------------
20160206-08:
		Se modifico mm_ipc.c la parte de VCOPY porque no soportaba a SELF como parametro.
		Se cambiaron en varios servidores y tareas el malloc() por posix_align()

		Se modifico demonize como plataforma de prueba para ejecutar un proceso en un nodo remoto.
NODO0: Nodo origen
NODO1: Nodo destino

		NODO0: SYSTASK0, PM0, DEMONIZE0
		NODO1: SYSTASK1, rmt_ep
		
	DEMONIZE0--->PM0: 	rexec(node1, ../test/test_rexec);
	PM0---->SYSTASK1: 	SYS_REXEC
	SYSTASK1: 			hace un fork y queda wait4bind()
						el parent hace un sys_fork() para registrar el proceso en el KERNEL1 y SYSTASK1
						rmt_ep = sys_fork()
						el child queda a la espera de un mensaje del PM0 de tipo MOLREXEC
	SYSTASK1---->PM0:	rmt_ep
	PM0-----SYSTASK0:	SYS_REXEC
	SYSTASK0:			mnx_rmtbind(node1, rmt_ep);
	PM0:				registrar al proceso localmente en PM0 similar a un mol_fork() y obtiene rmt_pid
	PM0------>rmt_ep:	MOLREXEC
	rmt_ep:				execvp() 
	PM0--->DEMONIZE0:	rmt_pid
	
REXEC: 
El programa USER0 quiere ejecutar el USER1 para ello hace rexec(node1, file1)
En el nodo LOCAL, tanto el PM0 como el FS0 deben saber de la existencia de USER1
PM0 al saber que es NODE1 el destinatario, dispara el syscall SYS_REXEC a SYSTASK1.
	
La SYSTASK1 elige un endpoint libre y se lo retorna a PM0
SYSTASK1 hace un fork() de linux y el child queda a la espera de un mensaje MOLREXEC de PM0

Se modifico sys_exit y se hicieron 2 aliases:
	sys_exit - informa del exit a la systask local
	sys_rexit - informa del exit a la systask remota y luego a local
	
	Secuencia de ejecucion remota:
		NODO0:
				Ejecutar ./minix.sh 0 1 hasta el PM inclusive
				luego cd ./commands/demonize
				luego ./demonize -R 0 11 1 /home/MoL_Module/mol-ipc/servers/test/test_rexec
		NODO1:
				Ejecutar ./minix.sh 1 0 hasta SYSTASK inclusive

---------------------------------------------------------------------------------------------------------------
20160212:
	ERROR:   el mnx_unbind() cuelga el sistema.
	SOLUCION: Era un DEBUG imprimiendo algo de la tarea task

	ERROR: SE CUELGA !!!! sin hacer nada practicamente !! el problema esta en mol-module
	
	Constratar con diff lo de fecha de hoy y la ultima anterior MoL_Module20160209.zip
	
	
---------------------------------------------------------------------------------------------------------------
20160213:
	Ahora no se cuelga sin hacer nada. Se reprogramaron do_unbind y relacionados.
	
ERROR: 	Dejo de funcionar la ejecucion remota. Se cuelga el NODO1 

Se probo insertando en do_rexec del NODO1 despues de constatar que el mensaje proviene de un PM REMOTO,
un ERROR_RETURN(EMOLBADNODEID); y el NODO0 lo recibe pero luego AMBOS NODOS SE CUELGAN

insertandolo LUEGO del VCOPY se cuelga NODO1 !!!!

Por el analisis del dmesg0.txt
	el vcopy solicitado por SYSTASK1 para copiar el path desde DEMONIZE se recibe en NODO0, lo envia el SPROXY0 
	pero no se recibe ninguna confirmacion de NODO1 en el RPROXY0!!
				
ERROR:  CUANDO SE HACE EL SHUTDOWN SE CUELGA!!!!!
		CUANDO SE HACE CNTRL-C DE demonize cuando no termina SE CUELGA
	
---------------------------------------------------------------------------------------------------------------
20160214:	
		El problema parece estar en el UNBIND !!!!
		
---------------------------------------------------------------------------------------------------------------
20160217: 
		El problema estaba en el mm_exit_unbind y mm_unbind 
		Cuando habia un error de algun tipo se liberaba la estructura TASK  que podia haber estado ya liberada y 
		eso producia el error en el kernel.
		
CONSIDERACIONES:
		- Se probo SOLO con los test locales
		- el mecanismo de exclusion mutua son los MUTEXES (se reemplazo RCU)
		
TODO:	- probar con tests remotos
		- volver a RCU

---------------------------------------------------------------------------------------------------------------
20160219: 
			Cuando se prueba en NODO0 el test local loop_mol_perf.sh al superar los 8 clientes el ep=8 trata de hacer	
			unbind, pero se cuelga segun la siguiente secuencia.
			
[  210.760281] DEBUG 2718:do_unbind:1574: WUNLOCK_PROC ep=-2 count=0
[  210.760283] DEBUG 2718:do_unbind:1569: WUNLOCK_PROC ep=8 count=0
[  210.760286] DEBUG 2718:do_unbind:1570: WLOCK_PROC ep=-1 count=0
[  210.760288] DEBUG 2718:do_unbind:1571: WLOCK_PROC ep=8 count=0
[  210.760290] DEBUG 2718:do_unbind:1574: WUNLOCK_PROC ep=-1 count=0

AQUI el unbind(8) revisa si ep=0 tiene respuestas pendientes o notifies
[  210.760292] DEBUG 2718:do_unbind:1569: WUNLOCK_PROC ep=8 count=0
[  210.760295] DEBUG 2718:do_unbind:1570: WLOCK_PROC ep=0 count=0
[  210.760297] DEBUG 2718:do_unbind:1571: WLOCK_PROC ep=8 count=0
[  210.760299] DEBUG 2718:do_unbind:1574: WUNLOCK_PROC ep=0 count=0

[  210.760301] DEBUG 2718:do_unbind:1569: WUNLOCK_PROC ep=8 count=
	AQUI SE CUELGA : es cuando trata de hacer WLOCK_PROC ep=1, pero, mas arriba en el log aparece 

[  210.757753] DEBUG 2710:mm_mini_receive:265: src_ep=31438
[  210.757757] DEBUG 2710:check_lock_caller:150: caller_pid=2710 caller_tgid=2710
[  210.757760] DEBUG 2710:check_lock_caller:184: WLOCK_PROC ep=1 count=0 <<<<<<<<<<<<<<<<<<<<< 
[  210.757762] DEBUG 2710:check_lock_caller:205: caller_pid=2710 
[  210.757765] DEBUG 2710:mm_mini_receive:272: caller_nr=1 caller_ep=1 src_ep=31438 
[  210.757767] DEBUG 2710:mm_mini_receive:285: vmid=0
	
	Luego el proceso 2710, que es el SERVER no sigue pero esta vivo!!
	
	POSIBLE DEADLOCK
		El proceso server tiene tomado el descriptor de proceso caller_ptr (check_lock_caller)
		luego necesita tomar la VM 
		
	UNBIND tiene correctamente expresada la secuencia de tomar VM y luego proc.

	SOLUCION: Se toma la estructura VM en modo LECTURA, se copia hacia una variable local vm_struct_t 
			de esa forma no se necesita acceder a la VM y al proceso concurrentemente
			
			
ERROR:		Funcion todo OK (excepto loop_mol_mthreads.sh) 
			Se cuelga NODO1 cuando se quieren correr las pruebas REMOTAS
			El SPROXY de NODE0 tiene el flag RECEIVING prendido en el momento que intenta hacer un nuevo 
			mnx_get2rmt() el que arroja error EMOLNOTREADY.

---------------------------------------------------------------------------------------------------------------
20160220:
			El SERVER0 tiene prendida su flag SENDING
			El SPROXY0 tiene prendida su flag RECEIVING
			
			Es decir, aparentemente el SERVER0 envio CMD_SEND_MSG, el SPROXY0 lo envio y queda esperando nuevos pedidos locales	
			El problema es que el NODE1 no le envio el CMD_SEND_ACK 			
			
			HAY 2 ERRORES NODE0: loop_server1 NODE1: loop_client1
				- Se cuelga en el unbind (loops = 0 + 1)
				- Se cuelga en el senrec 
			
			SPROXY: Copiar la vm struct a variable local.
	

---------------------------------------------------------------------------------------------------------------
20160221:
			Al tratar de ejecutar en NODO0 #minix.sh 0 1, se cuelga despues de arrancar el PM.
			
			ps_start_serving:334:SPROXY 2631: Waiting a message
			ps_start_serving:349:ERROR  mnx_get2rmt -106 ==> (EMOLNOTREADY    (_SIGN 106))
	
			SOLUCION?? : Hice cambios en SPROXY.
			
			
			Se sigue colgando al arrancar MINIX ACA
				/* Register into SYSTASK (as an autofork) */
				SVRDEBUG("Register PM into SYSTASK pm_lpid=%d\n",pm_lpid);
				pm_ep = sys_bindproc(PM_PROC_NR, pm_lpid, REPLICA_BIND);
				if(pm_ep < 0) ERROR_EXIT(pm_ep);			
	
---------------------------------------------------------------------------------------------------------------
20160228:
			TODAS LAS VERSIONES ENTRE 21 Y 27 de tienen errores.
			Se reprogramo basicamente gran parte del codigo en lo que refiere a IPC y locking.
			Las pruebas LOCALES y REMOTAS realizadas resultaron correctas usando MUTEXES
			Las pruebas LOCALES usando RCU  fallan en loop_sr_mperf 8 100
---------------------------------------------------------------------------------------------------------------
20160302:
			Se agrego el soporte de SPINLOCKS a procesos. De esta forma cuando se usa RCU en las demas estructuras
			debe usarse SPINLOCKS en los procesos.
			Aun asi  Las pruebas LOCALES usando RCU  fallan en loop_sr_mperf 8 100
	
---------------------------------------------------------------------------------------------------------------
20160303:
			Se modificaron MACROS y tampoco funciona colgandose  loop_sr_mperf 4 1000
			Se modifico el LOCKING de TODO a MUTEXES
			Pruebas LOCALES OK!
			Pruebas REMOTAS OK!			
			Implica que la logica de LOCKING es correcta, pero algo de la implementacion con RCU falla

---------------------------------------------------------------------------------------------------------------
20160305:	Se quiere probar nuevamenta la ejecucion remota 
			Esta fallando el PM en pm_init y aparentemente en sys_bindproc
			Probando con test_sysgetinfo tambien FALLA el mm_bind() 			

	ERROR:	El problema esta en que SYSTASK usa mnx_rcvrqst() y mnx_reply() <<<< este es el que FALLA!!!
	

---------------------------------------------------------------------------------------------------------------
20160306:
			SYSTASK: se reemplazo rcvrqst x receive  y se reemplazo reply() por send()
	
			Ahora funciona OK todo el proceso excepto que se cuelga NODE1
	TEST_REXEC1-exit->PM0 
 get_work:123:Wait for the next message and extract useful information from it.
 get_work:128:Request received from who_e=29, call_nr=1
 pm_isokendpt:40:endpoint=29 
 pm_isokendpt:42:*proc=29
 pm_isokendpt:43:kproc.p_endpoint=29
 do_pm_exit:178:who_p=29 who_e=29 lnx_pid=-1 status=0
 pm_exit:218:mnx_pid=4 status=0 proc_nr=29 proc_ep=29 
 pm_exit:232:parent_pid=-1
 pm_exit:249:nr=29 endp=29 vmid=0 flags=1000 misc=0 lpid=-1 nodeid=1 nodemap=2 name=test_rexec 		
			
		PM0->sys_getinfo()->SYSTASK0	
 main:128:RECEIVE msg:source=0 type=1562 m4l1=200 m4l2=29 m4l3=11 m4l4=35534 m4l5=167820032
 main:137:call_nr=26 who_e=0
 main:142:Calling vector 26
 do_getinfo:36:caller=0 rqst=11 addr=A00BB00 len=200
 do_getinfo:77:p_ep=29 p_nr=29 
 do_getinfo:78:BEFOREnr=29 endp=29 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 do_getinfo:80:AFTERnr=29 endp=29 vmid=0 flags=1000 misc=0 lpid=-1 nodeid=1 nodemap=2 name=test_rexec 
 main:153:REPLY msg:source=0 type=0 m1i1=200 m1i2=29 m1i3=11 m1p1=0x8ace m1p2=0xa00bb00 m1p3=(nil) 
 main:116:SYSTASK is waiting for requests
 main:120:mnx_receive rcode=0
 
		Sin embargo el proceso 29 (REMOTO) SIGUE VIVO !!!!
VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2671    0    0   80  27342  27342  27342  27342 systask        
 0   -3     -3     -1    1 1800    0  27342  27342  27342  27342 systask        
 0   -2     -2   2670    0    8   20  31438  27342  27342  27342 systask        
 0    0      0   2673    0    8   A0     -3  27342  27342  27342 pm         PM0 ESTA ESPERANDO RESUSPUESTA DE SYSTASK1       
 0   29     29     -1    1 1008    0      0  27342  27342  27342 test_rexec  
 
ERROR:
		CLARAMENTE EL PROBLEMA ESTA EN SYSTASK1, do_exit()
		rcode = mnx_unbind(vm_ptr->vm_vmid, proc_ep);

		
			WUNLOCK_PROC(proc_ptr);
			WUNLOCK_VM(vm_ptr);
			UNLOCK_TASK(task_ptr);
			do {
				/* wait until unbind() clears p_lpid */
				rcode = wait_event_interruptible(caller_ptr->p_wqhead,(other_pid != proc_ptr->p_usr.p_lpid));
				MOLDEBUG(INTERNAL,"other_pid=%d pid=%d\n",other_pid, proc_ptr->p_usr.p_lpid);
				}while(rcode != 0);
			LOCK_TASK(task_ptr); <<<< SE SUPONE QUE task esta muerto, por lo tanto esto va a fallar.
			WLOCK_VM(vm_ptr);
			WLOCK_PROC(proc_ptr);

		
NODE1:
			
[  109.326925] DEBUG 2607:mm_unbind:1319: vmid=0 proc_ep=29
[  109.326986] DEBUG 2607:mm_unbind:1330: RLOCK_VM vm=0 count=0
[  109.327046] DEBUG 2607:mm_unbind:1342: LOCK_TASK pid=2607 count=0
[  109.327058] DEBUG 2607:mm_unbind:1343: RLOCK_PROC ep=29 count=0
[  109.327058] DEBUG 2607:mm_unbind:1344: RUNLOCK_VM vm=0 count=0
[  109.327058] DEBUG 2607:mm_unbind:1384: RUNLOCK_PROC ep=29 count=0
[  109.327058] DEBUG 2607:mm_unbind:1385: UNLOCK_TASK pid=2607 count=0
[  109.327058] DEBUG 2607:mm_unbind:1389: LOCK_TASK pid=2610 count=0
[  109.327058] DEBUG 2607:mm_unbind:1397: WLOCK_VM vm=0 count=0
[  109.327058] DEBUG 2607:mm_unbind:1398: WLOCK_PROC ep=29 count=0
[  109.327058] DEBUG 2607:do_unbind:1453: nr=29 endp=29 vmid=0 flags=8 misc=20 lpid=2610 nodeid=1 nodemap=2 name=systask 
[  109.327058] DEBUG 2607:do_unbind:1485: Caller endpoint=-3 lpid=2607
[  109.327058] DEBUG 2607:do_unbind:1497: Sending SIGPIPE to pid=2610 <<<<<<<<<<<<<<< envia SIGPIPE 
[  109.327058] DEBUG 2607:mm_unbind:1400: WUNLOCK_PROC ep=29 count=0
[  109.327058] DEBUG 2607:mm_unbind:1401: WUNLOCK_VM vm=0 count=0

ademas del SIGPIPE tendria ver su estado.
	Si esta esperando algo,hacerle un wakeup.
	LOCAL_PROC_UP(proc, EMOLSHUTDOWN)	

El proceso MURIO, pero no se hizo el exit_unbind() !!!!!
			
---------------------------------------------------------------------------------------------------------------
20160307:
		Se hizo la lib/config
		Se hizo server/test/test_config
		para leer archivos de configuracion 

---------------------------------------------------------------------------------------------------------------
20160310:
		POR FIN FUNCIONO!! 
		Se corrigieron varios errores pequeños en SYSTASK do_exit.c y PM forkexit.c
		La clave sin embargo esta en mm_unbind y do_unbind y proxies_unbind
		Estrategia:
			Cuando un proceso LOCAL (SYSTASK) hace el unbind de otro proceso LOCAL 
			le envia un SIGNAL SIGPIPE.
			Luego verifica el estado del proceso, si tiene p_rts_flags != 0 
			hace un LOCAL_PROC_UP para que efectivamente se pueda enviar el SIGNAL		
			El proceso hace su propio UNBIND
			y el proceso demandante espera que este termine haciendo wait_event_interruptible()
			
		
---------------------------------------------------------------------------------------------------------------
20160312:
		Se soluciono (??) OTRO problema del unbind() que se cuelgaba cuando se hacia test_unbind 
		dado que el proceso ejecutante NO ESTA BINDEADO. 
		el problema era el mm_put_task_struct.
	
	ERROR:	CONFIRMADO loops/loop_mol_ipc4 funciona MAL 
		Revisar codigo de rcvrqst y reply
		
---------------------------------------------------------------------------------------------------------------
20160313:	
		Se arreglo rcvrqst y reply. Se incorporaron en los scripts de verificacion test y loops
		tanto LOCALES como REMOTOS.
		
---------------------------------------------------------------------------------------------------------------
20160319:
		Se implemento el rexec con pasaje de argumentos (hasta MOL_MAX_ARG = 10)
		Se detecto durante el desarrollo que cuando una SYSTASK informa a otra de una syscall que le competa
		tal como BINDPROC, FORK, EXIT, MIGRATE, haciendo un mcast_xxxx

NODE0: 
 mcast_exit_proc:151:nr=9 endp=9 vmid=0 flags=8 misc=20 lpid=2740 nodeid=0 nodemap=1 name=demonize 
 do_exit:62:UNBIND proc_ep=9
 slots_loop:1748:sender=#0.0#node0 Private_group=#0.0#node0 vm_name=VM0 service_type=32 SP_bytes=36 
 slots_loop:1761:message from #0.0#node0, of type 20, (endian 0) to 1 groups (36 bytes)
 slots_loop:1809:source=0 type=1 m3i1=9 m3i2=9 m3p1=0x60 m3ca1=[./demonize]
 sp_syscall:1465:source=0 m_type=1 p_nr=9 p_endpoint=9
 get_uptime:205:tv_sec=5110 tv_nsec=486123628 uptime=511048 
 clock_handler:108:clockTicks=100 next_timeout=2147483647 realtime=511048 
 do_exit:69:proc_nr=9 free_slots=157
 tmrs_clrtimer:17:prev_time=0
 do_exit:75:after nr=9 endp=9 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 main:153:REPLY msg:source=9 type=0 m1i1=9 m1i2=70 m1i3=8 m1p1=(nil) m1p2=0xbfe4edd8 m1p3=0xb7815020 
 mcast_exit_proc:151:nr=9 endp=9 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname  <<<<<<<<<<<<<< ??????

				
NODE1:
 sp_syscall:1465:source=0 m_type=1 p_nr=9 p_endpoint=9
 sp_syscall:1607:MOLEXIT p_nr=9 p_endpoint=9
 sp_syscall:1619:nr=9 endp=9 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 sp_syscall:1623:WARNING: slot is FREE
 
 
		Si la SYSTASK origen basa sus operaciones SUPONIENDO que los otros nodos YA RECIBIERON el BINDPROC por ejemplo
		puede que en los nodos remotos todavia no llego el BINDPROC y por lo tanto la operacion solicitada por
		la SYSTASK origen fallara retornando que el proceso remoto no esta bindeado.
		Para  ello se implemento entonces un wait en un event_wait llamado sys_syscall que cada vez
		que se envia un mcast_xxxx  se bloquea hasta recibir su propio mensaje de broadcast 
		cuando SLOTS recibe su propio BINDPROC, entonces hace un up del event wait.
		
		Tambien se incorporo el chequeo de la cantidad de miembros inicializados en el momento de hacer mcast_xxxx
	
PRUEBA LOCAL en NODE0:
	./demonize -l 0 12 12 /home/MoL_Module/mol-ipc/servers/test/test_rexec

	La primera vez que ejecuta SOLO (sin NODE1) funciona perfectamente.
	Cuando se ejecuta por segunda vez da error
	forkexit.c --do_bindproc: 377 Assertion mp_flags & 0001
	
	Cuando se ejecutan NODE0 y NODE1
	En NODE0 aparentemente todo OK
	En NODE1 NO se ve el proceso remoto, al terminar da
		slots_loop:1948 rcode=-324
---------------------------------------------------------------------------------------------------------------
20160320:		
	Se modifico PM para poder ejecutar demonize LOCAL
	Las flags del PM del proceso daban  IN_USE|ZOMBIE (en MINIX) por lo que quedaba como zombie
	y cuando se volvia a arrancar la ranura daba OCUPADA.
	
	TEMPORALMENTE se setean las flags=0 si es que es un proceso servidor. 
			es decir (proc_nr+vm_ptr->vm_nr_tasks) < (vm_ptr->vm_nr_sysprocs)
	
---------------------------------------------------------------------------------------------------------------
20160324:	
		Hay 3 tipos de BIND LOCALES
			- LCLBIND
			- REPLICA
			- BKUPBIND
		
		PREGUNTA: El mnx_wait4bind() en que momento debería retornar si se hace un BKUPBIND ??
			1- Cuando el BIND se efectiviza??
			2- Cuando el proceso se convierte de BKUPBIND a LCLBIND ??
		
		WAIT4BIND hacia un LOOP. Se incorporo a sched.h el campo de espera para el bind.
			wait_queue_head_t proc_wqhead;	/* wait queue to wait4bind */
	
FUNCIONAN OK, LCLBIND, SELFBIND, REPLICA_BIND-

ERROR:	BKUPBIND- Parece que se cuelga en 	wait4bind
	
---------------------------------------------------------------------------------------------------------------
20160325:

ERROR:	BKUPBIND- Parece que se cuelga en 	wait4bind
ERROR:	Cuando se dilata el BIND tambien fallan LCLBIND en wait4bind

Cuando se dilata el wait4bind el LCLBIND funciona pero el BKUPBIND sigue fallando

Se volvio a utilizar schedule_timeout() como metodo de espera, pero sigue fallando si el proceso tiene q esperar.

LISTO: FUNCIONA LOCAL-  SE MODIFICO wait4bind y main() de demonize 

---------------------------------------------------------------------------------------------------------------
20160326:
Se eliminaron  de task_struct
			wait_queue_head_t proc_wqhead;	/* wait queue to wait4bind */
			
Se cambio la protección de Linux TASK - Ahora se puede proteger como el resto de las estructuras.
#define USE_TASK_RWLOCK		0
#define USE_TASK_MUTEX		1
#define USE_TASK_SPINLOCK	2
#define USE_TASK_RCU		3

	Se incorporo como opcion de lock a los KERNEL RW_SEMAPHORES para todas las estructuras
		http://www.makelinux.net/books/lkd2/ch09lev1sec4

RESULTADO DE  TEST con RW_SEMAPHORES:
	- Locales:    	
					loop_mperf: OK
					loop_mpeer: se DETIENE (no se cuelga) antes de  loop_sr_mpeer 12
					loop_mthread:  se DETIENE (no se cuelga)
					loop_mol_ipc: OK
					loop_mol_size.sh: OK
					
	- Remotos:		TODOS OK
	- demonize:		LOCAL: OK
					REMOTO: OK
		
---------------------------------------------------------------------------------------------------------------
20160329:
		Se cambiaron algunos test locales 	(/loop)
	
---------------------------------------------------------------------------------------------------------------
20160402:
		Se terminaron todos los tests locales 	(/loop)
		Se comenzo con el script unificado REMOTO 
		rtest.sh <proxy_type> <lcl_nodeid> <nr_vms>

		
---------------------------------------------------------------------------------------------------------------
20160405:
		Se comenzo con el desarrollo de los test remotos 
		el loop_sr_client se "cuelga" el proceso cuando se lo ejecuta por segunda vez
		
---------------------------------------------------------------------------------------------------------------
20160416:	
		Se terminaron test REMOTOS que soportan MULTIPLES TRANFERENCIAS SIMULTANEAS
		
		NODO0						NODO1
		./rtest.sh t 0				./rtest.sh t 1-
		cd proxy/test				cd proxy/test
		loop_r-s_server				loop_sr_client
		loop_r-r_server				loop_sr_client
		loop_r-s_server				loop_s-r_client
		loop_r-n_server				loop_n-r_client			NO FUNKA BIEN 
		loop_copy_server			loop_copy_client
				


Se incorporaron a /procs/drvs/VM0/stats de transferencias de nro de mensajes y nro de copy data
Tambien Estadistiscas a los proxies y nodos

---------------------------------------------------------------------------------------------------------------
20160416:	

	Se probo  mnx_getep() usando test_getep.c antes del bind retorna  EMOLNOTBIND


---------------------------------------------------------------------------------------------------------------
20160423:		

		Se incorpora a los proxies (TCP en principio) el AUTOMATIC REMOTE ENDPOINT BINDING
		Cuando un server reside en un NODO0 y llega la peticion de un cliente de NODE1
		puede que el NODO0 no lo tenga a cliente como reconocido y el PUT2LCL del RPROXY daria error:
			EMOLNOTBIND:	/* the slot is free */			
			EMOLENDPOINT:	/* local node registers other endpoint using the slot */		
			EMOLNONODE:	/* local node register other node for this endpoint   */		
		El RPROXY entonces puede hacer el rmtbind() en lugar de que sea automatico y reejecutar el put2lcl
		Se supone que la SYSTASK tiene la tabla de SLOTS, por lo tanto sabe si un slot pertenece o no a un nodo
		el RPROXY deberia entonces consultar a la SYSTASK respecto a si ese endpoint pertenece al nodo de donde proviene
		el mensaje. De ser asi, lo bindea con rmtbind()
					
		PROBLEMA:	Para hacer una kernel call a la SYSTASK, el PROXY deberia tener endpoint.
		SOLUCION:	Se usa el endpoint "PROXY" == "IDLE". 
					Como multiples RPROXYes pueden usarse en la misma VM, cada RPROXY debe bindearse, operar y desbindearse

		PROBLEMA:	Como probar esto dado que solo trabaja con procesos registrados.
					Debe existir un SERVER en NODO0 y un CLIENT en NODO1
					
		SOLUCION:	Hacer un server generico y un cliente generico
					En NODO0 arrancar server 
					En NODO1 arrancar client
				
			1) Arrancar en NODE0	./minix.sh 0	
				Deberia estar arrancado el test_server en el endpoint 10 
				y en forma remota test_client en endpoint 11
				El test_client arranca en endpoint 11 pero luego hace un mol_fork y crea un hijo con 
				endpoint dinamico > nr_sys_procs que trata de comunicarse con el endpoint 10
	
MODOS DE EJECUCION DE PROCESOS:
		SERVER LOCAL:  	demonize -l <vmid> <endpoint> <pid> <path to executable> [args..]
		SERVER REMOTO:	demonize -R <vmid> <endpoint> <nodeid> <path to executable> [args..]
		USER LOCAL:		init
		USER REMOTO:
		
QUE PASA CON INIT??		
		UN UNICO INIT?
		
QUE PASA CON PM?
		UN UNICO PM ?

---------------------------------------------------------------------------------------------------------------
20160430:		
		
ERROR: El demonize funciona bien con argumentos.- FUNCIONA MAL SIN ARGUMENTOS
SOLUCIONADO!!

---------------------------------------------------------------------------------------------------------------
20160503:
	Se agrego el campo timeout_ms a mnx_get2rmt()
	Se cambio la estructura cmd_t
		se agrego numero de secuencia c_seq
		se agrego una union para compartir almacenamiento entre piggybacking o campos requeridos por vcopy

	El header podrá entonces contener:
		- NADA  
		- Mensaje
			CMD_SEND_MSG
			CMD_NTFY_MSG
			CMD_SNDREC_MSG
			CMD_REPLY_MSG
		- info requerida para vcopy (=> PAYLOAD)
			CMD_COPYIN_DATA
			CMD_COPYOUT_RQST
			CMD_COPYLCL_RQST
			CMD_COPYRMT_RQST
			CMD_COPYIN_RQST
			CMD_COPYOUT_DATA
		
---------------------------------------------------------------------------------------------------------------
20160507:
		
	Se Modifico el proxy TCP (tproxy) y las syscalls mm_get2rmt() y mm_put2lcl() para que el mensaje forme 
	parte del header en una union con los parametros para la copia vcopy_t.
	Se agregaron nuevos campos al header 
		unsigned long c_flags;			/* generic field for flags filled and controled by proxies not by M3-IPC 	*/
		unsigned long c_snd_seq;		/* send sequence #  - filled and controled by proxies not by M3-IPC 	*/
		unsigned long c_ack_seq;		/* acknowledge sequence #  - filled and controled by proxies not by M3-IPC 	*/

	Se modificaron los scripts de proxy/test para que soporten multiples procesos.
		loop_vcopy_node0.sh
		loop_vcopy_node1.sh	
		loop_ipc_node1.sh	
		loop_ipc_node0.sh

	Se modificaron proxies de TCP, TIPC y UDP
	UDP permite la copia de datos < 65536 de lo contrario da error EMSGSIZE    90  /* Message too long */
	
---------------------------------------------------------------------------------------------------------------
20160509:	
		
ATENCION. ERROR EN COPY_KRN2USR, no funciona bien.

SOLUCION:!!!
		ATENCION COPY_KRN2USR solo copia bien si ambos buffers estan alineados !!!!!!!
		Se modifico copy_krn2usr para controlar alineacion

Se modifico get2rmt y put2lcl para que soporte copia desde MODULO PROXY de kernel despues de modificar
que el mensaje no esta mas en el payload sino en el header. 
		
Se modifico do_bindproc() para procesos de usuario remotos.

---------------------------------------------------------------------------------------------------------------
20160510:
	MODIFIQUE SLOTS
	
UDT:	Para debian SQUEZEE no viene, por lo tanto lo instale desde el udt.sdk.4.11.tar.gz
		en /home/udt
		alli tar -xvzf udt.sdk.4.11.tar.gz
		cd udt4
		
		luego para probar fui a ./app pero no localizaba la libudt.so que esta en el ./src por lo que modifique
		el Makefile
			LDFLAGS = -L../src ../src/libudt.so -lstdc++ -lpthread -lm
			#LDFLAGS = -L../src -ludt -lstdc++ -lpthread -lm		
	 	
		Ahora compila perfectamente y ejecuta perfectamente 
		./test
		./appserver y ./appclient
		./sendfile  y ./recvfile
		
---------------------------------------------------------------------------------------------------------------
20160513:
		Hice el proxy tcp_th_proxy.c que utiliza THREADs en lugar de procesos 
		Esto me permitirá usar primitivas de sincronizacion de threads mas sencillas para implementar REMOTE CLIENT BINDING.


PROBLEMA A RESOLVER:
		De que manera un cliente remoto es bindeado en el nodo local cuando solicita un servicio a un server local ?

CASO1:			NODO0					NODO1
				Server0					Client1
				PM0
		Como el PM0 creo el Client1, entonces lo debio dar de alta en NODO0, por lo tanto Server0 lo reconoce
				
CASO2:			NODO0					NODO1
				PM0						Server1
				Client0
		Server1 fue creado por PM0 usando "demonize" en NODO1, por lo tanto SYSTASK0 ,  SYSTASK1 y KERNEL1 => No hay problemas
		Client0 es creado por PM0 usando fork() en NODO0, por lo tanto SYSTASK0 y KERNEL0 lo reconocen
		El problema se da cuando Client0 quiere solicitar un servicio de Server1
		AQUI esta el problema!



POSIBLE SOLUCION:
		Que el RPROXY1 cuando recibe mensaje desde Client0 e intenta hacer un put2lcl() falla por alguno de estos casos
			case EMOLNOTBIND:	/* the slot is free */	
			case EMOLENDPOINT:	/* local node registers other endpoint using the slot */
			case EMOLNONODE:	/* local node register other node for this endpoint   */
		Debería fabricar un pseudo mensaje de PM0 tipo CMD_SNDREC_MSG a la SYSTASK1 
		local correspondiente a la VM 
		Como parametros debe pasar:
			destino: 	SYSTASK1 local: SYSTASK(localnodeid)
			source:		PM remoto (dado que se requiere que la respuesta de SYSTASK pase por SPROXY)
			m_type:		SYS_BINDPROC (Esto require una modificacion de SYS_BINDPROC para soportar procesos remotos de usuario)
			M3_ENDPT:	Client0 endpoint obtenido del cmd_t
			M3_LPID:	Node0 obtenido del cmd_t
			M3_OPER: 	RMT_BIND
			m3_ca1:		"RClient%d", node
		luego el RPROXY1 se queda a la espera en un semaforo o esperando en un evento (TEMPORIZADO).
	
		Cuando SYSTASK1 recibe el mensaje SYS_BINDPROC hace un BIND o MIGRATE segun si el slot esta vacio o no
		luego como fue engañada responde al PM0 remoto
			destino:	PM0  
			source: 	SYSTASK1 local: SYSTASK(localnodeid)
			m_type:		rcode
			M1_ENDPT:	endpoint
			M1_OPER: 	RMT_BIND
		ATENCION!!: Tambien hayque ignorar el ACKNOWLEDGE !!!!
		Al enviarse este mensaje por el SPROXY1, este es detectado, NO ENVIADO y se despierta RPROXY1

		Luego RPROXY0 reintenta el segundo mnx_put2lcl()
			
		ATENCION!!!: WARNING: Podría haber un problema en caso de que el PM0 envie un mensaje SYS_BINDPROC tipo RMT_BIND a SYSTASK1
				Por ahora no parece ser necesario.
							
ATENCION: En el codigo de los proxies RECEIVER de remote client bind RMT_CLIENT_BIND
el proxy requiere bindearse a la VM para poder hacerle consultas a la SYSTASK acerca 
de quien es el dueño del SLOT. PEROOOOOO el proxy ya esta bindeado con mnx_proxy_bind.
Analizar si esto afecta o no.	
ADEMAS: Cuando se hace check_caller se busca en la tarea.

---------------------------------------------------------------------------------------------------------------
20160515:
		PRUEBA DE  REMOTE CLIENT BINDING.
		Se cambio minix.sh para que use tcp_th_proxy (opcion h)
NODE0:	
		# ./minix.sh h 0
		esto arranca SLOT0, SYSTASK0 y PM0
NODE1:	
		# ./minix.sh h 1
		esto arranca SLOT1, SYSTASK1

NODE0:	
		# ./demonize -R 0 12 1 /home/MoL_Module/mol-ipc/commands/demonize/single_server
		
	los SLOTS donados por NODE0 a NODE1 son
 slots_loop:1790:source=0 type=2 dest=1 mAnr=4 mAia[0]=96 mAia[1]=97 mAia[2]=98 mAia[3]=99
  Es decir los slots 96,97,98 y 99 no pueden usarse desde NODE0 por eso se elige el endpoint 100		
		
		probando ahora desde NODE0 ./test_sendrec 0 100 29

SECUENCIA CORRECTA 
NODE1: RPROXY recibe el mensaje desde endpoint 100 hacia 29
 pr_receive_header:120:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 pr_receive_header:122:RPROXY: cmd=3 vmid=0 src=100 dst=29 snode=0 dnode=1 rcode=0 len=0
 pr_process_message:150:RPROXY:cmd=3 vmid=0 src=100 dst=29 snode=0 dnode=1 rcode=0 len=0
 pr_process_message:169:RPROXY: source=100 type=1 m1i1=1 m1i2=2 m1i3=3 m1p1=(nil) m1p2=0x8049da0 m1p3=0x4 
 pr_process_message:180:RPROXY: put2lcl
 pr_process_message:187:RPROXY: REMOTE CLIENT BINDING rcode=-310 (EMOLNOTBIND)
 pr_process_message:218:RPROXY: cmd=3 vmid=0 src=0 dst=-3 snode=0 dnode=1 rcode=0 len=0
 pr_process_message:221:RPROXY: source=0 type=1568 m3i1=0 m3i2=100 m3p1=0x2 m3ca1=[RClient0]
 pr_process_message:224:RPROXY: mnx_put2lcl RMT_BIND to SYSTASK
 
 Se supone que SYSTASK hizo el bindproc que le "ordeno" PM0 y le responde
 ps_start_serving:514:SPROXY: 2609 cmd=1 vmid=0 src=-3 dst=0 snode=1 dnode=0 rcode=0 len=0
 ps_start_serving:527:SPROXY: source=-3 type=0 m1i1=100 m1i2=2 m1i3=2 m1p1=0x696c4352 m1p2=0x30746e65 m1p3=(nil) 
 ps_start_serving:532:SPROXY: reply for REMOTE CLIENT BINDING - Discard it ret=0 <<<< Tambien despierta a RPROXY
 ps_start_serving:494:SPROXY 2609: Waiting a message
 
 pr_process_message:232:RPROXY: Wait until SYSTASK replies through SPROXY

 ps_start_serving:514:SPROXY: 2609 cmd=1 vmid=0 src=-3 dst=0 snode=1 dnode=0 rcode=0 len=0
 ps_start_serving:527:SPROXY: source=-3 type=0 m1i1=100 m1i2=2 m1i3=2 m1p1=0x696c4352 m1p2=0x30746e65 m1p3=(nil) 
 ps_start_serving:532:SPROXY: reply for REMOTE CLIENT BINDING - Discard it ret=0
 ps_start_serving:494:SPROXY 2609: Waiting a message
 
 RPROXY Fabrica un falso SEND_ACK para SYSTASK 
 pr_process_message:255:RPROXY: cmd=8193 vmid=0 src=0 dst=-3 snode=0 dnode=1 rcode=0 len=0
 pr_process_message:258:RPROXY: mnx_put2lcl CMD_SEND_ACK to SYSTASK
 
 Luego reintenta con el put2lcl del ahora bindeado endpoint 100
 pr_process_message:266:RPROXY: PUT2LCL retry after REMOTE CLIENT BINDING
 pr_process_message:268:RPROXY: cmd=3 vmid=0 src=100 dst=29 snode=0 dnode=1 rcode=0 len=0
 pr_process_message:270:RPROXY: source=100 type=1 m1i1=1 m1i2=2 m1i3=3 m1p1=(nil) m1p2=0x8049da0 m1p3=0x4 
 
 TODO OK, el server en endpoint 29 le responde a 100 
 ps_start_serving:514:SPROXY: 2609 cmd=1 vmid=0 src=29 dst=100 snode=1 dnode=0 rcode=0 len=0
 ps_start_serving:536:SPROXY: CMD_SEND_MSG dont match REMOTE CLIENT BINDING ret=2
 ps_send_remote:465:SPROXY:cmd=1 vmid=0 src=29 dst=100 snode=1 dnode=0 rcode=0 len=0
 ps_send_header:401:SPROXY: send header bytesleft=88
 ps_send_header:419:SPROXY: socket=3 sent header=88 
 		
		
PROBLEMA: LA SYSTASK DETECTA 0x2001 PARA EL ESTADO DEL PROCESO
		do_bindproc:58:nr=100 endp=100 vmid=0 flags=2001 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		(BIT_SLOT_FREE | BIT_RMTOPER) <<<< que tiene que ver BIT_RMTOPER ?????????
SOLUCIONADO, ahora no aparece mas !!

	
---------------------------------------------------------------------------------------------------------------
20160516:
 
NODE0:	
		# ./minix.sh h 0
		esto arranca SLOT0, SYSTASK0 y PM0
NODE1:	
		# ./minix.sh h 1
		esto arranca SLOT1, SYSTASK1

NODE0:	
		# ./demonize R 0 13 1 /home/MoL_Module/mol-ipc/servers/test/single_server
		
PROBLEMA EN DEMONIZE: se supone que single_server deberia tener el endpoint 12 pero tiene el 29: PORQUE ??
		
PROBLEMA SOLUCIONADO!! 
			En ningun momento le pasaba el endpoint del proceso a arrancar en forma remota

ERROR: Sigue loop_r-r_server
		ERROR MAIN SERVER mnx_rmtbind 1: process client1 on node1 ret=-337
PROBLEMA SOLUCIONADO!! 			
			
HICE una prueba de upgrade de WHEEZE A SQUEEZE 	
			FALLA AL COMPILAR EL KERNEL
			FALLA AL COMPILAR EL MODULO MO-IPC
			FALLA AL COMPILAR EL MODULO KSOCKET

	
---------------------------------------------------------------------------------------------------------------
20160517:

COMIENZO DE INSTALACION DE DEBIAN JESSIE 
	VM: MOL-IPC NODE0 NEW
	Imagen ISO: debian-8.4.0-i386-netinst.iso
	apt-get update
	apt-get upgrade
	apt-get install joe jed mc wget lynx git sudo
	apt-get install module-assistant
	apt-get intall linux-kernel-headers libncurses5-devel
	dpkg -l | grep headers => esto me da la version de headers soportada
	bajar el kernel correspondiente a esa version
	To enable SSH login for a root user on Debian Linux system you need to first configure SSH server. Open /etc/ssh/sshd_config and change the following line:
		FROM:
		PermitRootLogin without-password
		TO:
		PermitRootLogin yes
	cd /usr/src/
	tar xvz linux.xxxx.tar.gz
	ln -s linux.xxxx linux
	cd linux
	make menuconfig
	make clean
	make bzImage
	make modules
	make modules_install
	make install
	configurar red
	apt-get install bootcd
	
	intento compilar ksocket
		se compila bien y se carga bien.
		sus simbolos aparecen /proc/kallsyms
		
	intengo compilar samples de ksocket
		y “no symbol version for xxxxxx ” siendo xxxxxx simbolos definidos por ksocket	
		hice cat > Module.symbols >> /lib/modules/$(shell uname -r)/build/Module.symbols
		y luego ejecute depmod -a
		Esto soluciono varias de las referencias pero me quedo una no resulta. kernel_thread
		
	Esto sucede porque kernel_thread no esta EXPORTADA!!
	http://stackoverflow.com/questions/32965535/kernel-symbol-marked-with-t-in-proc-kallsyms-is-not-exported
	
	To use kernel symbols that are global, but not exported (such as the machine_power_off symbol that you mention), you can use kallsyms_lookup in your module code:
	#include <linux/kallsyms.h>

	NO FUNCIONA IGUAL AUNQUE CAMBIE EL kernel_thread por kthread_load+kthread_run
	
	
	
static void (*machine_power_off_p)(void);
machine_power_off = (void*) kallsyms_lookup_name("machine_power_off");
Now you can call the machine_power_off function via the machine_power_off_p pointer:

(*machine_power_off_p)();

	
	
+ configurar kernel 
+ compilar kernel
+ migrar M3-IPC
+ Probar M3-IPC
	+ LOCAL
	+ REMOTO
+ INSTALAR TIPC
+ INSTALAR SPREAD
+ INSTALAR ksocket
+ Probar proxy de kernel

---------------------------------------------------------------------------------------------------------------
20160518:
	Trabajo en la generacion de reportes de SLOST (slot-report.pl)
	Se genera un archivo CSV donde se registran los eventos de interes a partir del log.

---------------------------------------------------------------------------------------------------------------
20160520:
		Pude cambiar el sources.list para actualizar y bajar soft de debian
		
		Pude instalar y probar PICOTCP segun
		https://github.com/tass-belgium/picotcp/wiki/Setting-up-the-environment#building-picotcp
		
		Se alinearon los mensajes en PM
		EXTERN message m_in  __attribute__((aligned(0x1000)));		/* the incoming message itself is kept here. */
		EXTERN message m_out  __attribute__((aligned(0x1000)));		/* the outgoing message is kept here. */		

		Tambien se alinearon mensajes en MOLLIB Y SYSLIB
		message m __attribute__((aligned(0x1000)));

		COMENCE CON LA MIGRACION DE INET Y ETH
		Se creo el script minix_ip.sh que arranca ETH y INET, todo en forma local

---------------------------------------------------------------------------------------------------------------
20160521:		
		ERROR: Hay problemas en el demonize con el pasaje de argumentos en ejecucion LOCAL	
		SOLUCIONADO!!!
		CONTINUE CON LA MIGRACION DE INET Y ETH

---------------------------------------------------------------------------------------------------------------
20160522:
		CONTINUE CON LA MIGRACION DE INET Y ETH

---------------------------------------------------------------------------------------------------------------
20160523:

	incorporar en minix.sh el montaje
		mount -t debugfs none /sys/kernel/debug

---------------------------------------------------------------------------------------------------------------

PROBLEMA: Como solucionar el tema del nombre con que aparece el programa al ejecutar demonize???
TODO:	Ver si podemos compartir la tabla de slots del kernel con la SYSTASK!!

Se probo interface con MMAP
https://github.com/blue119/kernel_user_space_interfaces_example
https://github.com/blue119/kernel_user_space_interfaces_example
dio errores 
se creo el directorio kui (kernel user interface)
fue modificado por PAP
	El unico programa que funciona es 
		mmap_bart_tanghe_dan_hordern.c
	el cual fue renombrado por simplicidad a 
		mmap_bart.c

	EN MAKEFILE solo se compila  mmap_bart
		#obj-m += sysctl.o
		obj-m += mmap_bart.o <<<<<<
		#obj-m += mmap_kernel2.o
	compilar con make 
	
	root@node0:/home/kui# mount -t debugfs none /sys/kernel/debug
	root@node0:/home/kui# insmod mmap_bart.ko
	root@node0:/home/kui# lsmod | grep bart
	mmap_bart               1485  0 
	root@node0:/home/kui# ./mmap_user 
	initial message: hello from kernel this is file: mmap_example
	changed message: hello from *user* this is file: mmap_example

	El resultado en dmesg 
		root@node0:/home/kui# dmesg
		/home/kui/mmap_bart.c, mmapexample_module_init
		[ 1460.861849] /home/kui/mmap_bart.c, my_open
		[ 1460.861866] /home/kui/mmap_bart.c, my_mmap
		[ 1460.861869] /home/kui/mmap_bart.c, mmap_open
		[ 1460.861987] /home/kui/mmap_bart.c, mmap_fault
		[ 1460.862337] /home/kui/mmap_bart.c, mmap_close
		[ 1460.862341] /home/kui/mmap_bart.c, my_close

---------------------------------------------------------------------------------------------------------------
20160524:		
		
 main:44:d_nr_vms=32 d_nr_nodes=32 d_nr_procs=189 d_nr_tasks=67 d_nr_sysprocs=96 
 main:46:local_nodeid=0
 init_systask:211:Binding to VM 0 with sys_nr=-2 sys_pid=2732
 init_systask:214:has binded with p_endpoint=-2 
 init_systask:227:Initialize the call vector to a safe default handler.
 init_systask:292:Allocated space for nr_procs=189 + nr_tasks=67 processes
 init_systask:298:Map kernel process table on /sys/kernel/debug/drvs/VM0/procs
 init_systask:305:open debug_fd=3
 init_systask:310:mmap -14 <= EMOLFAULT
 
 
[   51.064214] DEBUG 2692:proc_dbg_open:135: 
[   51.064252] DEBUG 2692:check_caller:157: caller_pid=2692 caller_tgid=2692
[   51.064264] DEBUG 2692:check_caller:193: WLOCK_PROC ep=-2 count=0
[   51.064269] DEBUG 2692:check_caller:222: WUNLOCK_PROC ep=-2 count=0
[   51.064326] DEBUG 2692:check_caller:225: vmid=0
[   51.064453] DEBUG 2692:check_caller:229: RLOCK_VM vm=0 count=0
[   51.064522] DEBUG 2692:check_caller:233: RUNLOCK_VM vm=0 count=0
[   51.064567] DEBUG 2692:check_caller:239: caller_pid=2692 
[   51.064663] DEBUG 2692:proc_dbg_open:143: RLOCK_PROC ep=-2 count=0
[   51.064721] DEBUG 2692:proc_dbg_open:145: caller's vmid=0
[   51.064784] DEBUG 2692:proc_dbg_open:146: RUNLOCK_PROC ep=-2 count=0
[   51.064891] DEBUG 2692:proc_dbg_open:149: d_name=[VM0] d_name=[VM0]
[   51.064977] DEBUG 2692:proc_dbg_open:154: caller's vmid=0

---------------------------------------------------------------------------------------------------------------
20160525:
		el archivo necesario para  mem_map_reserve() es <linux/wrapper.h> no esta en /usr/src/linux/include/linux	
		tuve que hacer un linked
			ln /usr/src/linux/drivers/staging/comedi/wrapper.h /usr/src/linux/include/linux/wrapper.h

		LISTO FUNCIONA !!!!	
 
		Probar si tambien funciona con VALLOC().
		FALLA. NO FUNCIONA CON VALLOC.

 

---------------------------------------------------------------------------------------------------------------
20160528:

SLOTS: ........................................................
run_20:  Se muestra como se contabiliza un DELAY de MENSAJE
Node[3](6501): Requesting slot donation

Node[4](6525): I'm next member. Replying with zero slots to Node#3
Node[4](6525): Donated_slots:0 requester:3 List: []

Node[3](6545): Donation of 0 slots from 4 to 3

Node[3](6557): Donation of 1 slots from 1 to 3
 
		267			3263		397572	42015	39			93694951	265	nodos
Time	 Requests	 Concurrent	 Fork	 Failed	 Delayed	 Used	 RespM	24
	Req/Fork			Failed/Fork				
	0,000671576			0,105678971				
								
	Multicast							
	3145							
	Multi/Fork							
	0,007910517							
........................................................
 

 M3-IPC: 	Se controla el tamaño del KALLOC 
			se incorpora un contador de open()
			Se modifican funciones de salida vm_end, vm_release 
			Se modifican funciones de salidad proc_dbg_close
						
 
https://kaiwantech.wordpress.com/2011/08/17/kmalloc-and-vmalloc-linux-kernel-memory-allocation-api-limits/ 
The upper limit (number of bytes that can be allocated in a single kmalloc request), is a function of:
the processor – really, the page size – and
the number of buddy system freelists (MAX_ORDER). (up to 2MAX_ORDER-1)
On both x86 and ARM, with a standard page size of 4 Kb and MAX_ORDER of 11, the kmalloc upper limit is 4 MB!
4096	0
8192	1
16384	2
32768	3
65536	4
131072	5 <<<<<<<<<<<<<<<<<
262144	6
524288	7
1048576	8
2097152	9
4194304	10
[   46.137012] DEBUG 2701:mm_vm_init:482: proctab_size=131072 order=5 MAX_ORDER=11
En función de esto, calcular el tamaño limite para la tabla de procesos.
 
	
	Se trata de incorporar macros de modules MOD_INC_USE_COUNT MOD_DEC_USE_COUNT
	http://people.netfilter.org/rusty/unreliable-guides/kernel-hacking/routines-module-use-counters.html		
	OBSOLETO: https://lwn.net/Articles/102959/
	Se reemplaza por try_module_get(THIS_MODULE);module_put(THIS_MODULE);
	https://gist.github.com/kerie/716772

---------------------------------------------------------------------------------------------------------------
20160529:	
		Se ELIMINO DE SYSTASK la tabla propia de kernel !!! y las funciones mnx_getprocinfo o mnx_getproctab.	
		PROBAR EJECUCION REMOTA!!
		
ERROR:	SLOTS: los request concurrentes deben ser respondidos por el NEXT_NODE. 
		Ahora no esta respondiendo entonces todos los otros nodos (con donacion 0) estan a la espera	
		CORREGIDO - FALTA PROBAR Y EJECUTAR
		
		TODO:	MAPEAR TABLA DE PRIVILEGIOS DEL KERNEL 
		REJECTED: La tabla de privilegios son solo punteros a la tabla de procesos (nr_sys_procs)
		
TODO: 	Incluir en la estructura SLOTS el nombre del proceso.
		
ERROR:	FALLA EJECUCION REMOTA - VOLVER A PROBAR !!!		
		El problema esta en el PM al cambiar sys_getproctab
 pm_isokendpt:40:endpoint=10 <<<<<<
 pm_isokendpt:42:*proc=10
 pm_isokendpt:43:kproc.p_endpoint=77 <<<<<

---------------------------------------------------------------------------------------------------------------
20160531:

	FUNCIONO DEMONIZE REMOTO con mappeo de tabla de procesos

	Hice varios XLS sobre SLOTS
	
		
---------------------------------------------------------------------------------------------------------------
20160605:
		
ERROR:
	Cuando cae el NODE0 quedan bindeados el PM0 y SYSTASK0  en NODE1
	Crear un estado o FLAG DISCONNECTED que mantiene el slot ocupado 
	pero que impide que se envien mensajes a  traves del proxy a uno de esos endpoints del NODE0

	SOLUCIONADO: Hay 2 momentos en que deben unbindearse las tareas del sistema		
		- sp_disconnect: PROBADO OK
		- sp_join, despues de un net_partition: esto indica que en realidad rebooteo. : PROBADO OK-
	
EN JOIN	
 sp_join:424:FSM_state=3000 new_member=0 primary_mbr=1 active_nodes=2 bm_active=3
 sp_join:459:member=1 FSM_state=3000
 sp_join:467:Is the new member previously considered as a member of other partition? new_mbr=0
 unbind_rmt_sysproc:592:nr=-2 endp=-2 vmid=0 flags=1800 misc=0 lpid=-1 nodeid=0 nodemap=1 name=remote 
 unbind_rmt_sysproc:594:nr=-2 endp=-2 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 unbind_rmt_sysproc:600:s_nr=33 s_endpoint=27342 s_flags=0 s_owner=1
 unbind_rmt_sysproc:592:nr=0 endp=0 vmid=0 flags=1800 misc=0 lpid=-1 nodeid=0 nodemap=1 name=remote 
 unbind_rmt_sysproc:594:nr=0 endp=0 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 unbind_rmt_sysproc:600:s_nr=35 s_endpoint=27342 s_flags=0 s_owner=1
		
EN DISCONNECT		
 unbind_rmt_sysproc:592:nr=-2 endp=-2 vmid=0 flags=1000 misc=0 lpid=-1 nodeid=0 nodemap=1 name=remote 
 unbind_rmt_sysproc:594:nr=-2 endp=-2 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 unbind_rmt_sysproc:600:s_nr=33 s_endpoint=27342 s_flags=0 s_owner=1
 unbind_rmt_sysproc:592:nr=0 endp=0 vmid=0 flags=1000 misc=0 lpid=-1 nodeid=0 nodemap=1 name=remote 
 unbind_rmt_sysproc:594:nr=0 endp=0 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 unbind_rmt_sysproc:600:s_nr=35 s_endpoint=27342 s_flags=0 s_owner=1
 sp_disconnect:1118:bm_donors=0 bm_pending=0	
	
---------------------------------------------------------------------------------------------------------------
20160606:
	continue con la migracion de INET 

---------------------------------------------------------------------------------------------------------------
20160611:
	Finalizada la migracion de INET, ahora hay que hacerla andar

	TODO: que cuando se hace un exec() el nombre del proceso se refleje en el kernel
	LISTO: Se creo mol_setpname() en PM y sys_setpname() en SYSTASK
			No se necesita interaccion con el kernel dado que SYSTASK accede a la memoria de kernel directamente
		
---------------------------------------------------------------------------------------------------------------
20160612:
	Continuo con la puesta en funcionamiento de INET. 	
	Ahora hay que hacer algo en ETH para que respondea cosas coherentes.
	
	http://www.os-forum.com/minix/net/
	http://www.cis.syr.edu/˜wedu/seed/Labs/IPSec/files/libtcpudp.tar
---------------------------------------------------------------------------------------------------------------
20160620:
		Continue con la migracion de INET y ETH.
		ATENCION: Parece que el Makefile de INET no esta correcto porque no borra en make clean los archivos
		de los subdirectorios.
		

INET:
 ipeth_main:95:de_state=0
 eth_ioctl:171:eth_ioctl fd=0 req=0x40146e10
 eth_ioctl:171:eth_ioctl fd=1 req=0x80486e12
 
---------------------------------------------------------------------------------------------------------------
20160625:
		Implemente lz4tcp_th_proxy utilizando LZ4

El algoritmo funciona bien. la compresion es MALA si se setean bloques con bytes en forma aleatoria.
con ALETORIOS:	 
	compress_payload:87:SPROXY: raw_len=65536 comp_len=65551
con REPETITIVOS_
	 compress_payload:87:SPROXY: raw_len=65536 comp_len=319
	
PROBLEMA:
		Intente programar SOCAT proxy con 2 programas por separado socat_sproxy y socat_rproxy para hacer
		node0> # socat_sproxy | socat - TCP:node1:3001 &
 		node0> # socat - TCP-LISTEN:node0:3000 | socat_rsproxy &
		
		El problema se da con     
			ret = mnx_proxies_bind(px.px_name, px.px_id, spid, rpid);

SOLUCION:
		node0> # LCLNODE=node0
		node0> # LCLPORT=3000
		node0> # RMTNODE=node1
		node0> # RMTPORT=3001
		node0> # PXID=1
		node0> # socat_sproxy | socat - TCP:$RMTNODE:$RMTPORT &
		node0> # SPID=$!
 		node0> # socat - TCP-LISTEN:$LCLNODE:$LCLPORT | socat_rproxy  &
		node0> # RPID=$!
		node0> # socat_up $RMTNODE $PXID $SPID $RPID
	
socat_up: debe hacer:
		ret = mnx_proxies_bind(px.px_name, px.px_id, spid, rpid);
		rcode = mnx_proxy_conn(px.px_id, CONNECT_SPROXY);
		rcode = mnx_proxy_conn(px.px_id, CONNECT_RPROXY);
		ret= mnx_node_up(px.px_name, px.px_id, px.px_id);	// Link a node withe a proxy pair

---------------------------------------------------------------------------------------------------------------
20160626:
		Se desarrollaron socat_sproxy, socat_rproxy y socat_up
		se modifico rtest.sh para soportar esta tipo de proxies
		el proxy RECEIVER de NODE0 queda conectado con el proxy SENDER de NODE1
		el otro par de proxies no se conecta
		Reemplace para probar socat por nc pero tampoco anda
		
---------------------------------------------------------------------------------------------------------------
TODO:

TRANSACCION 1:  JOIN->SYS_PUT_STATUS
	
	JOIN: 	Llega otro JOIN(new_mbr2) antes de recibir el SYS_PUT_STATUS
			new_mbr: no cambia su estado quedando a la espera del SYS_PUT_STATUS
			other_mbr: incorporan new_mbr2 a bm_waitsts 
			primary: envia UN SOLO SYS_PUT_STATUS
	
	PUT_STATUS:
			TODOS: incorporan a bm_init el nuevo init_mbr
					 borra al init_mbr de bm_waitsts
			init_mbr: cambia su estado a STS_REQ_SLOTS - tiene que heredar el bm_waitsts
			primary_mbr: envia UN SOLO SYS_PUT_STATUS

	DISCONNECT(disc_mbr):
			TODOS: remover a disc_mbr de bm_waitsts
			primary_mbr:  envia UN SOLO SYS_PUT_STATUS
	
	SYS_REQ_SLOTS:
			no afecta la transaccion
			
	SYS_DON_SLOTS: 
		Si afecta a la transaccion porque el SYS_PUT_PST que va a recibir el nuevo miembro que fue enviado por primary_mbr
		no contiene los cambios de propiedad que transporta este mensaje SYS_DON_SLOTS
		????????????????????????????????????????????????????
		POSIBLE SOLUCION: Incorporar los cambios de propietario en la tabla de slots local 
		hacer un merge de tablas entre la que llega del primary_mbr y la tabla local, dandole prioridad a la local 
		if( slot[i] == NO_PRIMARY_MBR) 
			slot[i] = tmp_slot[i]; /* reemplaza el elemento local por el recibido desde primary_mbr */
	
	PARTITION:
			TODOS: bm_waitsts &= bm_active
			primary_mbr: envia UN SOLO SYS_PUT_STATUS
			
	NET_MERGE: 		
			TODOS: 
				bm_init |= m_ptr->m_bm_init;
				bm_waitsts |= m_ptr->m_bm_waitsts;
			primary_mbr: 
				envia UN SOLO SYS_PUT_STATUS
		
TRANSACCION 2:  SYS_PUT_STATUS/SYS_REQ_SLOTS -> SYS_DON_SLOTS
		
	JOIN: 	Llega otro JOIN(new_mbr) antes de recibir el SYS_DON_SLOTS
			other_mbr: ignoran
			requester: bm_donors=0 dado que nadie mas va a donar wakeup
						si owned_slots == 0 => multicast SYS_REQ_SLOTS
			primary_mbr: incorporan new_mbr2 a bm_waitsts envia UN SOLO SYS_PUT_STATUS
	
	PUT_STATUS: 
			requester: vuelve a solicitar 

	DISCONNECT(disc_mbr):
			TODOS: recuperan sus slots que donaron a disc_mbr
			primary_mbr: envia UN SOLO SYS_PUT_STATUS
	
	PARTITION:
			TODOS: recuperan sus slots que donaron a los miembros de otras particiones
			TODOS:  bm_waitsts &= bm_active
			primary_mbr: envia UN SOLO SYS_PUT_STATUS
			requester: bm_donors &= bm_active 
						si owned_slots == 0 => multicast SYS_REQ_SLOTS
	
	SYS_REQ_SLOTS(other):
			no afecta la transaccion
		
	SYS_DON_SLOTS:
		????????????????????????????????????????????????????
			
	NET_MERGE: 		
			TODOS: 
				bm_init |= m_ptr->m_bm_init;
				bm_waitsts |= m_ptr->m_bm_waitsts;
			primary_mbr: envia UN SOLO SYS_PUT_STATUS
			si owned_slots == 0 => multicast SYS_REQ_SLOTS	
			
TRANSACCION 3: sp_net_merge -->sp_merge_status

	JOIN: 	
	
	PUT_STATUS: 

	DISCONNECT:
	
	PARTITION:
	
	SYS_REQ_SLOTS:
	
	SYS_DON_SLOTS:
		????????????????????????????????????????????????????
		
	NET_MERGE: 		
 		
		
---------------------------------------------------------------------------------------------------------------		
20160702:
	SLOTS: Hice varias modificaciones y al menos parece que funciona. 

LWIP_TAP:
		https://github.com/takayuki/lwip-tap
		Clonee el sitio con GIT segun se indica en el README.
		Por las dudas tengo un zip lwip-tap.zip (OJO- esta incompleto)
		
		Funciona perfectamente el PING al igual que el servidor HTTP !!!!!
		
root@node0:/home/MoL_Module/lwip-tap# ip tuntap add dev tap0 mode tap
root@node0:/home/MoL_Module/lwip-tap# ip link set dev tap0 up
root@node0:/home/MoL_Module/lwip-tap# ip addr add 172.16.0.1/24 dev tap0
root@node0:/home/MoL_Module/lwip-tap# ./lwip-tap -H -i name=tap0,addr=172.16.0.2,netmask=255.255.255.0,gw=172.16.0.1
El -H arranca el server web

---------------------------------------------------------------------------------------------------------------		
20160702:

		SOCAT PROXY: No funciono ni en NODO0, ni NODO1 ni en ambos simultaneamente.
		
TODO:	Revisar SOCAT PROXY
El SPROXY no BINDEA porque el PID que toma es el de SOCAT en lugar de SOCAT_SPROXY
[  180.538017] DEBUG 2691:mm_proxies_bind:1863: nr=0 endp=27342 vmid=-1 flags=0 misc=1 lpid=2689 nodeid=1 nodemap=0 name=socat 
[  180.538017] DEBUG 2691:mm_proxies_bind:1864: WUNLOCK_TASK pid=2689 count=0
[  180.538017] DEBUG 2691:mm_proxies_bind:1866: WLOCK_TASK pid=2687 count=0
[  180.538017] DEBUG 2691:init_proc_desc:255: p_name=$noname vmid=-1
[  180.538017] DEBUG 2691:mm_proxies_bind:1883: nr=0 endp=27342 vmid=-1 flags=0 misc=1 lpid=2687 nodeid=1 nodemap=0 name=socat_rproxy 

Eliminar socat_up en incorporarlo en socat_sproxy.
ejecutar socat_rproxy antes que socat_rproxy

shut-null
When one address indicates EOF, socat will send a zero sized packet to the write channel of the other address to transfer the EOF condition. This is useful with UDP and other datagram protocols. Has been tested against netcat and socat with option null-eof.

---------------------------------------------------------------------------------------------------------------		
20160703:
	Errores en PROXY UDT de alumnos al compilar
	Problema de C++ al interpretar funciones inline de CADA
	
	Se cambio por errores stub_syscall.h en el pasaje de parametros.
	Se cambio proxy_usr.h 
		
---------------------------------------------------------------------------------------------------------------		
20160707:
Libreria de funciones stub4cpp que pueden ser invocadas desde C++ que invocan a funciones inline
		rcode = mnx_put2lcl(p_header, p_payload);
		ret = mnx_bind(PROXY,p_header->c_vmid);
		mnx_unbind( p_header->c_vmid, PROXY);	
        rcode = mnx_proxy_conn(px.px_id, CONNECT_RPROXY);
		rcode = mnx_wait4bind_T(RETRY_US);
		ret = mnx_get2rmt(p_header, p_payload);       
		local_nodeid = mnx_getdrvsinfo(&drvs);
		ret = mnx_proxies_bind(px.px_name, px.px_id, spid, rpid);
		ret= mnx_node_up(px.px_name, px.px_id, px.px_id);	

---------------------------------------------------------------------------------------------------------------		
20160708:

TODO:	probar SOCAT con nuevos parametros.
		Se cambiaron algunos parametros ver rtest.sh
		el proxy receiver funciona bien. Esta recibiendo los CMD=0 tipo keepalive
 socat_rproxy.c:pr_process_message:127:RPROXY: About to receive header
 socat_rproxy.c:pr_receive_header:85:RPROXY: 
 socat_rproxy.c:pr_receive_header:93:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 socat_rproxy.c:pr_receive_header:95:RPROXY: cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 socat_rproxy.c:pr_process_message:130:RPROXY:cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 socat_rproxy.c:pr_process_message:127:RPROXY: About to receive header

		Se ve que proxy sender hace el mnx_get2rmt pero el socat no lo esta enviando.
		AHORA lo envia. faltaba el fflush(stdout)
		
		AHORA FUNCIONA BIEN pero en algun momento se cuelga porque no se hizo el bind del destination		
			NODO0: tcp_proxy (por procesos, sin hilos threads)
			NODO1: socat 
				
 pr_process_message:133:RPROXY: About to receive header
 pr_receive_header:102:socket=4
 pr_receive_header:108:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 pr_receive_header:110:RPROXY: cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:136:RPROXY:cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:155:RPROXY: source=1 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
 pr_process_message:166:RPROXY: put2lcl
 pr_start_serving:256:RPROXY: Message processing failure [-310] <<<<< 
 pr_process_message:133:RPROXY: About to receive header		
		
			NODO0: tcp_th_proxy (por  hilos threads)
			NODO1: socat 
			RESULTADO; PEOR, se cuelga 	
		
			NODO0: socat
			NODO1: socat 		
			RESULTADO:
					test_ipc_nodeX.sh: PERFECTO!!!!
					test_vcopy_nodeX.sh: PERFECTO!!!!
			ATENCION: se corrio con los flags en el sender -v -x que envian la salida a STDERR
			Eliminando -v -x 
					test_ipc_nodeX.sh: falla Message processing failure [-310] <<<<<
					test_vcopy_nodeX.sh: PERFECTO!!!!
			El problema esta en los tests que no sincronizan adecuadamente

---------------------------------------------------------------------------------------------------------------
20160710
			
		Se probo el bridge para luego probar el proxy. Se hizo el script start_tap.sh 
		En este caso en NODE0:
			eth0: 192.168.1.100
			br0: 172.16.0.1
			tap0: 172.16.0.1 gw 172.16.0.1
			tap1: 172.16.0.2 gw 172.16.0.1
			
		Topologia:
							NODE 0									NODE1
			tap0 -----br0--linux_routing----eth0 ===================eth0
			tap1-------|


#!/bin/bash
if [ $# -ne 1 ]
then 
	echo "usage: $0 <lcl_nodeid>"
	exit 1 
fi
let rmt=(1 - $1)
echo 1 >  /proc/sys/net/ipv4/ip_forward
brctl addbr br0
read  -p "Configuring tap0. Enter para continuar... "
ifconfig br0 172.16.$1.1 netmask 255.255.255.0 
ip tuntap add dev tap0 mode tap
ip link set dev tap0 up 
brctl addif br0 tap0
read  -p "Configuring tap0. Enter para continuar... "
ip tuntap add dev tap1 mode tap
ip link set dev tap1 up
brctl addif br0 tap1
read  -p "Running lwip-tap on tap0.Enter para continuar... "
./lwip-tap -H -i name=tap0,addr=172.16.$1.2,netmask=255.255.255.0,gw=172.16.$1.1 &
read  -p "Enter para continuar... "
./lwip-tap -H -i name=tap1,addr=172.16.$1.3,netmask=255.255.255.0,gw=172.16.$1.1 &
read  -p "Adding new route. Enter para continuar... "
route add -net 172.16.$rmt.0 gw node$rmt netmask 255.255.255.0
read  -p "Running lwip-tap on tap1.Enter para continuar... "
brctl show
exit
# EN NODE1 hacer
# route add -net 172.16.0.0 gw node0 netmask 255.255.255.0 
			
		En este caso en NODE0:
			eth0: 192.168.1.100
			br0: 172.16.0.1
			tap0: 172.16.0.1 gw 172.16.0.1
			tap1: 172.16.0.2 gw 172.16.0.1


		En este caso en NODE1:
			eth0: 192.168.1.101
			br0: 172.16.1.1
			tap0: 172.16.1.1 gw 172.16.1.1
			tap1: 172.16.1.2 gw 172.16.1.1

			
		Topologia:
							NODE 0									NODE1
			tap0 -----br0--linux_routing----eth0 ===================eth0----linux_routing---br0----tap0
			tap1-------|																	 |-----tap1
			
		COMPILACION DE PROGRAMA INCLUIDO EN lwip-tap
		cree la carpeta \MoL_Module\lwip-tap\lwip-contrib\apps\m3ipc-proxy donde copie el contenido del httpserver 
		y lo modifique 
		Tambien modifique lwip-tap para que contemple con la opcion M a M3-IPC
		modifique el Makefile.in (tambien el Makefile, pero seguramente esto se destruye al ejecutar ./configure)
		ejecute ./configure
		luego ejecuta make depend y make dep
		finalmente make all
		FUNCIONO OK!!
		root@node0:/home/MoL_Module/lwip-tap# lynx 172.16.0.2:3000
		---------------------------------------------------------------------------------
		                                                                     Congrats!
                                      Welcome to our lwIP M3-IPC PROXY !
		This is a small test page, served by m3ipc-proxy-netconn.
		---------------------------------------------------------------------------------

		Probar que funcione con la libreria de M3-IPC
		Se inserto la llamada 
			local_nodeid = mnx_getdrvsinfo(&drvs);
			d_ptr=&drvs;
			SVRDEBUG(DRVS_USR_FORMAT,DRVS_USR_FIELDS(d_ptr));
		Por las dudas hacer Make depend
		Arrancar primero el drvs utilizando mol-ipc/rtest.sh 0
			y detener despues de arrancar el DRVS
		luego ejecutar start_tap.sh 0
		FUNCIONO OK!!!! 
		
		Enter para continuar...  
		m3ipc_proxy_netconn_thread:74:d_nr_vms=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 

		Adding new route. Enter para continuar...  
		m3ipc_proxy_netconn_thread:74:d_nr_vms=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
		
		---------------------------------------------------------------------------------
		                                                                        Congrats!
                                       Welcome to our lwIP M3-IPC PROXY!
		local_nodeid=0
		---------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------
20160711/12:
		Compile la parte del rproxy
		queda por resolver la forma en que se pasan parametros como:
			nodo_destino, ip_destino, ip_local
		En lwip-tap.c esta como se parsea el argumento "-i"
		El formato podria ser:
			-M rmt_nodeid=<nodeid>
		a partir del $nodeid se puede obtener el nombre $rmt_name = "node$nodeid"
		a partir del $rmt_name con gethostbyname deberia obtener la IP del nodo remoto
		a partir del $nodeid se puede obtener el nro de puerto destino $rmt_port = BASE_PORT + $nodeid
		a partir de $local_nodeid se puede obtener el nombre $lcl_name = "node$local_nodeid"
		a partir del $lcl_name con gethostbyname deberia obtener la IP del nodo local
		a partir del $local_nodeid se puede obtener el nro de puerto local $lcl_port = BASE_PORT + $nodeid
	 
---------------------------------------------------------------------------------------------------------------
20160711/12:
		Compile el sproxy
		No se puede resolver con  netconn_gethostbyname dado que utiliza DNS
		Por ahora la direccion IP va a estar hardcodeada 

	 rmt_nodeid=1
 m3ipc_proxy_init:691:d_nr_vms=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
 m3ipc_proxy_init:698:lcl_name=vnode0
 m3ipc_proxy_init:700:rmt_name=node1
 m3ipc_proxy_init:703:MAIN: pthread_create RPROXY
 m3ipc_rproxy_thread:601:RPROXY: Initializing...
 m3ipc_proxy_init:711:MAIN: RPROXY td_tid=2700
 m3ipc_proxy_init:714:MAIN: pthread_create SPROXY
 m3ipc_sproxy_thread:283:SPROXY SENDER proxy thread
 m3ipc_proxy_init:722:MAIN: SPROXY td_tid=2701
 m3ipc_proxy_init:732:px_id=1 px_flags=0 px_name=node1
 m3ipc_proxy_init:733:binded to (2701,2700)
 m3ipc_sproxy_thread:296:SPROXY: mnx_wait4bind_T  rcode=27342
 m3ipc_sproxy_thread:306:SPROXY: rcode=27342
 m3ipc_sproxy_thread:328:sdesc.td_header=0x9fd1000 sdesc.td_payload=0x9fd2000 diff=4096
name=tap0,addr=172.16.0.2,netmask=255.255.255.0,gw=172.16.0.1
 m3ipc_rproxy_thread:614:RPROXY: mnx_wait4bind_T  rcode=-61
 m3ipc_rproxy_thread:616:RPROXY: mnx_wait4bind_T TIMEOUT
 m3ipc_rproxy_thread:614:RPROXY: mnx_wait4bind_T  rcode=27342
 m3ipc_rproxy_thread:624:RPROXY: rcode=27342
 m3ipc_rproxy_thread:646:rdesc.td_header=0xb5201000 rdesc.td_payload=0xb5202000 diff=4096
 ps_connect_to_remote:70:SPROXY: Initializing...
 ps_start_serving:195:SPROXY SENDER proxy thread
 ps_start_serving:198:SPROXY: Waiting a message
 pr_process_message:433:RPROXY: About to receive header
Assertion "netconn_accept: invalid recvmbox" failed at line 464 in lwip/src/api/api_lib.c
	SOLUCIONADO la conexion no era conn sino newconn
	
NO PUDE HACER IMPRIMIR LAS IP
		ip_addr_debug_print(SOCKETS_DEBUG,&local_ip);

---------------------------------------------------------------------------------------------------------------
20160715/16:

rmt_nodeid=1
 m3ipc_proxy_init:693:d_nr_vms=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
 m3ipc_proxy_init:700:lcl_name=vnode0
 m3ipc_proxy_init:702:rmt_name=node1
 m3ipc_proxy_init:705:MAIN: pthread_create RPROXY
 m3ipc_rproxy_thread:602:RPROXY: Initializing...
 m3ipc_proxy_init:713:MAIN: RPROXY td_tid=2755
 m3ipc_proxy_init:716:MAIN: pthread_create SPROXY
 m3ipc_sproxy_thread:284:SPROXY SENDER proxy thread
 m3ipc_proxy_init:724:MAIN: SPROXY td_tid=2756
 m3ipc_proxy_init:734:px_id=1 px_flags=0 px_name=node1
 m3ipc_proxy_init:735:binded to (2756,2755)
name=tap0,addr=172.16.0.2,netmask=255.255.255.0,gw=172.16.0.1
 m3ipc_sproxy_thread:297:SPROXY: mnx_wait4bind_T  rcode=27342
 m3ipc_sproxy_thread:307:SPROXY: rcode=27342
 m3ipc_sproxy_thread:329:sdesc.td_header=0x9557000 sdesc.td_payload=0x9558000 diff=4096
 m3ipc_rproxy_thread:615:RPROXY: mnx_wait4bind_T  rcode=-61
 m3ipc_rproxy_thread:617:RPROXY: mnx_wait4bind_T TIMEOUT
 m3ipc_rproxy_thread:615:RPROXY: mnx_wait4bind_T  rcode=27342
 m3ipc_rproxy_thread:625:RPROXY: rcode=27342
 m3ipc_rproxy_thread:647:rdesc.td_header=0xb5301000 rdesc.td_payload=0xb5302000 diff=4096
 m3ipc_rproxy_thread:655:RPROXY: listen on port=3001
 ps_connect_to_remote:70:SPROXY: Initializing...
 ps_connect_to_remote:88:SPROXY: connect to port=3000
 ps_start_serving:196:SPROXY SENDER proxy thread
 ps_start_serving:199:SPROXY: Waiting a message
 pr_process_message:434:RPROXY: About to receive header
 ps_start_serving:205:SPROXY: Sending HELLO 
 ps_start_serving:218:SPROXY: cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 ps_send_remote:172:SPROXY:cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 ps_send_header:111:SPROXY: send header bytesleft=88
 ps_send_header:125:SPROXY: sent header=88 
 ps_start_serving:199:SPROXY: Waiting a message
 pr_receive_header:420:RPROXY: cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 pr_process_message:437:RPROXY:cmd=0 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 pr_process_message:434:RPROXY: About to receive header
 pr_receive_header:420:RPROXY: cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:437:RPROXY:cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:456:RPROXY: source=1 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
 pr_process_message:467:RPROXY: put2lcl
 ps_start_serving:218:SPROXY: cmd=1 vmid=0 src=0 dst=1 snode=0 dnode=1 rcode=0 len=0
 ps_start_serving:240:SPROXY: CMD_SEND_MSG dont match REMOTE CLIENT BINDING rcode=2
 ps_send_remote:172:SPROXY:cmd=1 vmid=0 src=0 dst=1 snode=0 dnode=1 rcode=0 len=0
 ps_send_header:111:SPROXY: send header bytesleft=88
 ps_send_header:125:SPROXY: sent header=88 
 ps_start_serving:199:SPROXY: Waiting a message
 m3ipc_rproxy_serve:582:RPROXY: Message succesfully processed.
 pr_process_message:434:RPROXY: About to receive header
 pr_receive_header:420:RPROXY: cmd=8193 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:437:RPROXY:cmd=8193 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:467:RPROXY: put2lcl
 m3ipc_rproxy_serve:582:RPROXY: Message succesfully processed.
 pr_process_message:434:RPROXY: About to receive header
 
 
 
 tcpdump -i eth0 -A  > tcpdump.txt
    NODE1--HELLO->NODE0
21:30:16.272872 IP node1.51596 > node0.3001: Flags [P.], seq 1:89, ack 1, win 5840, length 88
E...h>@.@.d....e........6.......P...yc..........................................................................................
21:30:16.290943 IP node0.3001 > node1.51596: Flags [.], ack 89, win 8008, length 0
E..(......N........e........6...P..Hp.........
 
 NODE1--SENDREC->NODE0
21:30:16.922489 IP node1.51596 > node0.3001: Flags [P.], seq 89:177, ack 1, win 5840, length 88
E...h?@.@.d....e........6.......P...BI.................................................W7b.6....................................
21:30:17.054496 IP node0.3001 > node1.51596: Flags [.], ack 177, win 7920, length 0
E..(.	....N........e........6..hP...p.........

 NODE0--SEND->NODE1 
21:30:16.934663 IP node0.49153 > node1.3000: Flags [P.], seq 89:177, ack 1, win 8096, length 88
E.........NP.......e........tm..P...#..................................................W........................................
21:30:16.934998 IP node1.3000 > node0.49153: Flags [.], ack 177, win 5840, length 0
E..(..@.@..!...e........tm......P.......

NODE1--SENDACK->NODE0
21:30:17.054868 IP node1.51596 > node0.3001: Flags [P.], seq 177:353, ack 1, win 5840, length 176=2*88 !!!!!!!!!!!!!!!!!!!!!!!!!!!!
E...h@@.@.c....e........6..h....P....`... .............................................W$.+8.j...j...j.........................................................................W=.58....................................

AQUI NODE0 SE PIERDE Y NODE1 VUELVE A INTENTAR
21:30:17.287871 IP node1.51596 > node0.3001: Flags [P.], seq 177:353, ack 1, win 5840, length 176
E...hA@.@.c....e........6..h....P....`... .............................................W$.+8.j...j...j.........................................................................W=.58....................................
21:30:17.288592 IP node0.3001 > node1.51596: Flags [.], ack 353, win 7744, length 0
 
 
  pr_receive_header:402:RPROXY: netbuf_len=176 <<<<<<<<<<<<<<<<<<<<<<<<<
 pr_receive_header:414:RPROXY: cmd=8193 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:431:RPROXY:cmd=8193 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:461:RPROXY: put2lcl
 m3ipc_rproxy_serve:576:RPROXY: Message succesfully processed.
 pr_process_message:428:RPROXY: About to receive header
	 
---------------------------------------------------------------------------------------------------------------		
20160717:

	 2937 es loop_r-s_server con endp=0 nodeid=0
	 	 
 [ 2341.695291] DEBUG 2937:do_unbind:1710: initialized 
[ 2341.695291] DEBUG 2937:mm_unbind:1513: WUNLOCK_PROC ep=1 count=0
[ 2341.695291] DEBUG 2937:mm_unbind:1514: WUNLOCK_VM vm=0 count=0
[ 2341.698644] DEBUG 2937:mm_exit_unbind:2647: WLOCK_TASK pid=2937 count=0
[ 2341.698650] DEBUG 2937:mm_exit_unbind:2651: RLOCK_PROC ep=0 count=0
[ 2341.698656] DEBUG 2937:mm_exit_unbind:2683:  Exiting endpoint=0 lpid=2937
[ 2341.698661] DEBUG 2937:mm_exit_unbind:2685: RUNLOCK_PROC ep=0 count=0
[ 2341.698666] DEBUG 2937:mm_exit_unbind:2686: WLOCK_VM vm=0 count=0
[ 2341.698671] DEBUG 2937:mm_exit_unbind:2687: WLOCK_PROC ep=0 count=0
[ 2341.698676] DEBUG 2937:mm_exit_unbind:2694:  endpoint=0 lpid=2937
[ 2341.698683] DEBUG 2937:do_unbind:1557: nr=0 endp=0 vmid=0 flags=0 misc=20 lpid=2937 nodeid=0 nodemap=1 name=loop_r-s_server 
[ 2341.698690] DEBUG 2937:do_unbind:1587: Caller endpoint=0 lpid=2937
[ 2341.698695] DEBUG 2937:do_unbind:1621: wakeup with error those processes trying to send a message to the proc
[ 2341.698883] DEBUG 2937:do_unbind:1624: WLOCK_PROC ep=1 count=0
[ 2341.698951] DEBUG 2937:do_unbind:1635: Find process 1 trying to send a message to 0
[ 2341.699058] DEBUG 2937:do_unbind:1639: Wakeup SENDER with error ep=1  pid=-1
[ 2341.699093] DEBUG 2937:inherit_cpu:353: cpuid=0
[ 2341.699175] ERROR: 2937:inherit_cpu:357: rcode=-3 <<<<<<<< EMOLSRCH  (_SIGN  3)  /* no such process */

[ 2341.699189] DEBUG 2937:inherit_cpu:361: nr=1 endp=1 vmid=0 lpid=-1 p_cpumask=FF name=$noname 
[ 2341.699252] DEBUG 2937:do_unbind:1640: BEFORE UP lpid=-1 p_sem=0 rcode=-109
[ 2341.699353] DEBUG 2937:do_unbind:1644: WUNLOCK_PROC ep=1 count=0
[ 2341.699406] DEBUG 2937:do_unbind:1624: WLOCK_PROC ep=1 count=0
[ 2341.700028] BUG: unable to handle kernel paging request at 00100104
[ 2341.700028] IP: [<e09711a2>] do_unbind+0x232/0xf80 [mol_replace]
[ 2341.700028] *pde = 00000000 
[ 2341.700028] Oops: 0002 [#1] SMP 
[ 2341.700028] last sysfs file: /sys/devices/pci0000:00/0000:00:11.0/0000:02:01.0/device
[ 2341.700028] Modules linked in: mol_replace
[ 2341.700028] 
[ 2341.700028] Pid: 2937, comm: loop_r-s_server Not tainted (2.6.32 #855) VMware Virtual Platform
[ 2341.700028] EIP: 0060:[<e09711a2>] EFLAGS: 00010282 CPU: 0
[ 2341.700028] EIP is at do_unbind+0x232/0xf80 [mol_replace]
[ 2341.700028] EAX: 00200200 EBX: df3e4800 ECX: df0d1eb8 EDX: 00100100
[ 2341.700028] ESI: df3e4600 EDI: 00100054 EBP: df3e4954 ESP: df0d1eb4
[ 2341.700028]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
[ 2341.700028] Process loop_r-s_server (pid: 2937, ti=df0d0000 task=df960f80 task.ti=df0d0000)
[ 2341.700028] Stack:
[ 2341.700028]  e098a8d0 00000b79 e098eb7d 00000658 00000001 00000000 ffffff93 00000000
[ 2341.700028] <0> 00000020 00000b79 00000000 00000001 df3e4640 5d363736 162c0020 00000001
[ 2341.700028] <0> df3e4754 df3e46a4 df960f80 e098f2c0 df960f80 df960f80 df960f80 df960f80
[ 2341.700028] Call Trace:
[ 2341.700028]  [<e0976b74>] ? mm_exit_unbind+0x234/0x910 [mol_replace]
[ 2341.700028]  [<e0976940>] ? mm_exit_unbind+0x0/0x910 [mol_replace]
[ 2341.700028]  [<c10391d8>] ? do_exit+0x68/0x690
[ 2341.700028]  [<c1021c4c>] ? do_page_fault+0xec/0x2f0
[ 2341.700028]  [<c103983c>] ? do_group_exit+0x3c/0xa0
[ 2341.700028]  [<c10398b1>] ? sys_exit_group+0x11/0x20
[ 2341.700028]  [<c1002ee8>] ? sysenter_do_call+0x12/0x26
[ 2341.700028] Code: 98 e0 01 0f 85 17 06 00 00 8b 44 24 40 e8 97 22 c2 e0 f6 05 fd ef 98 e0 01 0f 85 c1 05 00 00 8b 93 ac 00 00 00 8b 83 b0 00 00 00 <89> 42 04 89 10 c7 83 ac 00 00 00 00 01 10 00 c7 83 b0 00 00 00 
[ 2341.700028] EIP: [<e09711a2>] do_unbind+0x232/0xf80 [mol_replace] SS:ESP 0068:df0d1eb4
[ 2341.700028] CR2: 0000000000100104
[ 2341.720851] ---[ end trace 5d7ae509fbf55595 ]---	 

Se modifico mm_hyper para que do_unbind() controle:
	1- que el propio proceso unbindeado tiene mensaje pendiente de envio en la cola de otro proceso
	2- que el propio proceso unbindeado esta esperando que otro proceso termine de migrar
	
	WARNING: Queda pendiente remover referencia si el propio proceso unbindeado esta en proceso de MIGRACION.
	
SECUENCIA CORRECTA
==================
 NODE1 ---SENDREC-->NODE0 	
 ps_start_serving:450:SPROXY: source=1 type=0 m1i1=49 m1i2=49 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
 ps_send_remote:394:SPROXY:cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 ps_send_header:330:SPROXY: send header=88 
 ps_send_header:348:SPROXY: socket=3 sent header=88 
 ps_start_serving:423:SPROXY 2697: Waiting a message
 
 NODE0--->SEND---NODE1
 pr_receive_header:108:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 pr_receive_header:110:RPROXY: cmd=1 vmid=0 src=0 dst=1 snode=0 dnode=1 rcode=0 len=0
 pr_process_message:136:RPROXY:cmd=1 vmid=0 src=0 dst=1 snode=0 dnode=1 rcode=0 len=0
 pr_process_message:155:RPROXY: source=0 type=0 m1i1=49 m1i2=49 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
 pr_process_message:166:RPROXY: put2lcl
 pr_start_serving:253:RPROXY: Message succesfully processed.
 pr_process_message:133:RPROXY: About to receive header
 pr_receive_header:102:socket=4
 
  NODE1 ---SENDACK-->NODE0 	
 ps_start_serving:443:SPROXY: 2697 cmd=8193 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 ps_send_remote:394:SPROXY:cmd=8193 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 ps_send_header:330:SPROXY: send header=88 
 ps_send_header:348:SPROXY: socket=3 sent header=88 
 ps_start_serving:423:SPROXY 2697: Waiting a message
 
 SECUENCIA INCOMPLETA
=====================

 NODE1 ---SENDREC-->NODE0 	
 ps_start_serving:450:SPROXY: source=1 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
 ps_send_remote:394:SPROXY:cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 ps_send_header:330:SPROXY: send header=88 
 ps_send_header:348:SPROXY: socket=3 sent header=88 
 ps_start_serving:423:SPROXY 2697: Waiting a message
 
En NODE0 se recibe correctamente SENDREC
 pr_process_message:448:RPROXY: About to receive header
 pr_receive_header:418:RPROXY: rdesc.td_bytes=88 bytes=88
 pr_receive_header:434:RPROXY: cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:451:RPROXY:cmd=3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 pr_process_message:470:RPROXY: source=1 type=0 m1i1=0 m1i2=0 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
 pr_process_message:481:RPROXY: put2lcl
 m3ipc_rproxy_serve:596:RPROXY: Message succesfully processed.
 
  QUEDO FUNCIONANDO OK loop_ipc_node0.sh pero LENTISIMOOOOOOOO!!!!!!!!!!!!!!!!!!!!!!
 
 PRUEBA DE COPY
 ==============
	./loop_copy_server 1 1 1024
	
NODE0--->COPYIN---->NODO1 !!!!!! ENVIA 2 COPIAS DE LO MISMO !!!!!! 
 15:24:08.867844 IP node0.49153 > node1.3000: Flags [P.], seq 30896:31920, ack 1, win 8096, length 1024
E..(.+....J........e.......v....P.......abcdefghijklmnopqrstuvwxyabcde.ghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvw.
15:24:08.867966 IP node0.49153 > node1.3000: Flags [P.], seq 31920:32944, ack 1, win 8096, length 1024
E..(.,....J........e.......v....P.......abcdefghijklmnopqrstuvwxyabcde.ghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvw.

 ps_start_serving:220:SPROXY: cmd=5 CMD_COPYIN_DATA vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1024
 ps_start_serving:254:SPROXY: src=2 dst=3 rqtr=2 saddr=0xb73c3000 daddr=0xb757c000 bytes=1024 
 ps_send_remote:174:SPROXY:cmd=5 CMD_COPYIN_DATA vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1024
 ps_send_header:113:SPROXY: send header bytesleft=88
 ps_send_header:127:SPROXY: sent header=88 
 ps_send_remote:181:SPROXY: send payload len=1024
 ps_send_payload:147:SPROXY: send header=1024 <<<<<<<<<<<<<<<<<<<< ??????????
 
 TCPDUMP MUESTRA CIENTOS DE ENVIOS DE LO MISMO!!!!!
16:07:20.118417 IP vnode0.49153 > node1.3000: Flags [P.], seq 80136:81160, ack 1, win 8096, length 1024
E..(.e....IK.......e......T...Z,P...y...abcdefghijklmnopqrstuvwxyabcde.ghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvw.
16:07:20.118621 IP vnode0.49153 > node1.3000: Flags [P.], seq 81160:82184, ack 1, win 8096, length 1024
E..(.f....IJ.......e......X...Z,P...u...abcdefghijklmnopqrstuvwxyabcde.ghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvw.
16:07:20.269971 IP node1.3000 > vnode0.49153: Flags [.], ack 82184, win 3072, length 0
E..(od@.?.^L...e..........Z,..\.P.......
16:07:20.270299 IP vnode0.49153 > node1.3000: Flags [P.], seq 82184:83208, ack 1, win 8096, length 1024
E..(.g....II.......e......\...Z,P...q...abcdefghijklmnopqrstuvwxyabcde.ghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvw.
16:07:20.270561 IP vnode0.49153 > node1.3000: Flags [P.], seq 83208:84232, ack 1, win 8096, length 1024
E..(.h....IH.......e......`...Z,P...m...abcdefghijklmnopqrstuvwxyabcde.ghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvwxyabcdefghijklmnopqrstuvw.
16:07:20.441954 IP node1.3000 > vnode0.49153: Flags [.], ack 84232, win 1024, length 0 
 
 ps_start_serving:220:SPROXY: cmd=5 CMD_COPYIN_DATA vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1024
 ps_start_serving:254:SPROXY: src=2 dst=3 rqtr=2 saddr=0xb73db000 daddr=0xb74b2000 bytes=1024 
 ps_send_remote:174:SPROXY:cmd=5 CMD_COPYIN_DATA vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1024
 ps_send_header:113:SPROXY: send header bytesleft=88
 m3ipc_rproxy_serve:614:RPROXY: Message succesfully processed.
 pr_process_message:466:RPROXY: About to receive header
 ps_send_header:127:SPROXY: sent header=88 
 ps_send_remote:181:SPROXY: send payload len=1024
 ps_send_payload:147:SPROXY: send payload=1024 
	AQUI QUEDA COMO COLGADO!!!
Falta el mensaje final de que envio todo 
	SVRDEBUG("SPROXY: sent payload=%d \n", total);	
 
ERROR: 	./loop_vcopy_node0 1 5
 FINALMENTE FUNCIONO!! pero en algun punto de la transferencia se CUELGA EL SISTEMA COMPLETO!!!!
 Debe ser problema del UNBIND

---------------------------------------------------------------------------------------------------------------		
20160718:
		Cambios en unbind()
		Se probo unbind con minix.sh 
		NODE0:	
		# ./minix.sh h 0
		esto arranca SLOT0, SYSTASK0 y PM0
NODE1:	
		# ./minix.sh h 1
		esto arranca SLOT1, SYSTASK1

NODE0:	
		# ./demonize -R 0 13 1 /home/MoL_Module/mol-ipc/servers/test/single_server
 
		FUNCIONO OK!!!
		
		endp0--send-->endp1
		[  354.730820] DEBUG MAIN:mm_mini_send:44: caller_nr=0 caller_ep=0 dst_ep=1 
		[  354.731008] DEBUG SPROXY:mm_get2rmt:459: cmd=1 CMD_SEND_MSG vmid=0 src=0 dst=1 snode=0 dnode=1 rcode=0 len=0
		endp1--->sendrec-->endp0 !!!!!! WHYYYYYY
		[  355.799773] DEBUG RPROXY:mm_put2lcl:734: CMD_SNDREC_MSG vmid=0 rmt_ep=1 rmt_nr=1 lcl_ep=0 lcl_nr=0
		[  355.799786] DEBUG RPROXY:send_rmt2lcl:74: destination is not waiting dst_ptr->p_usr.p_rts_flags=4. Enqueue at the TAIL.

		
---------------------------------------------------------------------------------------------------------------		
20160719:	CAMBIOS PROFUNDOS!!!!!!!
			Se modifico nuevamente el do_unbind
				verificando que el proceso unbinding no este esperando por el unbinding de otro
				despertar a todos los procesos que estaban esperando el unbind.
			Se modifico mm_unbind incorporando la espera de unbinding para cuando un caller_ptr != proc_ptr  
				hace un sigpipe(proc_ptr). De esta forma, el proceso invocante queda a la espera de 
				que el proc_ptr haga su propio exit_unbind, es decir su do_unbind y lo despierte al terminar.
			Se agrego un nuevo campo al proceso p_waitunbind  que indica por que proceso se espera para que unbinding
			Se agrego una cola para la espera p_ulist, p_uhead 
			Se corrigieron en IPC los loops de espera cuando el proceso destino estaba migrando. Se cambiaron los break; por continue;
			Se modificaron los proxy/tests/

			AL EJECUTAR LA PRUEBA DE DEMONIZE : TODO OK

---------------------------------------------------------------------------------------------------------------		
20160720:			


		Se  modifico wait4bind para que sea mnx_wait4bind(endpoint)
			- endpoint SELF: coincide con si mismo y espera q otro lo bindee 
			- endpoint OTHER: espera el binding del proceso CONDICION:  el caller debe estar bindeado

		Se modifico wait4unbind:
			- endpoint SELF: no valido
			- endpoint OTHER: verifica que esta unbindeado y espera. CONDICION: El caller debe estar bindeado
	 

---------------------------------------------------------------------------------------------------------------		
20160722/23:			
		Se MODIFICO wait4bind y wait4unbind con test_wait4bind	
		El mnx_wait4bindep_T funciona OK
		El mnx_wait4unbind_T funciona OK
		
		TODO:	Modificar el WAIT4BIND y WAIT4UNBIND para procesos remotos.
		Si el proceso REMOTE fue bindeado localmente hay que comprobar si realmente esta bindeado en su propio nodo
		Habria que hacer un nuevo comando para los proxies y para el PUT2LCL para que solo compruebe el BINDING del proceso
	
		RESPUESTA: NO TIENE SENTIDO
			1) Si el proceso REMOTE fue bindeado localmente hay que comprobar si realmente esta bindeado en su propio nodo:
				Cuando se intente realizar una operacion lcl_proc->SPROXY----->RPROXY el RPROXY REMETO reportar EMOLNOTBIND
				La aplicación local deberá reintentar periodicamente.
			2) Si el proceso REMOTE fue unbindeado localmente, no hay mas nada que hacer. Aun cuando este bindeado en el REMOTE

			
---------------------------------------------------------------------------------------------------------------		
20160724:			
		Volviendo a probar nuevamente LWIP se cuelga en el test de IPC
		
		Le asigne al MAIN THREAD del LWIP el endpoint de SYSTASK (-2) y parece funcionar mas estable.
		Al menos no cuelga el sistema. Seguramente es un problema de threads y M3-IPC.
		De todos modos sigue habiando un error de sincronizacion
				
MAIN THREAD (ep=0) espera recibir mensajes de uno de sus threads (ep=2) 				
[  323.452664] DEBUG 2779:mm_mini_receive:280: RLOCK_PROC ep=0 count=0
[  323.452667] DEBUG 2779:mm_mini_receive:284: caller_nr=0 caller_ep=0 src_ep=2 
[  323.452670] DEBUG 2779:mm_mini_receive:288: vmid=0
[  323.452673] DEBUG 2779:mm_mini_receive:292: RUNLOCK_PROC ep=0 count=0
[  323.452676] DEBUG 2779:mm_mini_receive:294: RLOCK_VM vm=0 count=0
[  323.452678] DEBUG 2779:mm_mini_receive:297: RUNLOCK_VM vm=0 count=0
[  323.452681] DEBUG 2779:mm_mini_receive:301: RLOCK_PROC ep=0 count=0
[  323.452684] DEBUG 2779:mm_mini_receive:311: RUNLOCK_PROC ep=0 count=0
[  323.452687] DEBUG 2779:mm_mini_receive:313: RLOCK_PROC ep=2 count=0
[  323.452690] DEBUG 2779:mm_mini_receive:315: RUNLOCK_PROC ep=2 count=0
[  323.452693] ERROR: 2779:mm_mini_receive:315: rcode=-108  <<<<<<<<<<<<<<<<<EMOLSRCDIED 

ENDPOIN 2 Se bindea luego !!!
[  323.455874] DEBUG 2780:mm_bind:953: oper=0 vmid=0 pid=2780 endpoint=2 nodeid=-1
[  323.455878] DEBUG 2780:mm_bind:975: RLOCK_VM vm=0 count=0
[  323.455881] DEBUG 2780:mm_bind:988: WLOCK_PROC ep=2 count=0
[  323.455884] DEBUG 2780:init_proc_desc:255: p_name=$noname vmid=0
		
---------------------------------------------------------------------------------------------------------------		
20160724:			
		Volviendo a probar nuevamente LWIP se cuelga en el test de IPC y de VCOPY
			OJO!!! Parece que funcion correctamente con loop_r-s_server.c y loop_sr_client.c
			los otros fallan => pueden ser errores de mm_rcvrqst() y mnx_reply()
			la copia de datos funciona tambien correctamente loop_copy_server.c y loop_copy_client.c
ERROR:	Puede ser que el problema esté en la manipulación de threads que hace LWIP, aun asi, no debería colgarse.
		
		Se elimino el flag DONATING = (1<<BIT_DONATING) del kernel
		Si un proceso va a donar un slot a un nodo remoto lo pone como BIT_NO_MAP para que no pueda ser usado como libre.
				SET_BIT(proc_ptr->p_rts_flags, BIT_NO_MAP);
		Luego si falla, SYSTASK lo vuelve a poner como local, porque total lo tiene marcado en su tabla de slots
				SET_BIT( slot[i].s_flags, BIT_SLOT_DONATING);	

TODO:	PROBAR !!!!!!		

---------------------------------------------------------------------------------------------------------------		
20160727:
		Probando el script minix.sh con arranque de init por demonize
		
	
---------------------------------------------------------------------------------------------------------------		
20160728:	
			CAMBIO DRASTICO!!!!
			La descripcion de un proxy debería contener:
				1) el máximo tamaño de bloque de transferencia que soporta
				2) la maxima cantidad de batched messages que soporta	
				
			Esto implico cambios en varias partes del modulo con respecto a VCOPY 
			El tratamiento de batched messages es un protocolop entre proxies.
			
			Ahora el formato es
			 mnx_proxies_bind(name, pxnr, spid, rpid, maxcopybuf)
			 int k_proxies_bind(char *px_name, int px_nr, int spid, int rpid , int maxcopybuf);

			probando ./rtest.sh opcion h: tcp threads FUNCIONO OK
			probando ./rtest.sh opcion l en node0 y opcion t en node1 se cuelga 
					loop_vcopy_node0 
			
			probando con el kernel ksocket en ambos nodos se detiene por syncronizacion. Estimo que el problema	
				estaria en el recvrqst() o reply() dado que son sendrec -->receive/send funciona OK
					loop_vcopy_node0 tambien deja de funcionar - no se cuelga el nodo0
			 
	SE REVISO loop_vcopy_node0 y loop_vcopy_node1 se mejoro sincronizacion.
	

---------------------------------------------------------------------------------------------------------------		
20160729:
			Volvi a probar LWIP_TAP Aparentemente hay un problema con el manejo de hilos
			Se va a tener que hacer por PROCESOS.
		PROBLEMA 1: Si se hace con FORK entonces se tiene el problema de que se crea un STACK completo LWIP para ese proceso
		PROBLEMA 2: Si se hace con PROCESOS SEPARADOS entonces se tiene el problema de que no se conoce el PID del otro proceso
				para hacer proxy_bind
		PROBLEMA 3: Si se  siguen usando THREADS pero se ponde un mutex antes de usar las primitivas M3IPC ambas primitivas	
				son bloqueantes, por lo que el MUTEX queda tomado y el otro se cuelga.

TODO:			Hacer funcionar MINIX.SH para que levante los 2 inits.

NODE0 hace demonize -R node1 /.../init 
				AHORA FUNCIONA BIEN

				INIT0 e INIT1 lanzados por demonize y compitiendo por SLOTS 
				funciona OK, pero tengo dudas de que el mol_exit remoto este funcionando.

				EJECUCION DE INIT LOCAL ----- OK
				NODE0			NODE1
				=====			=====
				SYSTASK0		SYSTASK1
				PM0
				DEMONIZE0
				INIT0
	
				EJECUCION DE INIT REMOTO  ----- ERROR
				NODE0			NODE1
				=====			=====
				SYSTASK0		SYSTASK1
				PM0				INIT1	(endpoint=12)
				DEMONIZE0		
	
				SINGLE SERVER REMOTO FUNCION OK!!!
		
EJECUCION DE INIT1 Y SUS HIJOS:		
		SUPONIENDO QUE INIT1 YA ARRANCO
		INIT1 hace fork y el child espera en wait4bind
		INIT1 MOL_FORK (child_lpid, ANY) le pasa el linux PID pero no tiene asignado ENDPOINT
		PM0 SYS_FORK a SYSTASK1 quien le retorna un ENDPOINT
		PM0 BINDPROC REMOTE con ENDPOINT y NODEID a SYSTASK0 
		PM0 Seleciona el MINIX PID  y se lo retorna a parent de INIT1
		CHILD INIT1 ya se desperto, por lo que deberia quedar esperando hasta que
		PM0 Le envia un mensaje RFORK a CHILD de INIT1
	

	
---------------------------------------------------------------------------------------------------------------		
20160801:	
			AHORA FUNCIONA INIT REMOTO
			
		SE COLGO NODO0 
		
WARNING:	Cuando el PM hace una operación debería ver si la llamada 
		sys_xxxxxx es contra su propia SYSTASK o la SYSTASK del proceso USER 		

---------------------------------------------------------------------------------------------------------------		
20160801:
		
ATENCION: PM FORKEXIT.C
	QUE HACEMOS??? SEND O NOTIFY ??? Se usa para desbloquear al CHILD que hizo el FORK o REXEC
	/* Reply to child to wake it up. */
	if(! TEST_BIT( rkp->p_rts_flags,BIT_REMOTE)) {
		mnx_notify(child_ep);					/* only parent gets details */
	}else{
		/* Wakeup the remote process  */
		rmc->mp_reply.m_type   = MOLFORK;
		rmc->mp_reply.M7_PID	= new_pid;
		rcode = mnx_send_T(child_ep, &rmc->mp_reply, FORK_WAIT_MS);
		if(rcode < 0) ERROR_RETURN(rcode);
	}

	SE REEMPLAZO TODO POR NOTIFY 

	ESTA FALLANDO EL INIT LOCAL
	
---------------------------------------------------------------------------------------------------------------		
20160802:
		
		REVISAR EL SCRIPT SLOT-REPORT.PL EN EL CALCULO DE RESP_M
		ESTA FALLANDO EL INIT LOCAL  - AHORA FUNCIONA BIEN PERO .....

NODO1: Cuando NODO0 hace el JOIN, el NODE1 le dona 4 slots		
		 slots.c:slots_loop:1939:source=1 type=2 dest=0 mAnr=4 mAia[0]=64 mAia[1]=65 mAia[2]=66 mAia[3]=67
		
		Como los numeros de slots son respecto al 0 absoluto, y nr_sysprocs = 35
		El primer slot libre deberia ser el 64-35 = 29 sin embargo
		NODE0 otorga al 30 como el primer slot libre.
		Ver porque esto es un problema de simbolos > y <= de las comparaciones contra nr_sysprocs
		Se debe definir cuando empiezan los de usuarios y cuando los de sistemas. Para ello ver que hace MINIX.
		
VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2735    0    0   80  27342  27342  27342  27342 systask        
 0   -3     -3     -1    1 1000    0  27342  27342  27342  27342 remote         
 0   -2     -2   2734    0  400   20  27342  27342  27342  27342 systask        
 0    0      0   2737    0  408   A0     -2  27342  27342  27342 pm             
 0   11     11   2744    0    C   20      0      0  27342  27342 init           
 0   12     12     -1    1 100C    0      0      0  27342  27342 init           
 0   30     30   2809    0    C   20      0      0  27342  27342 init  <<<< 65        
 0   31     31   2810    0    C   20      0      0  27342  27342 init  <<<< 66         
 0   32     32   2811    0    C   20      0      0  27342  27342 init  <<<< 67         
 0   33     33     -1    1 1008    0      0  27342  27342  27342 init           
 0   34     34     -1    1 1008    0      0  27342  27342  27342 init           
 0   35     35   2762    0    C   20      0      0  27342  27342 init           
 0   36     36   2763    0    C   20      0      0  27342  27342 init           
 0   37     37     -1    1 1008    0      0  27342  27342  27342 init           
 0   38     38     -1    1 1008    0      0  27342  27342  27342 init           

LUEGO NODE1 dona: 
  slots.c:slots_loop:1939:source=1 type=2 dest=0 mAnr=4 mAia[0]=68(33) mAia[1]=69(34) mAia[2]=70(35) mAia[3]=71(36)

  Como se puede ver en el listado anterior tanto el 33 y 34 estan ocupados por el NODE1
  
 ---------------------------------------------------------------------------------------------------------------		
20160804: 
		Se modificaron los fuentes que tienen relacion con nr_sys_procs, particularmente habia errores en slots.c
		Tambien se cambió el module m3-ipc
		
NODE0: PM0		
		ERROR: forkexit.c:do_fork:182: rcode=-335 (EMOLPRIVILEGES)
			/* Reply to child to wake it up. */
			rcode = mnx_notify(child_ep);
		ERROR: forkexit.c:do_fork:123: rcode=-16 (EMOLBUSY)
			PM0
			 pm.c:get_work:123:Wait for the next message and extract useful information from it.
			 pm.c:get_work:128:Request received from who_e=11, call_nr=2
			 utility.c:pm_isokendpt:40:endpoint=11 
			 utility.c:pm_isokendpt:42:*proc=11
			 utility.c:pm_isokendpt:43:kproc.p_endpoint=11
			 utility.c:sys_procinfo:105:Sending SYS_GETINFO request 11 to SYSTEM for p_nr=11
			 utility.c:sys_procinfo:109:nr=11 endp=11 vmid=0 flags=8 misc=20 lpid=2794 nodeid=0 nodemap=1 name=init 
			 forkexit.c:do_fork:113:nr=11 endp=11 vmid=0 flags=8 misc=20 lpid=2794 nodeid=0 nodemap=1 name=init 
			 forkexit.c:do_fork:115:source=11 type=2 m3i1=2803 m3i2=31438 m3p1=(nil) m3ca1=[init]
			 forkexit.c:do_fork:118:who_p=11 child_lpid=2803 M3_ENDPT=31438
			 sys_fork.c:sys_rfork:21:Sending SYS_FORK to SYSTASK child_lpid=2803 st_nodeid=0
			 forkexit.c:do_fork:122:child_ep=-16 child_lpid=2803 
			 pm.c:main:70:call_nr=2 result=-16
			 pm.c:setreply:310:proc_nr=11 result=-16
			 pm.c:main:80:Send out all pending reply messages
			 pm.c:main:90:Replying to 11
			 
			SYSTASK0
			 system.c:main:116:SYSTASK is waiting for requests
			 system.c:main:120:mnx_receive rcode=0
			 system.c:main:128:RECEIVE msg:source=0 type=1536 m4l1=2803 m4l2=11 m4l3=11 m4l4=35534 m4l5=165008920
			 system.c:main:137:call_nr=0 who_e=0
			 system.c:main:142:Calling vector 0
			 do_fork.c:do_fork:38:child_lpid=2803 PMnodeid=0
			 do_fork.c:get_free_proc:164:next_child=65 free_slots=5 return=30
			 do_fork.c:do_fork:54:nr=30 endp=30 vmid=0 flags=8 misc=20 lpid=2795 nodeid=0 nodemap=1 name=init 
			 system.c:main:153:REPLY msg:source=0 type=-16 m1i1=2803 m1i2=11 m1i3=11 m1p1=0x8ace m1p2=0x9d5d618 m1p3=(nil) 

	
 
 
TODO:			SLOTS!! un campo interesante para analizar es cuantos slots se solicitaron/obtenidos
	REQUESTER:	Node[2](55): Sending request of slots: 330
				Node[2](84): Donation of 110 slots from 1 to 2
				Node[2](92): Donation of 0 slots from 3 to 2
OTRO EJEMPLO:
				Node[4](34192): Sending request of slots: 3
				Node[4](34229): Donation of 1 slots from 1 to 4
				Node[4](34243): Donation of 1 slots from 2 to 4
				Node[4](34248): Donation of 1 slots from 3 to 4
				Node[4](34271): Donation of 1 slots from 5 to 4
				Node[4](34285): Donation of 1 slots from 6 to 4
				Node[4](34309): Donation of 1 slots from 7 to 4
				Node[4](34320): Donation of 1 slots from 8 to 4
				Node[4](34343): Donation of 1 slots from 9 to 4
				Node[4](34362): Donation of 0 slots from 10 to 4
				Node[4](34382): Donation of 0 slots from 11 to 4
				Node[4](34394): Donation of 1 slots from 12 to 4
				Node[4](34416): Donation of 0 slots from 13 to 4
				Node[4](34427): Donation of 1 slots from 14 to 4
				Node[4](34445): Donation of 0 slots from 15 to 4
			El nodo 16 todavia no habia hecho JOIN-
				Node[16](12179): Received my own Join message
	AQUI PIDE NUEVAMENTE Node[4](70545): Sending request of slots: 4


	
ERROR DETECTADO!!!: en PM/FORKEXIT.C no de puede enviar un notify!!!! porque los privilegios del PM son correctos
			pero los del proceso de Usuario destinatario NO LO SON!!!
			Volver para atras y poner send_T o mejor aun
			hacer una nueva llamada CHILD que sea un sendrec desde el child al PM y el PM al recibirla la responde y lo 
			rehabilita.
			
		SOLUCION: Deshabilitar el control de procesos USER para que puedan recibir NOTIFY
			NO FUNCIONA: Porque en MINIX original hay una sola estructura de privilegios.
				Si bien en MOL hay una estructura priv por cada estructura proc y se podria implementar, quizas pueda
				resultar en algun tipo de problema mas adelante.
				
		PARENT		PM					CHILD
		mol_fork--->					wait4bind
		           sys_lclbind --wakeup-> 
										mol_wait4fork
				   PM bind
			<--reply--

					do_wait4fork 	
					if it is bound
				    ------reply------>>
		

FUNCIONA OK!!!! Ahora lo que no anda bien es el mol_exit();
		
OTRAS COSAS A REVISAR:   LA SYSTASK QUEDA A FULL TRANTANDO DE HACER UN SYS_VCOPY 
EL PROBLEMA ESTA EN QUE QUEDA EN ESTE LOOP, PERO PORQUE HAY ALGO QUE EL PROXY RECEIVER NO HIZO
REVISAR 
int sleep_proc3(struct proc *proc, struct proc *other1, struct proc *other2 , long timeout) 
	if( ret) {
		MOLDEBUG(INTERNAL,"pid=%d ret=%d\n",current->pid, ret);  
		if(proc->p_pseudosem < 0) proc->p_pseudosem = 0; 
//		del_timer_sync(&proc->p_timer);
		proc->p_rcode = ret; 
		while(test_bit(BIT_ONCOPY, &proc->p_usr.p_rts_flags)) {
			WUNLOCK_PROC3(proc, other1, other2);
			schedule();
			WLOCK_PROC3(proc, other1, other2);
		}
	}

PROCESOS EN NODE0
VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2870    0    0   80  27342  27342  27342  27342 systask        <<< misc 0x80 => MIS_BIT_REPLICATED
 0   -3     -3     -1    1 1000    0  27342  27342  27342  27342 remote         
 0   -2     -2   2869    0  400   20  27342  27342  27342  27342 systask   <<<< ONCOPY 
 0    0      0   2872    0  408   A0     -2  27342  27342  27342 pm    <<<<  ESTA EN ONCOPY ESPERANDO A SYSTASK 
 0   11     11   2879    0    C   20      0      0  27342  27342 init           
 0   12     12     -1    1 100C    0      0      0  27342  27342 init           
 0   29     29   2880    0    C   20      0      0  27342  27342 init           
 0   30     30   2884    0    C   20      0      0  27342  27342 init           
 0   31     31   2886    0    C   20      0      0  27342  27342 init           
........  
 0   74     74     -1    1 1408    0      0  27342  27342  27342 init  <<<< VER FLAGS- ESTA EN ONCOPY Y RMTOPER ESPERANDO A PM 

 
  ACA HAY UN PROBLEMA!!!
	El RPROXY0 recibe un mensaje de SLOT1 !!! (endpoint -34) que a su vez es replicado.
		cmd=2 vmid=0 src=-34 dst=0 snode=1 dnode=0 rcode=0 len=0 <<< CMD_NTFY_MSG 
  
EL PROXY RECEIVER NODE0 ERRNO
ERROR: tcp_th_proxy.c:pr_process_message:194: rcode=-309 EMOLLCLPROC The process is LOCAL 
ERROR: tcp_th_proxy.c:pr_process_message:227: rcode=-309
ERROR: tcp_th_proxy.c:pr_process_message:228: rcode=-310 EMOLNOTBIND 
 
EL PROXY RECEIVER NODE0 STDOUT
 tcp_th_proxy.c:pr_receive_header:120:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 tcp_th_proxy.c:pr_receive_header:122:RPROXY: cmd=2 vmid=0 src=-34 dst=0 snode=1 dnode=0 rcode=0 len=0
 tcp_th_proxy.c:pr_process_message:150:RPROXY:cmd=2 vmid=0 src=-34 dst=0 snode=1 dnode=0 rcode=0 len=0
 tcp_th_proxy.c:pr_process_message:180:RPROXY: put2lcl
 tcp_th_proxy.c:pr_process_message:187:RPROXY: REMOTE CLIENT BINDING rcode=-309
 tcp_th_proxy.c:pr_start_serving:315:RPROXY: Message processing failure [-309]
 tcp_th_proxy.c:pr_process_message:147:RPROXY: About to receive header
 tcp_th_proxy.c:pr_receive_header:114:socket=5
 
 
 
  tcp_th_proxy.c:pr_process_message:147:RPROXY: About to receive header
 tcp_th_proxy.c:pr_receive_header:114:socket=5
 tcp_th_proxy.c:ps_start_serving:514:SPROXY: 2859 cmd=8193 vmid=0 src=0 dst=-3 snode=0 dnode=1 rcode=0 len=0
 tcp_th_proxy.c:ps_send_remote:465:SPROXY:cmd=8193 vmid=0 src=0 dst=-3 snode=0 dnode=1 rcode=0 len=0
 tcp_th_proxy.c:ps_send_header:401:SPROXY: send header bytesleft=88
 tcp_th_proxy.c:ps_send_header:419:SPROXY: socket=3 sent header=88 
 tcp_th_proxy.c:ps_start_serving:494:SPROXY 2859: Waiting a message
 tcp_th_proxy.c:pr_receive_header:120:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 tcp_th_proxy.c:pr_receive_header:122:RPROXY: cmd=3 vmid=0 src=73 dst=0 snode=1 dnode=0 rcode=0 len=0
 tcp_th_proxy.c:pr_process_message:150:RPROXY:cmd=3 vmid=0 src=73 dst=0 snode=1 dnode=0 rcode=0 len=0
 tcp_th_proxy.c:pr_process_message:169:RPROXY: source=73 type=100 m1i1=2751 m1i2=73 m1i3=0 m1p1=0x74696e69 m1p2=(nil) m1p3=(nil) 
 tcp_th_proxy.c:pr_process_message:180:RPROXY: put2lcl
 tcp_th_proxy.c:pr_process_message:187:RPROXY: REMOTE CLIENT BINDING rcode=-310
 tcp_th_proxy.c:pr_process_message:218:RPROXY: cmd=3 vmid=0 src=0 dst=-2 snode=1 dnode=0 rcode=0 len=0
 tcp_th_proxy.c:pr_process_message:221:RPROXY: source=0 type=1568 m3i1=1 m3i2=73 m3p1=0x2 m3ca1=[RClient1]
 tcp_th_proxy.c:pr_process_message:224:RPROXY: mnx_put2lcl RMT_BIND to SYSTASK
 tcp_th_proxy.c:pr_start_serving:315:RPROXY: Message processing failure [-310]
 
 
 SYSTASK0-
 system.c:main:116:SYSTASK is waiting for requests
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=0 type=1551 m4l1=500 m4l2=74 m4l3=0 m4l4=134521236 m4l5=134553884
 system.c:main:137:call_nr=15 who_e=0
 system.c:main:142:Calling vector 15
 do_copy.c:do_copy:32:src=74, saddr=134521236, dst=0 daddr=134553884, bytes=5
 do_copy.c:do_copy:52:SRC nr=74 endp=74 vmid=0 flags=1008 misc=0 lpid=-1 nodeid=1 nodemap=2 name=init 
 do_copy.c:do_copy:55:DST nr=0 endp=0 vmid=0 flags=8 misc=A0 lpid=2872 nodeid=0 nodemap=1 name=pm 
 
 DMESG0: se puede ver haciendo sleep_proc3 => Requester SYSTASK, SOURCE=74, DEST=PM
Se supone que SYSTASK debio enviar un cmd
	CMD_COPYRMT_RQST (8),	/* REQUESTER to SENDER to copy data out to RECEIVER */


 [ 2349.467803] DEBUG 2869:sleep_proc3:816: WUNLOCK_PROC ep=-2 count=0
[ 2349.467806] DEBUG 2869:sleep_proc3:816: WUNLOCK_PROC ep=74 count=0
[ 2349.467809] DEBUG 2869:sleep_proc3:816: WUNLOCK_PROC ep=0 count=0
[ 2349.467812] DEBUG 2869:sleep_proc3:818: WLOCK_PROC ep=-2 count=0
[ 2349.467814] DEBUG 2869:sleep_proc3:818: WLOCK_PROC ep=0 count=0
[ 2349.467817] DEBUG 2869:sleep_proc3:818: WLOCK_PROC ep=74 count=0

 
 EN NODE1 LOS PROXIES REGISTRAN
 tcp_th_proxy.c:ps_start_serving:494:SPROXY 2708: Waiting a message
 tcp_th_proxy.c:pr_receive_header:120:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 CMD_COPYOUT_RQST (6),	/* The remote process send to local process the data requested 	*/	
 tcp_th_proxy.c:pr_receive_header:122:RPROXY: cmd=6 vmid=0 src=-2 dst=74 snode=0 dnode=1 rcode=0 len=0 
 tcp_th_proxy.c:pr_process_message:150:RPROXY:cmd=6 vmid=0 src=-2 dst=74 snode=0 dnode=1 rcode=0 len=0
 tcp_th_proxy.c:pr_process_message:180:RPROXY: put2lcl
 tcp_th_proxy.c:pr_start_serving:312:RPROXY: Message succesfully processed.
 
 LE DEVUELVE LO SOLICITADO  CMD_COPYOUT_DATA 	(CMD_COPYOUT_RQST | CMD_ACKNOWLEDGE)
 tcp_th_proxy.c:ps_start_serving:514:SPROXY: 2708 cmd=8198 vmid=0 src=74 dst=-2 snode=1 dnode=0 rcode=0 len=5
 DESDE 74 A PM0 CON REQUESTER SYSTASK0
 tcp_th_proxy.c:ps_start_serving:548:SPROXY: src=74 dst=0 rqtr=-2 saddr=0x804a194 daddr=0x805211c bytes=5 
 tcp_th_proxy.c:ps_send_remote:465:SPROXY:cmd=8198 vmid=0 src=74 dst=-2 snode=1 dnode=0 rcode=0 len=5
 tcp_th_proxy.c:ps_send_header:401:SPROXY: send header bytesleft=88
 tcp_th_proxy.c:ps_send_header:419:SPROXY: socket=3 sent header=88 
 tcp_th_proxy.c:ps_send_remote:472:SPROXY: send payload len=5
 tcp_th_proxy.c:ps_send_payload:435:SPROXY: send header=5 
 
 EN NODE0 LOS PROXIES  REGISTRAN
 tcp_th_proxy.c:ps_start_serving:514:SPROXY: 2859 cmd=6 vmid=0 src=-2 dst=74 snode=0 dnode=1 rcode=0 len=0
 tcp_th_proxy.c:ps_send_remote:465:SPROXY:cmd=6 vmid=0 src=-2 dst=74 snode=0 dnode=1 rcode=0 len=0
 tcp_th_proxy.c:ps_send_header:401:SPROXY: send header bytesleft=88
 tcp_th_proxy.c:ps_send_header:419:SPROXY: socket=3 sent header=88 
 tcp_th_proxy.c:ps_start_serving:494:SPROXY 2859: Waiting a message
 tcp_th_proxy.c:pr_receive_header:120:RPROXY: n:88 | received:88 | HEADER_SIZE:88
 tcp_th_proxy.c:pr_receive_header:122:RPROXY: cmd=8198 vmid=0 src=74 dst=-2 snode=1 dnode=0 rcode=0 len=5
 tcp_th_proxy.c:pr_process_message:150:RPROXY:cmd=8198 vmid=0 src=74 dst=-2 snode=1 dnode=0 rcode=0 len=5
 tcp_th_proxy.c:pr_receive_payload:93:payload_size=5
 tcp_th_proxy.c:pr_receive_payload:98:RPROXY: n:5 | received:5
 tcp_th_proxy.c:pr_process_message:180:RPROXY: put2lcl
 tcp_th_proxy.c:pr_start_serving:312:RPROXY: Message succesfully processed.
 
 TODO: Como se ve el proxy receiver no imprime las caracteristicas de un VCOPY y eso es importante.
 
 ENTONCES!!! Hay un problema entre mnx_vcopy() y el put2lcl() que hace que quede loopeando en el kernel   ???????
	NO, EL PROBLEMA ESTABA EN slots.c que sp_syscall(message  *spin_ptr) al finalizar envia un mnx_ntfy_value()
	al PM en nombre de SLOTS
	Como SLOTS esta replicado el RPROXY lo rechazaba por ser "LOCAL"
	
---------------------------------------------------------------------------------------------------------------
20160812:
		Cuando NODE0 ejecuta el INIT LOCAL parece andar todo OK
		Cuando ejecuta el INIT REMOTO es como que PM termina y se cuelta completo.
		
 SECUENCIA CORRECTA	SYSTASK1
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=0 type=1536 m4l1=3186 m4l2=12 m4l3=11 m4l4=35534 m4l5=150636268
 system.c:main:137:call_nr=0 who_e=0
 system.c:main:142:Calling vector 0
 do_fork.c:do_fork:38:child_lpid=3186 PMnodeid=0
 do_fork.c:get_free_proc:163:nr=58 endp=58 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 do_fork.c:get_free_proc:166:next_child=93 free_slots=163 return=58 s_owner=1
 do_fork.c:do_fork:54:nr=58 endp=58 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 do_fork.c:do_fork:63:bind vmid=0 child_lpid=3186, child_nr=58  child_ep=58
 do_fork.c:do_fork:65:child_ep=58 rcode=58
 do_fork.c:do_fork:77:nr=58 endp=58 vmid=0 flags=0 misc=20 lpid=3186 nodeid=1 nodemap=2 name=init 
 do_fork.c:do_fork:81:child_ep=58
 do_fork.c:do_fork:85:s_id=64 s_warn=27342 s_level=0 trap=0 call=0
 do_fork.c:do_fork:102:child_nr=58 free_slots=162
 system.c:main:153:REPLY msg:source=0 type=0 m1i1=58 m1i2=58 m1i3=11 m1p1=0x8ace m1p2=0x8fa86ec m1p3=(nil) 
 init.c:do_fork:35:CHILD: mnx_wait4bind_T  rcode=58
 mol_wait4fork.c:mol_wait4fork:14:M3_LPID=3186 M3_ENDPT=58
 system.c:main:116:SYSTASK is waiting for requests
 mol_fork.c:mol_fork:22:child_pid=31 ret=31
 init.c:do_fork:55:mol_fork: child_lpid=3186 child_pid=31
 mol_wait4fork.c:mol_wait4fork:23:child_pid=31 ret=31
 init.c:main:146:INIT CHILD Linux PID:3186 Minix PID:31
 init.c:main:149:mol_gettimeofday: tv.tv_sec=54256 tv.tv_usec=910065408
 init.c:main:152:mol_time: tt=54256 tr=54256
 init.c:main:158:mol_getprocnr: child_ep=58
 init.c:main:161:mol_getpprocnr: parent_ep=11
 init.c:main:164:mol_getnprocnr: ep=58
 init.c:main:168:mol_pm_findproc: ep=11
 clock.c:get_uptime:205:tv_sec=14075 tv_nsec=17089024 uptime=1407501 
 clock.c:clock_handler:108:clockTicks=100 next_timeout=2147483647 realtime=1407501 
 	
	
 SECUENCIA FALLIDA SYSTASK1
 init.c:main:241:head=10 tail=0 dif=10
 init.c:do_fork:29:PARENT
 init.c:do_fork:50:PARENT: child_lpid=3188
 init.c:do_fork:37:CHILD: mnx_wait4bind_T TIMEOUT
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=0 type=1536 m4l1=3188 m4l2=12 m4l3=11 m4l4=35534 m4l5=150636268
 system.c:main:137:call_nr=0 who_e=0
 system.c:main:142:Calling vector 0
 do_fork.c:do_fork:38:child_lpid=3188 PMnodeid=0
 do_fork.c:get_free_proc:163:nr=59 endp=59 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 do_fork.c:get_free_proc:166:next_child=94 free_slots=162 return=59 s_owner=1
 do_fork.c:do_fork:54:nr=59 endp=59 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 do_fork.c:do_fork:63:bind vmid=0 child_lpid=3188, child_nr=59  child_ep=59
 do_fork.c:do_fork:65:child_ep=59 rcode=59
 do_fork.c:do_fork:77:nr=59 endp=59 vmid=0 flags=0 misc=20 lpid=3188 nodeid=1 nodemap=2 name=init 
 do_fork.c:do_fork:81:child_ep=59
 do_fork.c:do_fork:85:s_id=64 s_warn=27342 s_level=0 trap=0 call=0
 do_fork.c:do_fork:102:child_nr=59 free_slots=161
 system.c:main:153:REPLY msg:source=0 type=0 m1i1=59 m1i2=59 m1i3=11 m1p1=0x8ace m1p2=0x8fa86ec m1p3=(nil) 
 init.c:do_fork:35:CHILD: mnx_wait4bind_T  rcode=59
 mol_wait4fork.c:mol_wait4fork:14:M3_LPID=3188 M3_ENDPT=59 <<<<<<<<<<<<<<<<
 
 PUDE DEBERSE A QUE EL LPID=3188 PARA EL PROCESO LOCAL Y PARA EL REMOTO
 NODE1 - SYSTASK1 
  do_fork.c:do_fork:77:nr=59 endp=59 vmid=0 flags=0 misc=20 lpid=3188 nodeid=1 nodemap=2 name=init 
 NODE0 - PM0
  forkexit.c:do_wait4fork:206:nr=35 endp=35 vmid=0 flags=8 misc=20 lpid=3188 nodeid=0 nodemap=1 name=init 

   mol_wait4fork.c:mol_wait4fork:14:M3_LPID=2720 M3_ENDPT=33
   
 tcp_th_proxy.c:ps_start_serving:514:SPROXY: 2704 cmd=3 vmid=0 src=33 dst=0 snode=1 dnode=0 rcode=0 len=0
 tcp_th_proxy.c:ps_start_serving:544:SPROXY: source=33 type=100 m1i1=2720 m1i2=33 m1i3=0 m1p1=0x74696e69 m1p2=(nil) m1p3=(nil) 
 tcp_th_proxy.c:ps_send_remote:465:SPROXY:cmd=3 vmid=0 src=33 dst=0 snode=1 dnode=0 rcode=0 len=0
 tcp_th_proxy.c:ps_send_header:401:SPROXY: send header bytesleft=88
 tcp_th_proxy.c:ps_send_header:419:SPROXY: socket=3 sent header=88

---------------------------------------------------------------------------------------------------------------
20160817: 
			TRABAJE EN EL  UDT_PROXY_CPP
			No se conecta y veo en iptraf que hay trafico 127.0.0.1:3000 en lugar de 192.168.1.100:30000

			
ERROR:	Resolver el tema del init remoto y local de minix.sh
ERROR:	UDT_PROXY_CPP
		
---------------------------------------------------------------------------------------------------------------
20160924:
		Se implemento en las VM  para advertir a un proceso (vm_warn2proc) cuando un proceso termina
		sin pasar por el PM con mensaje desde el kernel_sendrec() de tipo vm_msgtype2 desde el propio proceso salientes
		Ej: vm_warn2proc = PM
			vm_msgtype2 = MOLEXIT
			
		Si un proceso termina sin hacer mol_exit(), el exit() de LINUX hace mm_exit_unbind() donde se le envia
		un mensaje MOLEXIT al PM para que lo de de baja de sus tablas, de la del FS y de SYSTASK.
		
		El kernel invoca a mm_exit_unbind, si no es que la SYSTASK le envio un KILL, entonces 
		prepara un pseudo mensajes EXIT para enviar al PM el que es enviado por kernel_warn2proc
			caller_ptr->p_message.m_type = vm_ptr->vm_usr.vm_wanmsg;
			caller_ptr->p_message.m1_i1 = (int) code;
			caller_ptr->p_message.m1_i2 = (-1); /* caller is exiting */
		Realiza un pseudo sendrec(PM,msg) y queda a la espera de ser despertado.
		el PM envia un sys_exit a la SYSTASK
		La systask, verifica que ella no mato al proceso mirando el bit MIS_BIT_KILLED
		Si tiene el bit MIS_BIT_KILLED, es porque fue matado por otro proceso o por si mismo.
		La SYSTASK le envia un mnx_ntfy_value(PM_PROC_NR, proc_ep, PM_PROC_NR) y luego
		queda a la espera de wait4unbind() de ese proceso, para, definitivamente darlo por muerto.
		El proceso moribundo una vez despierto invoca por si mismo a unbind() y con eso
		despierta a la SYSTASK.
		

	
TODO:	Probar el funcionamiento
		modificar  kernel_sendrec - MEJORARLO (crear kipc.c en el module donde estaran todas las ipc de kernel)

---------------------------------------------------------------------------------------------------------------
20160925:
	Se cambio minix.sh para que la VM arranque mencionando al warning process y warning message
		/home/MoL_Module/mol-ipc/tests/test_vm_init -v 0 -P 0 -m 1 # PM y MOLEXIT
	
	DMESG arroja 
[  146.051079] DEBUG 2709:mm_exit_unbind:2967: WLOCK_TASK pid=2709 count=0
[  146.051163] DEBUG 2709:mm_exit_unbind:2971: RLOCK_PROC ep=31 count=0
[  146.051228] DEBUG 2709:mm_exit_unbind:3003:  Exiting endpoint=31 lpid=2709
[  146.051272] DEBUG 2709:mm_exit_unbind:3005: RUNLOCK_PROC ep=31 count=0
[  146.051334] DEBUG 2709:mm_exit_unbind:3006: WLOCK_VM vm=0 count=0
[  146.051380] DEBUG 2709:mm_exit_unbind:3007: WLOCK_PROC ep=31 count=0
[  146.051419] DEBUG 2709:mm_exit_unbind:3014:  endpoint=31 lpid=2709
[  146.051500] DEBUG 2709:mm_exit_unbind:3036: WUNLOCK_PROC ep=31 count=0
[  146.051563] DEBUG 2709:mm_exit_unbind:3037: WLOCK_PROC ep=31 count=0
[  146.051617] DEBUG 2709:mm_exit_unbind:3057: vm_vmid=0 vm_warn2proc=0 vm_warnmsg=1
[  146.051680] DEBUG 2709:kernel_sendrec:2901: caller_ep=31 warn_ep=0 
[  146.051756] DEBUG 2709:kernel_sendrec:2904: nr=0 endp=0 vmid=0 flags=8 misc=A0 lpid=2699 nodeid=0 nodemap=1 name=pm 
[  146.051802] DEBUG 2709:kernel_sendrec:2909: destination is waiting. Copy the message and wakeup destination
[  146.051900] DEBUG 2709:kernel_sendrec:2919: source=31 type=1 m1i1=0 m1i2=-1 m1i3=0 m1p1=(null) m1p2=(null) m1p3=(null) 
[  146.051932] DEBUG 2709:inherit_cpu:357: cpuid=0
[  146.051940] DEBUG 2709:inherit_cpu:365: nr=0 endp=0 vmid=0 lpid=2699 p_cpumask=FF name=pm 
[  146.051977] DEBUG 2709:kernel_sendrec:2930: BEFORE UP lpid=2699 p_sem=-1 rcode=0
[  146.052045] DEBUG 2709:sleep_proc2:725: BEFORE DOWN lpid=2709 p_sem=0 timeout=-1
[  146.052048] DEBUG 2709:sleep_proc2:727: WUNLOCK_PROC ep=31 count=0
[  146.052051] DEBUG 2709:sleep_proc2:727: WUNLOCK_PROC ep=0 count=1
[  146.052054] DEBUG 2709:sleep_proc2:728: endpoint=31 flags=8
[  146.052071] DEBUG 2709:sleep_proc2:740: endpoint=31 flags=8 cpuid=0
[  146.052075] DEBUG 2709:sleep_proc2:741: WLOCK_PROC ep=0 count=1
[  146.052078] DEBUG 2709:sleep_proc2:741: WLOCK_PROC ep=31 count=0
[  146.052110] DEBUG 2709:sleep_proc2:744: pid=2709 ret=-512<<<<<<<ERESTARTSYS     512
[  146.052154] DEBUG 2709:sleep_proc2:767: nr=31 endp=31 vmid=0 lpid=2709 p_cpumask=FF name=init 
[  146.052158] DEBUG 2709:sleep_proc2:769: someone wakeups me: sem=0 p_rcode=-512
[  146.052211] DEBUG 2709:mm_exit_unbind:3063: WUNLOCK_PROC ep=0 count=1
[  146.052256] ERROR: 2709:mm_exit_unbind:3067: rcode=-320 <<<<  EMOLPROCRUN 	(_SIGN 320)


PM RECIBE CORRECTAMENTE EL MENSAJE 
 pm.c:get_work:123:Wait for the next message and extract useful information from it.
 pm.c:get_work:128:Request received from who_e=31, call_nr=1
 utility.c:pm_isokendpt:40:endpoint=31 
 utility.c:pm_isokendpt:42:*proc=31
 utility.c:pm_isokendpt:43:kproc.p_endpoint=31
 forkexit.c:do_pm_exit:255:who_p=31 who_e=31 lnx_pid=-1 status=0
 forkexit.c:pm_exit:283:mnx_pid=5 status=0 proc_nr=31 proc_ep=31 
 forkexit.c:pm_exit:297:parent_pid=1
 forkexit.c:pm_exit:314:nr=31 endp=31 vmid=0 flags=8 misc=20 lpid=2709 nodeid=0 nodemap=1 name=init 
ERROR: forkexit.c:pm_exit:319: rcode=-324 <<<EMOLPROCSTS (_SIGN 324) 

EL PM VE AL PROCESO BLOQUEADO pero porque tiene informacion VIEJA.

	/* Tell SYSTASK to unbind the exiting process	*/
	SVRDEBUG(PROC_USR_FORMAT,PROC_USR_FIELDS(rkp));
	rcode = _sys_exit(rkp->p_endpoint, rkp->p_nodeid);
	if(rcode < 0 && rcode != EMOLNOTBIND)		
  		ERROR_EXIT(rcode); <<<<<<<<<<<<<<<<<<<<<<<<<<<  EMOLPROCSTS (_SIGN 324) 

SYSTASK 
 system.c:main:116:SYSTASK is waiting for requests
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=0 type=1538 m4l1=31 m4l2=35 m4l3=0 m4l4=134521236 m4l5=134553884
 system.c:main:137:call_nr=2 who_e=0
 system.c:main:142:Calling vector 2
 do_exit.c:do_exit:31:proc_ep=31
 do_exit.c:do_exit:43:before nr=31 endp=31 vmid=0 >>flags=1<< misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 system.c:main:153:REPLY msg:source=0 type=-324 m1i1=31 m1i2=35 m1i3=0 m1p1=0x804a194 m1p2=0x805211c m1p3=0x5 
 
LA SYSTASK VE AL PROCESO COMO EN ESTADO FREE (>>flags=1<<) , POR QUE??

		
		
EL PROBLEMA ESTA EN sleep_proc2(). Al estar el proceso SIGNALEADO, enseguida retorna del
	if( timeout < 0) {
		ret = wait_event_interruptible(proc->p_wqhead, (proc->p_pseudosem >= 0));
con error ERESTARTSYS
	
Cambiar el nombre de la funcion kernel_sendrec x kernel_warn2proc

Hacer una version reducida que use
	/* WARNING do not use sleep_proc2 because the process could be signaled !! */
	proc->p_pseudosem = -1; 
	WUNLOCK_PROC2(proc, other); 	
	MOLDEBUG(INTERNAL,"endpoint=%d flags=%lX cpuid=%d\n",proc->p_usr.p_endpoint, proc->p_usr.p_rts_flags, smp_processor_id());  

	ret = wait_event (proc->p_wqhead, (proc->p_pseudosem >= 0));

	WLOCK_PROC2(proc, other)
	proc->p_rcode = ret; 
	MOLDEBUG(INTERNAL,"pid=%d ret=%d\n",current->pid, ret); 
	if( ret != 0 && proc->p_pseudosem < 0) 
		proc->p_pseudosem++; 
	pu_ptr = &proc->p_usr;
	MOLDEBUG(INTERNAL, PROC_CPU_FORMAT, PROC_CPU_FIELDS(pu_ptr));
	

---------------------------------------------------------------------------------------------------------------		
20160929:

El proceso KILLED hace todo correctamente
[  149.783469] DEBUG KILLED:mm_exit_unbind:2966: WLOCK_TASK pid=KILLED count=0
[  149.783553] DEBUG KILLED:mm_exit_unbind:2970: RLOCK_PROC ep=31 count=0
[  149.783617] DEBUG KILLED:mm_exit_unbind:3002:  Exiting endpoint=31 lpid=KILLED
[  149.783660] DEBUG KILLED:mm_exit_unbind:3004: RUNLOCK_PROC ep=31 count=0
[  149.783720] DEBUG KILLED:mm_exit_unbind:3005: WLOCK_VM vm=0 count=0
[  149.783766] DEBUG KILLED:mm_exit_unbind:3006: WLOCK_PROC ep=31 count=0
[  149.783808] DEBUG KILLED:mm_exit_unbind:3013:  endpoint=31 lpid=KILLED
[  149.783899] DEBUG KILLED:mm_exit_unbind:3035: WUNLOCK_PROC ep=31 count=0
[  149.783964] DEBUG KILLED:mm_exit_unbind:3036: WLOCK_PROC ep=31 count=0
[  149.784043] DEBUG KILLED:mm_exit_unbind:3056: vm_vmid=0 vm_warn2proc=0 vm_warnmsg=1
[  149.784122] DEBUG KILLED:kernel_warn2proc:2901: caller_ep=31 warn_ep=0 
[  149.784193] DEBUG KILLED:kernel_warn2proc:2904: nr=0 endp=0 vmid=0 flags=8 misc=A0 lpid=PM0 nodeid=0 nodemap=1 name=pm 
[  149.784243] DEBUG KILLED:kernel_warn2proc:2909: destination is waiting. Copy the message and wakeup destination
[  149.784345] DEBUG KILLED:kernel_warn2proc:2919: source=31 type=1 m1i1=0 m1i2=-1 m1i3=0 m1p1=(null) m1p2=(null) m1p3=(null) 
[  149.784371] DEBUG KILLED:inherit_cpu:357: cpuid=0
[  149.784622] DEBUG KILLED:inherit_cpu:365: nr=0 endp=0 vmid=0 lpid=PM0 p_cpumask=FF name=pm 
[  149.784666] DEBUG KILLED:kernel_warn2proc:2922: BEFORE UP lpid=PM0 p_sem=-1 rcode=0
[  149.784730] DEBUG KILLED:kernel_warn2proc:2942: WUNLOCK_PROC ep=31 count=0
[  149.784778] DEBUG KILLED:kernel_warn2proc:2942: WUNLOCK_PROC ep=0 count=1  <<<<<<<<<<< count deberia se CERO !!!!!!!!!


PM0 recibe el sendrec de KILLED
[  149.784860] DEBUG PM0:sleep_proc:671: endpoint=0 flags=0 cpuid=0
[  149.784864] DEBUG PM0:sleep_proc:672: WLOCK_PROC ep=0 count=1
[  149.784869] DEBUG PM0:sleep_proc:698: nr=0 endp=0 vmid=0 lpid=PM0 p_cpumask=FF name=pm 
[  149.784872] DEBUG PM0:sleep_proc:700: someone wakeups me: sem=0 p_rcode=0
[  149.784944] DEBUG PM0:mm_mini_receive:448: source=31 type=1 m9i1=0 m9l1=-1 m9t1.tv_sec=0 m9t1.tv_nsec=0
[  149.784954] DEBUG PM0:mm_mini_receive:453: WUNLOCK_PROC ep=0 count=1 <<<<<<<<<<< count deberia se CERO !!!!!!!!!

PM0 hace _sys_exit pero aparentemente queda colgado.
	sys_exit.c:_sys_exit:17:SYS_EXIT request to SYSTEM(-2) proc=31 nodeid=0
[  149.785020] DEBUG PM0:mm_mini_sendrec:470: srcdst_ep=-2
[  149.785020] DEBUG PM0:check_caller:157: caller_pid=PM0 caller_tgid=PM0
[  149.785020] DEBUG PM0:check_caller:193: WLOCK_PROC ep=0 count=1
[  149.785020] DEBUG PM0:check_caller:222: WUNLOCK_PROC ep=0 count=1
[  149.785020] DEBUG PM0:check_caller:225: vmid=0

SYSTASK0 sale del receive pero por 512!!!!!!!!!!
[  150.427475] DEBUG SYSTASK0:sleep_proc:671: endpoint=-2 flags=8 cpuid=0
[  150.427482] DEBUG SYSTASK0:sleep_proc:672: WLOCK_PROC ep=-2 count=0
[  150.427486] DEBUG SYSTASK0:sleep_proc:675: pid=SYSTASK0 ret=-512
[  150.427494] DEBUG SYSTASK0:sleep_proc:698: nr=-2 endp=-2 vmid=0 lpid=SYSTASK0 p_cpumask=FF name=systask 
[  150.427497] DEBUG SYSTASK0:sleep_proc:700: someone wakeups me: sem=0 p_rcode=-512
[  150.427500] DEBUG SYSTASK0:mm_mini_receive:453: WUNLOCK_PROC ep=-2 count=0
[  150.427503] ERROR: SYSTASK0:mm_mini_receive:454: rcode=-512

SYSTASK0 hace un nuevo receive pero aparentemente queda colgado
[  150.427689] DEBUG SYSTASK0:mm_mini_receive:269: src_ep=31438
[  150.427694] DEBUG SYSTASK0:check_caller:157: caller_pid=SYSTASK0 caller_tgid=SYSTASK0
[  150.427698] DEBUG SYSTASK0:check_caller:193: WLOCK_PROC ep=-2 count=0
[  150.427701] DEBUG SYSTASK0:check_caller:222: WUNLOCK_PROC ep=-2 count=0
[  150.427704] DEBUG SYSTASK0:check_caller:225: vmid=0

---------------------------------------------------------------------------------------------------------------		
20160930:
		Si el proceso invoca mol_exit  y PM -> sys_exit
			la SYSTASK setea el bit MIS_BIT_KILLED. 
			la SYSTASK invoca a mm_unbind que envia un SIGPIPE al proceso EXITED  
			El proceso EXITED terminado hace do_unbind() 
			PM NO ENVIA respuesta al proceso KILLED.
	
OPCION 1:	NO ANDA
		Si el proceso es killed por otro proceso LINUX cualquiera (bash por ejemplo) 
			Detecta que el bit MIS_BIT_KILLED esta apagado, lo prende 
			le envia un mensaje SENDREC(EXIT) al PM		
			El PM le envia un sys_exit a SYSTASK
			La SYSTASK detecta el bit MIS_BIT_KILLED por lo que NO INVOCA a sys_exit()
			PM ENVIA respuesta al proceso KILLED.  <<< No funciona, al tratar de enviar el mensaje de respuesta
														da error (-71) porque el proceso ya esta killed para linux
			KILLED hace do_unbind() 
			
OPCION 2:	NO ANDA
		Si el proceso es killed por otro proceso LINUX cualquiera (bash por ejemplo) 
			Detecta que el bit MIS_BIT_KILLED esta apagado, lo prende 
			le envia un mensaje SEND(EXIT) al PM y luego hace unbind()		
			El PM le envia un sys_exit a SYSTASK
			La SYSTASK puede ver al proceso KILLED:
				1) Activo: Detecta el bit MIS_BIT_KILLED por lo que NO INVOCA a sys_exit()
				2) Muerto: porque como ya se hizo el unbind() 
					<<<<<<<<<<<<<<<< En este caso NO se hace el multicast para informar al resto de los nodos.
					y encima da error EMOLNOTBIND al retornar al PM.

OPCION 3:	
		Si el proceso es killed por otro proceso LINUX cualquiera (bash por ejemplo) 
			Detecta que el bit MIS_BIT_KILLED esta apagado, lo prende 
			le envia un mensaje SENDREC(EXIT) al PM 
			El PM le envia un sys_exit a SYSTASK
			La SYSTASK detecta el bit MIS_BIT_KILLED por lo que NO INVOCA a sys_exit()
			SYSTASK retorna REPLYPENDING al PM 
			PM envia NOTIFY al proceso KILLED.
			KILLED hace do_unbind() 


ALTERNATIVA: SYSTASK QUE HAGA el NOTIFY como si fuese el PM
mnx_ntfy_value(HARDWARE, killed_ep, PM_PROC_NR)

Luego que haga un wait4unbind() esperando que el proceso KILLED termine antes de devolver el control a PM

FUNCIONO OK!! EL EXIT POR KILL

SE MODIFICO mnx_notify()
	AHORA se puede enviar un notify a un procesos sin privegios siempre y 
	cuando este se encuentre en estado RECEIVING

SE ACTUALIZO GENERACION EN SYSTASK do_fork()		
	/* Register the process to the kernel */
	generation = _ENDPOINT_G(child_ptr->p_endpoint); <<<< FALTA SUMAR 1
	child_ep = _ENDPOINT(generation, child_ptr->p_nr);

---------------------------------------------------------------------------------------------------------------		
20160930:	
			
		AHORA NO FUNCIONA EL mol_exit() ->sys_exit() invocado por el propio proceso.


PM0:
 pm.c:get_work:128:Request received from who_e=29, call_nr=1
 utility.c:pm_isokendpt:40:endpoint=29 
 utility.c:pm_isokendpt:42:*proc=29
 utility.c:pm_isokendpt:43:kproc.p_endpoint=29
 forkexit.c:do_pm_exit:241:who_p=29 who_e=29 lnx_pid=-1 status=-7
 forkexit.c:pm_exit:288:mnx_pid=3 status=-7 proc_nr=29 proc_ep=29 
 forkexit.c:pm_exit:302:parent_pid=1
 forkexit.c:pm_exit:319:nr=29 endp=29 vmid=0 flags=8 misc=20 lpid=2715 nodeid=0 nodemap=1 name=init 
 sys_exit.c:_sys_exit:17:SYS_EXIT request to SYSTEM(-2) proc=29 nodeid=0
 utility.c:sys_procinfo:105:Sending SYS_GETINFO request 11 to SYSTEM for p_nr=29
ERROR: forkexit.c:pm_exit:328: rcode=-108 EMOLSRCDIED <<< murio SYSTASK
	
SYSTASK0:
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=0 type=1538 m4l1=29 m4l2=30 m4l3=0 m4l4=134521236 m4l5=134553884
 system.c:main:137:call_nr=2 who_e=0
 system.c:main:142:Calling vector 2
 do_exit.c:do_exit:31:proc_ep=29
 do_exit.c:do_exit:43:before nr=29 endp=29 vmid=0 flags=8 misc=20 lpid=2715 nodeid=0 nodemap=1 name=init 
 do_exit.c:do_exit:54:vm_nr_tasks=35 proc_nr=29 vm_nr_sysprocs=64
 do_exit.c:do_exit:72:UNBIND proc_ep=29
 do_exit.c:do_exit:91:endpoint 29 unbound
 do_exit.c:do_exit:96:proc_nr=29 free_slots=193
 timers.c:tmrs_clrtimer:17:prev_time=0
 do_exit.c:do_exit:107:after nr=29 endp=29 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 system.c:main:153:REPLY msg:source=0 type=0 m1i1=29 m1i2=30 m1i3=0 m1p1=0x804a194 m1p2=0x805211c m1p3=0x5 
 system.c:main:116:SYSTASK is waiting for requests
 APARENTEMENTE SYSTASK  TERMINA CORRECTAMENTE
 PERO 
 system.c:main:120:mnx_receive rcode=-106 EMOLNOTREADY <<<<<< QUIZAS ESTO ES PORQUE HUBO UN PROBLEMA EN SLOTS

PORQUE SYSTASK TIENE FLAG=2 NO_MAP		= (1<<BIT_NO_MAP)
 [  257.969073] DEBUG SYSTASK:do_unbind:1708: Caller nr=-2 endp=-2 vmid=0 flags=2 misc=20 lpid=SYSTASK nodeid=0 nodemap=1 name=systask 

EXITED PROCESS tambien  FLAG=2 NO_MAP
[  257.976970] DEBUG EXITED:do_unbind:1679: nr=29 endp=29 vmid=0 flags=2 misc=120 lpid=EXITED nodeid=0 nodemap=1 name=init 

	
PM Esta saliendo por aqui
	/* Tell SYSTASK copy the kernel entry to kproc[proc_nr]   	*/
  	if((rcode =sys_procinfo(proc_nr)) != OK) 
		ERROR_EXIT(rcode); <<<< rcode=-108 EMOLSRCDIED <<< murio SYSTASK

SYSTASK es despertado por PM		
[  257.967923] DEBUG SYSTASK:sleep_proc:671: endpoint=-2 flags=0 cpuid=0
[  257.967928] DEBUG SYSTASK:sleep_proc:672: WLOCK_PROC ep=-2 count=0
[  257.967936] DEBUG SYSTASK:sleep_proc:698: nr=-2 endp=-2 vmid=0 lpid=SYSTASK p_cpumask=FF name=systask 
[  257.967942] DEBUG SYSTASK:sleep_proc:700: someone wakeups me: sem=0 p_rcode=0
[  257.967947] DEBUG SYSTASK:mm_mini_receive:453: WUNLOCK_PROC ep=-2 count=0	

		
		Se modifico WAIT4BIND para que sea bloqueante. Para ello se agruego una wait_q en la estructura task de LINUX.

---------------------------------------------------------------------------------------------------------------		
20161015:	
		Pude hacer andar tasks/eth con /dev/tap0.
		Luego, desde INET envio un frame trucho a traves de la funcion de nw_test()

---------------------------------------------------------------------------------------------------------------		
20161015:
		
		El frame trucho sale perfectamente por la interface capturado por tcpdump.
			tcpdump -i tap$vmid -A  > /home/MoL_Module/mol-ipc/tcpdump.txt &
		
	11:23:41.891493 47:48:49:4a:4b:4c (oui Unknown) > 41:42:43:44:45:46 (oui Unknown) Null Information, send seq 0, rcv seq 0, Flags [Command], length 1500
	............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................

		Desde inet se envia un frame de length 1500	con los primeros caracteres SRC_MAC:"ABCDEF" DST_MAC:"GHIJKL"
		
		Luego se envio un frame vacio a la mac address del BRIDGE seteada desde INET y con SOURCE MAC la del tap0  seteada por ETH

eth0      Link encap:Ethernet  HWaddr 00:0c:29:ff:e3:2a  
          inet addr:192.168.1.100  Bcast:192.168.1.255  Mask:255.255.255.0
		  
		  
tap0      Link encap:Ethernet  HWaddr 5e:9c:02:df:e5:a4  
          inet6 addr: fe80::5c9c:2ff:fedf:e5a4/64 Scope:Link	  

	12:01:38.052928 5e:9c:02:df:e5:a4 (oui Unknown) > 00:0c:29:ff:e3:2a (oui Unknown) Null Information, send seq 0, rcv seq 0, Flags [Command], length 1500
	............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
		  
		Se cambio el tcpdump
tcpdump -i tap$vmid -N -n -e -vvv -XX > /home/MoL_Module/mol-ipc/tcpdump.txt &
		
		Se conformo un paquete ARP y enviar desde TAP0 a ETH0.
		tap0      Link encap:Ethernet  HWaddr 96:76:45:85:20:27 
		eth0      Link encap:Ethernet  HWaddr 00:0c:29:ff:e3:2a  
		
		96:76:45:85:20:27 > 00:0c:29:ff:e3:2a, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Request who-has 0.240.185.191 tell 126.183.0.0, length 28

			
---------------------------------------------------------------------------------------------------------------		
20161018:
						TAP0				BROADCAST						
04:56:39.394666 16:18:2f:e5:56:be > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.1.101 tell 172.16.0.2, length 28
	0x0000:  ffff ffff ffff 1618 2fe5 56be 0806 0001  ......../.V.....
	0x0010:  0800 0604 0001 1618 2fe5 56be ac10 0002  ......../.V.....
	0x0020:  0000 0000 0000 c0a8 0165                 .........e
	
					ETH0				BROADCAST	
11:59:55.989395 00:0c:29:ff:e3:2a > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.1.3 tell 192.168.1.100, length 28
	0x0000:  ffff ffff ffff 000c 29ff e32a 0806 0001  ........)..*....
	0x0010:  0800 0604 0001 000c 29ff e32a c0a8 0164  ........)..*...d
	0x0020:  0000 0000 0000 c0a8 0103                 ..........

			
---------------------------------------------------------------------------------------------------------------		
20161021:
		Cambie de tactica. Se reemplazo TAP0 por ETH0
TODO:
		En INET hay que construir el frame completo para enviar un paquete ICMP 
		En ETH hay que enviar el frame en crudo.-

---------------------------------------------------------------------------------------------------------------		
20161022:		
		Se volvio atras de ETH0 a TAP0
		Tap0:172.16.1.4---Br0:172.16.1.4
		Tap9:172.16.1.9--------|

		ATENCION: INET.C tiene HARDCODEADO las IPs de origen y destino como asi tambien la MAC address de TAP9
		Por otro lado minix.sh fue modificado para levantar tanto TAP0 como TAP9.
		
ARP REQUEST: FUNCIONO!!!
					TAP0: ARP who-has 172.16.1.9 tell 172.16.1.4
08:34:40.925702 b6:10:1f:ef:9b:98 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Request who-has 172.16.1.9 tell 172.16.1.4, length 28
	0x0000:  ffff ffff ffff b610 1fef 9b98 0806 0001  ................
	0x0010:  0800 0604 0001 b610 1fef 9b98 ac10 0104  ................
	0x0020:  0000 0000 0000 ac10 0109                 ..........
	
					TAP9: ARP Reply 172.16.1.9 is-at 72:89:78:ff:88:ef
08:34:40.926969 72:89:78:ff:88:ef > b6:10:1f:ef:9b:98, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Reply 172.16.1.9 is-at 72:89:78:ff:88:ef, length 28
	0x0000:  b610 1fef 9b98 7289 78ff 88ef 0806 0001  ......r.x.......
	0x0010:  0800 0604 0002 7289 78ff 88ef ac10 0109  ......r.x.......
	0x0020:  b610 1fef 9b98 ac10 0104  
		
PING REQUEST: FUNCIONO!!!
					TAP0: 172.16.1.4 > 172.16.1.9: ICMP echo request
08:34:40.932984 b6:10:1f:ef:9b:98 > 72:89:78:ff:88:ef, ethertype IPv4 (0x0800), length 46: (tos 0x0, ttl 255, id 0, offset 0, flags [none], proto ICMP (1), length 32)
    172.16.1.4 > 172.16.1.9: ICMP echo request, id 1000, seq 0, length 12
	0x0000:  7289 78ff 88ef b610 1fef 9b98 0800 4500  r.x...........E.
	0x0010:  0020 0000 0000 ff01 61af ac10 0104 ac10  ........a.......
	0x0020:  0109 0800 2c3e 03e8 0000 5465 7374       ....,>....Test
	
					TAP9: 172.16.1.9 > 172.16.1.4: ICMP echo reply
08:34:40.934584 72:89:78:ff:88:ef > b6:10:1f:ef:9b:98, ethertype IPv4 (0x0800), length 46: (tos 0x0, ttl 64, id 45280, offset 0, flags [none], proto ICMP (1), length 32)
    172.16.1.9 > 172.16.1.4: ICMP echo reply, id 1000, seq 0, length 12
	0x0000:  b610 1fef 9b98 7289 78ff 88ef 0800 4500  ......r.x.....E.
	0x0010:  0020 b0e0 0000 4001 6fcf ac10 0109 ac10  ......@.o.......
	0x0020:  0104 0000 343e 03e8 0000 5465 7374       ....4>....Test		

	
---------------------------------------------------------------------------------------------------------------
20161022:
		Se recibe el frame de respuesta

 eth.c:tapif_input:515:
 eth.c:low_level_input:489:
 eth.c:low_level_input:494:Frame receiced: len=42 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< ARP REPLY 
 eth.c:reply:671:err=0 may_block=0
 eth.c:reply:688:clockTicks =100
 eth.c:reply:693:now =3
 eth.c:reply:696:source=134515156 type=2325 m3i1=0 m3i2=9 m3p1=(nil) m3ca1=[]
 eth.c:main:837:ETH: source=9 type=2051 m2i1=0 m2i2=9 m2i3=46 m2l1=0 m2l2=3 m2p1=0x9881000
 eth.c:do_vwrite:277:from_int=0 vectored=0
 eth.c:do_vwrite:278:source=9 type=2051 m2i1=0 m2i2=9 m2i3=46 m2l1=0 m2l2=3 m2p1=0x9881000
 eth.c:do_vwrite:321:copy frame from INET to ETH
 eth.c:do_vwrite:327:Frame copied [r‰xÿˆï]
 eth.c:tapif_input:515:
 eth.c:low_level_input:489:
 eth.c:low_level_input:494:Frame receiced: len=46 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< PING REPLY 
 eth.c:reply:671:err=0 may_block=0
 eth.c:reply:688:clockTicks =100
 eth.c:reply:693:now =3
 eth.c:reply:696:source=1514 type=2325 m3i1=0 m3i2=9 m3p1=(nil) m3ca1=[]
 eth.c:tapif_input:515:
 eth.c:low_level_input:489:
 eth.c:low_level_input:494:Frame receiced: len=42 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< ARP REQUEST 
 eth.c:tapif_input:515:
 eth.c:low_level_input:489:
 eth.c:low_level_input:494:Frame receiced: len=42  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< ARP REQUEST 
 eth.c:tapif_input:515:
 eth.c:low_level_input:489:
 eth.c:low_level_input:494:Frame receiced: len=42  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< ARP REQUEST 
		
		Se modifico minix.sh para setear la MAC ADDRESS del tap9 de pruebas.
			ip tuntap add dev tap9 mode tap
			ip link set dev tap9 address 72:89:78:FF:88:EF
			ip link set dev tap9 up 
			ifconfig tap9 172.16.1.9 netmask 255.255.255.0 
---------------------------------------------------------------------------------------------------------------
20161105:
			INET --sendrec-> SYSTASK
			SYSTASK---->INET 
[  732.455496] DEBUG INET:sleep_proc:671: endpoint=9 flags=0 cpuid=0
[  732.455499] DEBUG INET:sleep_proc:672: WLOCK_PROC ep=9 count=0
[  732.455504] DEBUG INET:sleep_proc:698: nr=9 endp=9 vmid=0 lpid=INET p_cpumask=FF name=inet 
[  732.455507] DEBUG INET:sleep_proc:700: someone wakeups me: sem=0 p_rcode=0
[  732.455510] DEBUG INET:mm_mini_sendrec:704: WUNLOCK_PROC ep=9 count=0

			El valor retornado por SYSTASK hace que INET finelice
[  732.455872] inet[INET]: segfault at 8 ip 08063a33 sp bfc758d0 error 4 in inet[8048000+33000] 
 

		INET -->SENDREC(1553=0x611=SYS_VIRVCOPY)->SYTASK
  system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=9 type=1553 m4l1=0 m4l2=0 m4l3=1 m4l4=135233440 m4l5=0
 system.c:main:137:call_nr=17 who_e=9
 system.c:main:142:Calling vector 17
SYSTASK: got unused request 1553 from 9
 system.c:main:153:REPLY msg:source=9 type=-107 m1i1=0 m1i2=0 m1i3=1 m1p1=0x80f7fa0 m1p2=(nil) m1p3=(nil)
 
 system.c:main:116:SYSTASK is waiting for requests
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=0 type=1538 m4l1=9 m4l2=1 m4l3=11 m4l4=35534 m4l5=137051600
 system.c:main:137:call_nr=2 who_e=0
 system.c:main:142:Calling vector 2
 do_exit.c:do_exit:31:proc_ep=9
 do_exit.c:do_exit:43:before nr=9 endp=9 vmid=0 flags=8 misc=120 lpid=2749 nodeid=0 nodemap=1 name=inet 
 do_exit.c:do_exit:54:vm_nr_tasks=35 proc_nr=9 vm_nr_sysprocs=64
 do_exit.c:do_exit:60:s_nr=44 s_endpoint=27342 s_flags=0 s_owner=-1
 do_exit.c:do_exit:61:proc_nr=9 inet
 slots.c:mcast_exit_proc:226:nr=9 endp=9 vmid=0 flags=8 misc=120 lpid=2749 nodeid=0 nodemap=1 name=inet 
 do_exit.c:do_exit:76:NOTIFY proc_ep=9
 do_exit.c:do_exit:80:mnx_wait4unbind_T
 do_exit.c:do_exit:83:mnx_wait4unbind_T  rcode=0
 do_exit.c:do_exit:91:endpoint 9 unbound
 do_exit.c:do_exit:96:proc_nr=9 free_slots=193
 timers.c:tmrs_clrtimer:17:prev_time=0
 do_exit.c:do_exit:107:after nr=9 endp=9 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
 system.c:main:153:REPLY msg:source=0 type=0 m1i1=9 m1i2=1 m1i3=11 m1p1=0x8ace m1p2=0x82b3dd0 m1p3=(nil)
  
---------------------------------------------------------------------------------------------------------------
20161106:
 
		SE implemento SYS_VIRVCOPY porque la usa INET
		SE probó NWIOSIPCONF Y  NWIOGIPCONF
 inet.c:main:134:mnx_receive
 inet.c:main:140:source=1 type=1029 m2i1=4 m2i2=1 m2i3=0 m2l1=1074818592 m2l2=135234848 m2p1=0xbfa5cff0
 inet.c:main:145:message from FS_PROC_NR
 sr.c:sr_rec:120:sr_rec m_type=1029
 sr.c:sr_rwio:277:source=1 type=1029 m2i1=4 m2i2=1 m2i3=0 m2l1=1074818592 m2l2=135234848 m2p1=0xbfa5cff0
 sr.c:sr_rwio:282:minor=4 m_type=405 request=40106E20
 sr.c:sr_getchannel:618:minor=4
 sr.c:sr_getchannel:628:srf_port=0 srf_flags=2 1 2 
 sr.c:sr_rwio:303:type=1029
 sr.c:sr_rwio:340:request=40106E20
 sr.c:sr_rwio:364:srf_fd=3 request=40106E20
 generic/ip_ioctl.c:ip_ioctl:50:fd=3 req=40106E20
 generic/ip_ioctl.c:ip_ioctl:175:ip_port:0 req=40106E20 
 generic/ip_ioctl.c:ip_ioctl:191:req=NWIOSIPCONF
 sr.c:sr_get_userdata:687:fd=4 offset=0 count=16 for_ioctl=1
 sr.c:cp_u2b:860:proc=1 size=16 
 generic/ip_ioctl.c:ip_ioctl:195:req=NWIOSIPCONF acc_length=16
 generic/ip_ioctl.c:ip_setconf:474:ip_port_nr=0  nwic_flags=1 
 generic/ip_ioctl.c:ip_setconf:500:nwic_ipaddr=A0A0A0A
 generic/eth.c:eth_ioctl:174:fd=1 req=0x40146e10
 ipeth_main:95:de_state=2
 generic/ip_ioctl.c:ip_ioctl:50:fd=0 req=40386E22
 generic/ip_ioctl.c:ip_ioctl:50:fd=1 req=40386E22
 generic/ip_ioctl.c:ip_ioctl:50:fd=1 req=80106E21
 generic/ip_ioctl.c:ip_ioctl:50:fd=2 req=40386E22
 generic/ip_ioctl.c:ip_ioctl:50:fd=2 req=80106E21
 inet.c:main:134:mnx_receive
 inet.c:main:140:source=1 type=1029 m2i1=4 m2i2=1 m2i3=0 m2l1=-2146406879 m2l2=0 m2p1=0xbfa5cff0
 inet.c:main:145:message from FS_PROC_NR
 sr.c:sr_rec:120:sr_rec m_type=1029
 sr.c:sr_rwio:277:source=1 type=1029 m2i1=4 m2i2=1 m2i3=0 m2l1=-2146406879 m2l2=0 m2p1=0xbfa5cff0
 sr.c:sr_rwio:282:minor=4 m_type=405 request=80106E21
 sr.c:sr_getchannel:618:minor=4
 sr.c:sr_getchannel:628:srf_port=0 srf_flags=2 1 2 
 sr.c:sr_rwio:303:type=1029
 sr.c:sr_rwio:340:request=80106E21
 sr.c:sr_rwio:364:srf_fd=3 request=80106E21
 generic/ip_ioctl.c:ip_ioctl:50:fd=3 req=80106E21
 inet.c:main:134:mnx_receive
 
--------------------------------------------------------------------------------------------------------------- 

	INET
 inet.c:main:134:mnx_receive
 inet.c:main:140:source=1 type=1028 (0x404-DEV_WRITE) m2i1=4 m2i2=1 m2i3=30 m2l1=134550363 m2l2=0 m2p1=0xbf872ff0
 inet.c:main:145:message from FS_PROC_NR
 sr.c:sr_rec:120:sr_rec m_type=1028
 sr.c:sr_rwio:277:source=1 type=1028 m2i1=4 m2i2=1 m2i3=30 m2l1=134550363 m2l2=0 m2p1=0xbf872ff0
 sr.c:sr_rwio:282:minor=4 m_type=404 request=805135B
 sr.c:sr_getchannel:618:minor=4
 sr.c:sr_getchannel:628:srf_port=0 srf_flags=2 1 2 
 sr.c:sr_getchannel:633:return srf_fd=3
 generic/ip_write.c:ip_write:34:fd=3 count=30
 sr.c:sr_get_userdata:689:fd=4 offset=0 count=30 for_ioctl=0
 sr.c:cp_u2b:862:proc=1 size=30 
 generic/ip_write.c:ip_send:68:fd=3 data_len=30
 generic/ip_write.c:ip_send:279:dstaddr=DEADBEAF type=0
 generic/ip_eth.c:ipeth_send:255:dest=DEADBEAF type=0
 sr.c:sr_get_userdata:689:fd=4 offset=30 count=0 for_ioctl=0
 sr.c:sr_get_userdata:717:result=30 is_revive=0 source=1 type=1028 m2i1=4 m2i2=1 m2i3=30 m2l1=134550363 m2l2=0 m2p1=0xbf872ff0
 mnx_eth.c:eth_write_port:236:dst=6 source=0 type=2051 (0x804-DL_WRITE) m2i1=0 m2i2=9 m2i3=60 m2l1=0 m2l2=134514344 m2p1=0x80eeb30
 mnx_eth.c:eth_write_port:280:m_type=915 DL_PORT=0 etp_port=0
 mnx_eth.c:eth_write_port:282:DL_PROC=9 this_proc=9 DL_STAT=1 
 
	INET seteo una ALARMA por lo que seguramente SYSTASK le esta notificando el vencimiento de la alarma.
 inet.c:main:134:mnx_receive 
 inet.c:main:140:source=-2 type=4129 (NOTIFY_MESSAGE+0x21=> p_nr= 33-35 =-2 = SYSTASK) m2i1=0 m2i2=0 m2i3=1478590748 m2l1=650811576 m2l2=0 m2p1=(nil)
eth_check_drivers: got a notification from -2
eth_check_drivers: got bad getname reply (-107) from -2
 inet.c:main:134:mnx_receive
 
	SYSTASK
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=9 type=1560 (0x618 SYS_SETALARM) m4l1=6 m4l2=0 m4l3=0 m4l4=51 m4l5=1300000000
 system.c:main:137:call_nr=24 who_e=9
 system.c:main:142:Calling vector 24
 do_setalarm.c:do_setalarm:32:who_p=9 who_e=9 exp_time=51, use_abs_time=0
 do_setalarm.c:do_setalarm:40:PRIV s_id=44 s_warn=27342 s_level=0 trap=0 call=0
 clock.c:get_uptime:205:tv_sec=86 tv_nsec=145658724 uptime=8614 
 do_setalarm.c:do_setalarm:49:now=8614 init_time=6256 exp_time=51
 clock.c:set_timer:217:exp_time=8665
 timers.c:tmrs_settimer:83:exp_time=8665
 timers.c:tmrs_clrtimer:17:prev_time=0
 clock.c:get_uptime:205:tv_sec=86 tv_nsec=145711102 uptime=8614 
 clock.c:set_timer:220:realtime=2358 exp_time=8665 next_timeout=8665 
 system.c:main:153:REPLY msg:source=9 type=0 m1i1=6 m1i2=0 m1i3=0 m1p1=(nil) m1p2=0x4d7c6d00 m1p3=(nil) 
 
  
 system.c:main:120:mnx_receive rcode=0
 system.c:main:128:RECEIVE msg:source=9 type=2058 (DL_GETNAME) m4l1=0 m4l2=0 m4l3=1478590748 m4l4=650811576 m4l5=0
 system.c:main:137:call_nr=522 who_e=9
 system.c:main:153:REPLY msg:source=9 type=-107(EMOLBADREQUEST) m1i1=0 m1i2=0 m1i3=1478590748 m1p1=0x26ca98b8 m1p2=(nil) m1p3=(nil)
 
	ETH recibe el pedido de write 
  eth.c:main:837:ETH: source=9 type=2051 (0x804-DL_WRITE) m2i1=0 m2i2=9 m2i3=60 m2l1=0 m2l2=134514344 m2p1=0x80eeb30
 eth.c:do_vwrite:277:from_int=0 vectored=0
 eth.c:do_vwrite:278:source=9 type=2051 (0x804-DL_WRITE) m2i1=0 m2i2=9 m2i3=60 m2l1=0 m2l2=134514344 m2p1=0x80eeb30
 eth.c:do_vwrite:321:copy frame from INET to ETH
 eth.c:do_vwrite:327:Frame copied [ÿÿÿÿÿÿ^p1¬Ÿé]
 eth.c:reply:671:err=0 may_block=0
 eth.c:reply:688:clockTicks =100
 eth.c:reply:693:now =1
 eth.c:reply:696:source=134515156 type=2325 (0x915=DL_TASK_REPLY) m3i1=0 m3i2=9 m3p1=(nil) m3ca1=[]
 
	El problema debe estar en SYSTASK que informa el vencimieto de una ALARMA en forma incorrecta.
	
EN REALIDAD el problema estaba en el mm_mini_notify. Se modificaron	
	- mol_macros.h  BUILD_NOTIFY_MSG
	- mol_ipc: mm_mini_notify y mm_mini_receive
	- mol_rproxy: notify_rmt2lcl
	
Estaba mal calculado en NOTIFY_FROM() que sumaba al p_nr el nr_tasks

ahora INET hace
	 inet.c:main:145:message from FS_PROC_NR
	 sr.c:sr_rec:120:sr_rec m_type=1028
	 sr.c:sr_rwio:277:source=1 type=1028 m2i1=4 m2i2=1 m2i3=30 m2l1=134550363 m2l2=0 m2p1=0xbf99dff0
	 sr.c:sr_rwio:282:minor=4 m_type=404 request=805135B
	 sr.c:sr_getchannel:618:minor=4
	 sr.c:sr_getchannel:628:srf_port=0 srf_flags=2 1 2 
	 sr.c:sr_getchannel:633:return srf_fd=3
	 generic/ip_write.c:ip_write:34:fd=3 count=30
	 sr.c:sr_get_userdata:689:fd=4 offset=0 count=30 for_ioctl=0
	 sr.c:cp_u2b:862:proc=1 size=30 
	 generic/ip_write.c:ip_send:68:fd=3 data_len=30
	 generic/ip_write.c:ip_send:279:dstaddr=DEADBEAF type=0
	 generic/ip_eth.c:ipeth_send:255:dest=DEADBEAF type=0
	 sr.c:sr_get_userdata:689:fd=4 offset=30 count=0 for_ioctl=0
	 sr.c:sr_get_userdata:717:result=30 is_revive=0 source=1 type=1028 m2i1=4 m2i2=1 m2i3=30 m2l1=134550363 m2l2=0 m2p1=0xbf99dff0
	 mnx_eth.c:eth_write_port:236:dst=6 source=0 type=2051 m2i1=0 m2i2=9 m2i3=60 m2l1=0 m2l2=134514344 m2p1=0x80eeb30
	 mnx_eth.c:eth_write_port:280:m_type=915 DL_PORT=0 etp_port=0
	 mnx_eth.c:eth_write_port:282:DL_PROC=9 this_proc=9 DL_STAT=1 
	 inet.c:main:134:mnx_receive
	 inet.c:main:140:source=-2 type=-35 m2i1=0 m2i2=0 m2i3=1478614267 m2l1=584927109 m2l2=0 m2p1=(nil)
	 inet.c:main:134:mnx_receive
	 inet.c:main:140:source=-2 type=-35 m2i1=0 m2i2=0 m2i3=1478614268 m2l1=586773839 m2l2=0 m2p1=(nil)
	 inet.c:main:134:mnx_receive
	 inet.c:main:140:source=-2 type=-35 m2i1=0 m2i2=0 m2i3=1478614269 m2l1=588090524 m2l2=0 m2p1=(nil)
	 inet.c:main:134:mnx_receive
	 inet.c:main:140:source=-2 type=-35 m2i1=0 m2i2=0 m2i3=1478614270 m2l1=588687016 m2l2=0 m2p1=(nil)
	 inet.c:main:134:mnx_receive
	 inet.c:main:140:source=-2 type=-35 m2i1=0 m2i2=0 m2i3=1478614271 m2l1=609202799 m2l2=0 m2p1=(nil)
	 inet.c:main:134:mnx_receive
	 inet.c:main:140:source=-2 type=-35 m2i1=0 m2i2=0 m2i3=1478614272 m2l1=610826367 m2l2=0 m2p1=(nil)
	 inet.c:main:134:mnx_receive
	 inet.c:main:140:source=-2 type=-35 m2i1=0 m2i2=0 m2i3=1478614273 m2l1=614665184 m2l2=0 m2p1=(nil)
	 inet.c:main:134:mnx_receive
 
 Seguramente queda esperando que ETH le informe que realizo el WRITE.
 ETH le responde a INET que copio en sus buffers el frame pero todavia no lo envio.
 	 eth.c:main:837:ETH: source=9 type=2051 m2i1=0 m2i2=9 m2i3=60 m2l1=0 m2l2=135223328 m2p1=0x80eeb30
	 eth.c:do_vwrite:277:from_int=0 vectored=0
	 eth.c:do_vwrite:278:source=9 type=2051 m2i1=0 m2i2=9 m2i3=60 m2l1=0 m2l2=135223328 m2p1=0x80eeb30
	 eth.c:do_vwrite:321:copy frame from INET to ETH
	 eth.c:do_vwrite:327:Frame copied [ÿÿÿÿÿÿ®’¶¾µ]
	 eth.c:reply:671:err=0 may_block=0
	 eth.c:reply:688:clockTicks =100
	 eth.c:reply:693:now =6
	 eth.c:reply:696:source=1514 type=2325 m3i1=0 m3i2=9 m3p1=(nil) m3ca1=[]

---------------------------------------------------------------------------------------------------------------
20161112:

[   61.605458] DEBUG 2762:mm_mini_sendrec:470: srcdst_ep=0
[   61.605466] DEBUG 2762:check_caller:157: caller_pid=2762 caller_tgid=ETH
[   61.605470] DEBUG 2762:check_caller:177: RLOCK_TASK pid=ETH count=0
[   61.605513] DEBUG 2762:check_caller:185: RUNLOCK_TASK pid=ETH count=0
[   61.605519] DEBUG 2762:check_caller:193: WLOCK_PROC ep=6 count=0
[   61.605592] DEBUG 2762:check_caller:217: nr=6 endp=6 vmid=0 flags=8 misc=20 lpid=ETH nodeid=0 nodemap=1 name=eth 
[   61.605634] DEBUG 2762:check_caller:218: WUNLOCK_PROC ep=6 count=0
[   61.605677] ERROR: 2762:check_caller:218: rcode=-106
[   61.605712] ERROR: 2762:mm_mini_sendrec:474: rcode=-106 (EMOLNOTREADY)    El main thread esta haciendo receive();
[   61.606097] DEBUG 2762:mm_exit_unbind:2986: WLOCK_TASK pid=2762 count=0
[   61.606101] DEBUG 2762:mm_exit_unbind:3111: WUNLOCK_TASK pid=2762 count=0


---------------------------------------------------------------------------------------------------------------
20161113:
		INET esta esperando algo de ETH 
		
INET:
	generic/eth.c:eth_init:97:
	mnx_eth.c:osdep_eth_init:42:
	mnx_eth.c:osdep_eth_init:62:ETH found
	mnx_eth.c:osdep_eth_init:83:source=-1218797666 type=2055 m2i1=0 m2i2=9 m2i3=134726136 m2l1=0 m2l2=134513704 m2p1=0xb76e1ac0

	
	[   68.077419] DEBUG INET:mm_mini_receive:269: src_ep=6
[   68.077422] DEBUG INET:check_caller:157: caller_pid=INET caller_tgid=INET
[   68.077426] DEBUG INET:check_caller:193: WLOCK_PROC ep=9 count=0
[   68.077429] DEBUG INET:check_caller:222: WUNLOCK_PROC ep=9 count=0
[   68.077432] DEBUG INET:check_caller:225: vmid=0
[   68.077435] DEBUG INET:check_caller:229: RLOCK_VM vm=0 count=0
[   68.077438] DEBUG INET:check_caller:233: RUNLOCK_VM vm=0 count=0
[   68.077441] DEBUG INET:check_caller:239: caller_pid=INET 
[   68.077444] DEBUG INET:mm_mini_receive:280: RLOCK_PROC ep=9 count=0
[   68.077447] DEBUG INET:mm_mini_receive:284: caller_nr=9 caller_ep=9 src_ep=6 
[   68.077450] DEBUG INET:mm_mini_receive:288: vmid=0
[   68.077453] DEBUG INET:mm_mini_receive:292: RUNLOCK_PROC ep=9 count=0
[   68.077456] DEBUG INET:mm_mini_receive:294: RLOCK_VM vm=0 count=0
[   68.077459] DEBUG INET:mm_mini_receive:297: RUNLOCK_VM vm=0 count=0
[   68.077537] DEBUG INET:mm_mini_receive:301: RLOCK_PROC ep=9 count=0
[   68.077603] DEBUG INET:mm_mini_receive:311: RUNLOCK_PROC ep=9 count=0
[   68.077663] DEBUG INET:mm_mini_receive:313: RLOCK_PROC ep=6 count=0
[   68.077726] DEBUG INET:mm_mini_receive:343: RUNLOCK_PROC ep=6 count=0
[   68.077743] DEBUG INET:mm_mini_receive:350: WLOCK_PROC ep=9 count=0
[   68.077747] DEBUG INET:mm_mini_receive:436: Any suitable message from 6 was not found.
	
	VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2679    0    0   80  27342  27342  27342  27342 systask        
 0   -2     -2   2678    0    8   20  31438  27342  27342  27342 systask        
 0    0      0   2681    0    8   A0  31438  27342  27342  27342 pm             
 0    1      1   2745    0    C   20      9      9  27342  27342 test_inet      
 0    6      6   2736    0    8   20  31438  27342  27342  27342 eth            
 0    9      9   2739    0    8   20      6  27342  27342  27342 inet    <<<<<<<<<<<<
 0   11     11   2750    0    0   20  27342  27342  27342  27342 init           
 0   29     29   2790    0    0   20  27342  27342  27342  27342 init           
 0   30     30   2791    0    0   20  27342  27342  27342  27342 init   
		
ETH0	CUANDO RECIBE UN SIGNAL TERIN
 
		[   66.437219] DEBUG ETH0:sleep_proc:671: endpoint=6 flags=0 cpuid=0
[   66.437222] DEBUG ETH0:sleep_proc:672: WLOCK_PROC ep=6 count=0
[   66.437227] DEBUG ETH0:sleep_proc:698: nr=6 endp=6 vmid=0 lpid=ETH0 p_cpumask=FF name=eth 
[   66.437230] DEBUG ETH0:sleep_proc:700: someone wakeups me: sem=0 p_rcode=0
[   66.437233] DEBUG ETH0:mm_mini_sendrec:704: WUNLOCK_PROC ep=6 count=0
[   66.437983] DEBUG ETH0:mm_mini_receive:269: src_ep=31438
[   66.437986] DEBUG ETH0:check_caller:157: caller_pid=ETH0 caller_tgid=ETH0
[   66.437990] DEBUG ETH0:check_caller:193: WLOCK_PROC ep=6 count=0
[   66.437993] DEBUG ETH0:check_caller:222: WUNLOCK_PROC ep=6 count=0
[   66.437996] DEBUG ETH0:check_caller:225: vmid=0
[   66.437998] DEBUG ETH0:check_caller:229: RLOCK_VM vm=0 count=0
[   66.438001] DEBUG ETH0:check_caller:233: RUNLOCK_VM vm=0 count=0
[   66.438003] DEBUG ETH0:check_caller:239: caller_pid=ETH0 
[   66.438006] DEBUG ETH0:mm_mini_receive:280: RLOCK_PROC ep=6 count=0
[   66.438009] DEBUG ETH0:mm_mini_receive:284: caller_nr=6 caller_ep=6 src_ep=31438 
[   66.438012] DEBUG ETH0:mm_mini_receive:288: vmid=0
[   66.438014] DEBUG ETH0:mm_mini_receive:292: RUNLOCK_PROC ep=6 count=0
[   66.438017] DEBUG ETH0:mm_mini_receive:294: RLOCK_VM vm=0 count=0
[   66.438020] DEBUG ETH0:mm_mini_receive:297: RUNLOCK_VM vm=0 count=0
[   66.438023] DEBUG ETH0:mm_mini_receive:350: WLOCK_PROC ep=6 count=0
[   66.438026] DEBUG ETH0:mm_mini_receive:436: Any suitable message from 31438 was not found.
[   66.438029] DEBUG ETH0:sleep_proc:646: timeout=50000
[   66.438032] DEBUG ETH0:sleep_proc:657: BEFORE DOWN lpid=ETH0 p_sem=0 timeout=50000
[   66.438035] DEBUG ETH0:sleep_proc:659: WUNLOCK_PROC ep=6 count=0
[   66.438037] DEBUG ETH0:sleep_proc:660: endpoint=6 flags=8
[   67.377514] DEBUG ETH0:sleep_proc:671: endpoint=6 flags=8 cpuid=0
[   67.377601] DEBUG ETH0:sleep_proc:672: WLOCK_PROC ep=6 count=0
[   67.377609] DEBUG ETH0:sleep_proc:675: pid=ETH0 ret=-512  <<<< ERESTARTSYS    
[   67.377633] DEBUG ETH0:sleep_proc:698: nr=6 endp=6 vmid=0 lpid=ETH0 p_cpumask=FF name=eth 
[   67.377636] DEBUG ETH0:sleep_proc:700: someone wakeups me: sem=0 p_rcode=-512
[   67.377640] DEBUG ETH0:mm_mini_receive:453: WUNLOCK_PROC ep=6 count=0
[   67.377643] ERROR: ETH0:mm_mini_receive:454: rcode=-512

---------------------------------------------------------------------------------------------------------------
20161119:
	Se implemento mnx_wakeup(vmid, ep) que despierta de su espera a mnx_receive(ANY) O mnx_rcvrqst() 
	o deja un flag MIS_BIT_WOKENUP para que cuando estas funciones sean invocadas, terminen con error EMOLWOKENUP 
	
		mnx_wakeup(0, 6);
[  158.262347] DEBUG 2754:mm_wakeup:3141: vmid=0 proc_ep=6
[  158.262402] DEBUG 2754:mm_wakeup:3150: caller_pid=2754 caller_tgid=2754
[  158.262485] DEBUG 2754:mm_wakeup:3155: RLOCK_VM vm=0 count=0
[  158.262636] DEBUG 2754:mm_wakeup:3158: RUNLOCK_VM vm=0 count=0
[  158.262747] DEBUG 2754:mm_wakeup:3176: WLOCK_PROC ep=6 count=0
[  158.262806] DEBUG 2754:mm_wakeup:3178: nr=6 endp=6 vmid=0 flags=8 misc=20 lpid=2741 nodeid=0 nodemap=1 name=eth 
[  158.262890] DEBUG 2754:inherit_cpu:365: cpuid=0
[  158.262900] DEBUG 2754:inherit_cpu:373: nr=6 endp=6 vmid=0 lpid=2741 p_cpumask=FF name=eth 
[  158.262937] DEBUG 2754:mm_wakeup:3253: BEFORE UP lpid=2741 p_sem=-1 rcode=-348
[  158.263000] DEBUG 2754:mm_wakeup:3266: WUNLOCK_PROC ep=6 count=0

	EL PROCESO EN ESPERA SE DESPIERTA terminando con  EMOLWOKENUP
[  158.263175] DEBUG 2741:sleep_proc:676: endpoint=6 ret=35531 p_rcode=-348
[  158.263179] DEBUG 2741:sleep_proc:678: endpoint=6 flags=0 cpuid=0
[  158.263183] DEBUG 2741:sleep_proc:680: WLOCK_PROC ep=6 count=0
[  158.263194] DEBUG 2741:sleep_proc:695: pid=2741 ret=-348
[  158.263204] DEBUG 2741:sleep_proc:718: nr=6 endp=6 vmid=0 lpid=2741 p_cpumask=FF name=eth 
[  158.263208] DEBUG 2741:sleep_proc:720: someone wakeups me: sem=0 p_rcode=-348
[  158.263212] DEBUG 2741:mm_mini_receive:453: WUNLOCK_PROC ep=6 count=0
[  158.263216] ERROR: 2741:mm_mini_receive:454: rcode=-348

---------------------------------------------------------------------------------------------------------------
20161120:
		Las pruebas de ETH funcionan bastante bien. 
	

---------------------------------------------------------------------------------------------------------------
20161120:
		Quiero obtener la MAC ADDRESS de la placa Ethernet	
			test_inet --> inet --> Eth
		
		Anduvo bien y ahora no anda mas.
 
 
---------------------------------------------------------------------------------------------------------------
20161126:		
		Funciona bien el envio de ARP 
		
01:55:25.469227 f6:db:67:56:ea:7d > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 60: Ethernet (len 6), IPv4 (len 4), Request who-has 172.16.1.9 tell 172.16.1.4, length 46
	0x0000:  ffff ffff ffff f6db 6756 ea7d 0806 0001  ........gV.}....
	0x0010:  0800 0604 0001 f6db 6756 ea7d ac10 0104  ........gV.}....
	0x0020:  0000 0000 0000 ac10 0109 0000 0000 0000  ................
	0x0030:  0000 0000 0000 0000 0000 0000            ............
01:55:25.471731 72:89:78:ff:88:ef > f6:db:67:56:ea:7d, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Reply 172.16.1.9 is-at 72:89:78:ff:88:ef, length 28
	0x0000:  f6db 6756 ea7d 7289 78ff 88ef 0806 0001  ..gV.}r.x.......
	0x0010:  0800 0604 0002 7289 78ff 88ef ac10 0109  ......r.x.......
	0x0020:  f6db 6756 ea7d ac10 0104                 ..gV.}....

		
		TEST_INET
test_inet.c:main:214:TEST ARP minor_eth=5
test_inet.c:main:284:TEST ARP source=9 type=1028 m2i1=5 m2i2=1 m2i3=60 m2l1=-1215946752 m2l2=86 m2p1=0x9ccd000
test_inet.c:main:288:TEST ARP source=9 type=67 m2i1=1 m2i2=60 m2i3=129 m2l1=-1215946752 m2l2=86 m2p1=0xb7862000
		
		INET
 inet.c:main:134:mnx_receive
 inet.c:main:140:source=1 type=1028 m2i1=5 m2i2=1 m2i3=60 m2l1=-1216327680 m2l2=66 m2p1=0x8fba000
 inet.c:main:145:message from FS_PROC_NR
 sr.c:sr_rec:120:sr_rec m_type=1028
 sr.c:sr_rwio:277:source=1 type=1028 m2i1=5 m2i2=1 m2i3=60 m2l1=-1216327680 m2l2=66 m2p1=0x8fba000
 sr.c:sr_getchannel:621:minor=5
 sr.c:sr_getchannel:631:srf_port=0 srf_flags=2 1 2 
 sr.c:sr_getchannel:636:return srf_fd=2
 sr.c:sr_rwio:292:DEV_WRITE type=1028
 sr.c:sr_rwio:333:DEV_WRITE type=1028
 generic/eth.c:eth_write:387:fd=2 count=60 etp_port=0 <<<<<<< ARP REQUEST
 sr.c:sr_get_userdata:692:fd=5 offset=0 count=60 for_ioctl=0
 sr.c:cp_u2b:868:proc=1 size=60 
 generic/eth.c:eth_send:452:fd=2 data_len=60 etp_port=0
 mnx_eth.c:eth_write_port:192:eth_port=0
 mnx_eth.c:eth_write_port:238:dst=6 source=-1080267560 type=2051 m2i1=0 m2i2=9 m2i3=60 m2l1=0 m2l2=5 m2p1=0x80f07d0
 mnx_eth.c:eth_write_port:282:m_type=915 DL_PORT=0 etp_port=0
 mnx_eth.c:eth_write_port:284:DL_PROC=9 this_proc=9 DL_STAT=3 
 generic/eth.c:reply_thr_get:1096:ef_srfd=5, result=60, for_ioctl=0
 sr.c:sr_get_userdata:692:fd=5 offset=60 count=0 for_ioctl=0
 sr.c:sr_get_userdata:720:result=60 is_revive=0 source=1 type=1028 m2i1=5 m2i2=1 m2i3=60 m2l1=-1216327680 m2l2=66 m2p1=0x8fba000
 sr.c:sr_rwio:372:
 mnx_eth.c:eth_recvev:682:
 mnx_eth.c:read_int:536:port=0 count=42
 generic/eth.c:eth_arrive:780:eth_port=0 pack_size=42 
 mol_time.c:mol_time:12:source=0 type=0 m2i1=6 m2i2=0 m2i3=0 m2l1=110 m2l2=1600000000 m2p1=(nil)
 mnx_eth.c:setup_read:563:etp_flags=3 etp_port=0 etp_ethaddr=4A:D9:65:42:17:88
 mnx_eth.c:setup_read:588:eth0: mnx_sending DL_READV
 mnx_eth.c:setup_read:590:source=40 type=2054 m2i1=0 m2i2=9 m2i3=3 m2l1=135204352 m2l2=128 m2p1=0x926f144
 mnx_eth.c:setup_read:592:source=6 type=2325 m2i1=0 m2i2=9 m2i3=0 m2l1=0 m2l2=1 m2p1=0x12f
 mnx_eth.c:setup_read:639:m_type=915 DL_PORT=0 etp_port=0
 mnx_eth.c:setup_read:641:DL_PROC=9 this_proc=9 DL_STAT=0 
 mnx_eth.c:eth_mnx_sendev:707:
 mnx_eth.c:write_int:513:port=0
 generic/eth.c:eth_arrive:780:eth_port=0 pack_size=60 <<<<<<< ARP REPLY
 generic/eth.c:packet2user:962:
 generic/eth.c:reply_thr_put:1107:ef_srfd=0, result=60, for_ioctl=0
 generic/eth.c:eth_read:523:fd=1 count=1514
 generic/eth.c:packet2user:962:
 generic/eth.c:eth_restart_write:748:eth_port=0		 
	 
	 ETH 
 eth.c:ec_user2nic:333:ec_port=0 offset=0 nic_addr=8055EF8 count=60
 eth.c:frame_send:700:Copy IO vector from INET to TAP
 eth.c:frame_send:703:iod_iovec_s=1  iod_proc_nr=9
 eth.c:frame_send:707:	 i=0 iov_addr=80F07D0 iov_size=60
 eth.c:frame_send:715:Frame copied
 eth.c:tapif_input:647:port=0
 eth.c:tapif_input:655:tapif_input: mnx_wakeup ret=-320
 eth.c:low_level_input:580:if_name=tap0 flags=0x     210
 eth.c:low_level_input:584:if_name=tap0 Frame received: length=42
 eth.c:low_level_input:593:ETH_HDR: dst=F6:DB:67:56:EA:7D src=72:89:78:FF:88:EF proto=608 <<<<<<<<<<<<<<<< ARP REPLY
 eth.c:low_level_input:596:IP_HDR: vers=0 tos=1 len=8 id=406 flag=200 ttl=72 proto=89 chk=FF78 src=10ACEF88 dst=DBF60901
 eth.c:frame_send:728:ETH_HDR: dst=FF:FF:FF:FF:FF:FF src=F6:DB:67:56:EA:7D proto=608 <<<<<<<<<<<<<<<< ARP REQUEST
 eth.c:frame_send:731:IP_HDR: vers=0 tos=1 len=8 id=406 flag=100 ttl=F6 proto=DB chk=5667 src=10AC7DEA dst=401
 eth.c:frame_send:738:ARP_HDR: htype=100 ptype=8 hlen=6 plen=4 opcode=256
 eth.c:frame_send:707:	 i=1 iov_addr=0 iov_size=0
 
PROBLEMA:
		ETH: Cuando el tread frame_send() hace VCOPY da error porque basicamente el que esta bindeado es MAIN.
			 eth.c:frame_send:710:mnx_vcopy rcode=-106 (EMOLNOTREADY)
			 Esto está indicando que MAIN esta haciendo seguramente su mnx_receive() o cualquier otro M3-IPC
			 
POSIBLE SOLUCION:
			Hacerle un mnx_wakeup indicando que es un send() 

	
---------------------------------------------------------------------------------------------------------------	
20161127:
		TEST_INET funciona razonablemente bien enviando:
		
	PRUEBA DE ARP: ARP->172.16.1.3
23:59:49.930699 66:ff:11:4e:16:01 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 60: Ethernet (len 6), IPv4 (len 4), Request who-has 172.16.1.3 tell 172.16.1.4, length 46
	0x0000:  ffff ffff ffff 66ff 114e 1601 0806 0001  ......f..N......
	0x0010:  0800 0604 0001 66ff 114e 1601 ac10 0104  ......f..N......
	0x0020:  0000 0000 0000 ac10 0103 0000 0000 0000  ................
	0x0030:  0000 0000 0000 0000 0000 0000            ............
23:59:49.931492 66:ff:11:4e:16:01 > 66:ff:11:4e:16:01, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Reply 172.16.1.3 is-at 66:ff:11:4e:16:01, length 28
	0x0000:  66ff 114e 1601 66ff 114e 1601 0806 0001  f..N..f..N......
	0x0010:  0800 0604 0002 66ff 114e 1601 ac10 0103  ......f..N......
	0x0020:  66ff 114e 1601 ac10 0104                 f..N......

	PRUEBA DE PING: Aparentemente en forma automatica hace un ARP 172.16.1.9 
	con un intervalo de tiempo aprox 51 segundos (153/3)
23:59:52.956386 66:ff:11:4e:16:01 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 60: Ethernet (len 6), IPv4 (len 4), Request who-has 172.16.1.9 tell 172.16.1.4, length 46
	0x0000:  ffff ffff ffff 66ff 114e 1601 0806 0001  ......f..N......
	0x0010:  0800 0604 0001 66ff 114e 1601 ac10 0104  ......f..N......
	0x0020:  0000 0000 0000 ac10 0109 adde adde adde  ................
	0x0030:  adde adde adde adde adde adde            ............
23:59:52.956476 66:ff:11:4e:16:01 > 66:ff:11:4e:16:01, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Reply 172.16.1.9 is-at 66:ff:11:4e:16:01, length 28
	0x0000:  66ff 114e 1601 66ff 114e 1601 0806 0001  f..N..f..N......
	0x0010:  0800 0604 0002 66ff 114e 1601 ac10 0109  ......f..N......
	0x0020:  66ff 114e 1601 ac10 0104                 f..N......
00:00:43.581613 66:ff:11:4e:16:01 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 60: Ethernet (len 6), IPv4 (len 4), Request who-has 172.16.1.9 tell 172.16.1.4, length 46
	0x0000:  ffff ffff ffff 66ff 114e 1601 0806 0001  ......f..N......
	0x0010:  0800 0604 0001 66ff 114e 1601 ac10 0104  ......f..N......
	0x0020:  0000 0000 0000 ac10 0109 adde adde adde  ................
	0x0030:  adde adde adde adde adde adde            ............
00:00:43.581668 66:ff:11:4e:16:01 > 66:ff:11:4e:16:01, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Reply 172.16.1.9 is-at 66:ff:11:4e:16:01, length 28
	0x0000:  66ff 114e 1601 66ff 114e 1601 0806 0001  f..N..f..N......
	0x0010:  0800 0604 0002 66ff 114e 1601 ac10 0109  ......f..N......
	0x0020:  66ff 114e 1601 ac10 0104                 f..N......
	
	Se modifico para utilizar M3IPC_LOCK que bloquea al resto cuando alguien usa M3_IPC.
	Se inhibio por ahora el thread_receive 

IMPORTANTE:
	Se resolvio el tema del envio de frames haciendolo sincronico. Directamente el MAIN thread
	espera que el SEND_THREAD haga el write tap0 para luego retornar a INET que el frame se envio.
	
---------------------------------------------------------------------------------------------------------------
2016130: hice mnx_hdw_notify(vmid, endpoint) como una derivacion  de mnx_notify
		Equivalente a HARDWARE NOTIFY. No necesita estar bindeado el emisor
		envia el notify en nombre de hardware y el value puede ser (0 <= value < nr_sys_procs) 

		CUANDO SE ESTA ESPERANDO EL MENSAJE EN RECEIVE
[  309.541445] DEBUG 2712:mm_mini_notify:736: src_nr=-1 dst_ep=10 update_proc=0 
[  309.541481] DEBUG 2712:check_caller:157: caller_pid=2712 caller_tgid=2712
[  309.541531] ERROR: 2712:check_caller:191: rcode=-310
[  309.541630] DEBUG 2712:mm_hdw_notify:1811: vmid=0 dst_ep=10 
[  309.541701] DEBUG 2712:mm_hdw_notify:1820: RLOCK_VM vm=0 count=0
[  309.541748] DEBUG 2712:mm_hdw_notify:1823: RUNLOCK_VM vm=0 count=0
[  309.541839] DEBUG 2712:mm_hdw_notify:1838: WLOCK_PROC ep=10 count=0
[  309.541926] DEBUG 2712:mm_hdw_notify:1859: RLOCK_PROC ep=-1 count=0
[  309.541975] DEBUG 2712:mm_hdw_notify:1864: destination is waiting. Build the message and wakeup destination
[  309.542058] DEBUG 2712:mm_hdw_notify:1867: source=-1 type=4106 m9i1=0 m9l1=0 m9t1.tv_sec=1480551065 m9t1.tv_nsec=541128460
[  309.542121] DEBUG 2712:inherit_cpu:357: cpuid=0
[  309.542129] DEBUG 2712:inherit_cpu:365: nr=10 endp=10 vmid=0 lpid=2711 p_cpumask=FF name=test_receive 
[  309.542129] DEBUG 2712:mm_hdw_notify:1872: BEFORE UP lpid=2711 p_sem=-1 rcode=0
[  309.542129] DEBUG 2712:mm_hdw_notify:1888: RUNLOCK_PROC ep=-1 count=0
[  309.542129] DEBUG 2712:mm_hdw_notify:1889: WUNLOCK_PROC ep=10 count=0

		CUANDO NO SE ESTA ESPERANDO EL MENSAJE EN RECEIVE
[  572.571124] DEBUG 2747:mm_mini_notify:736: src_nr=-1 dst_ep=10 update_proc=0 
[  572.571131] DEBUG 2747:check_caller:157: caller_pid=2747 caller_tgid=2747
[  572.571134] ERROR: 2747:check_caller:191: rcode=-310
[  572.571138] DEBUG 2747:mm_hdw_notify:1811: vmid=0 dst_ep=10 
[  572.571141] DEBUG 2747:mm_hdw_notify:1820: RLOCK_VM vm=0 count=0
[  572.571144] DEBUG 2747:mm_hdw_notify:1823: RUNLOCK_VM vm=0 count=0
[  572.571147] DEBUG 2747:mm_hdw_notify:1838: WLOCK_PROC ep=10 count=0
[  572.571150] DEBUG 2747:mm_hdw_notify:1859: RLOCK_PROC ep=-1 count=0
[  572.571244] DEBUG 2747:mm_hdw_notify:1876: destination is not waiting dst_ptr->p_usr.p_rts_flags=0 s_id=45 << (35+10)
[  572.571308] DEBUG 2747:mm_hdw_notify:1886: set_sys_bit s_notify_pending=0 s_id=8192
[  572.571323] DEBUG 2747:mm_hdw_notify:1888: RUNLOCK_PROC ep=-1 count=0
[  572.571327] DEBUG 2747:mm_hdw_notify:1889: WUNLOCK_PROC ep=10 count=0
[  572.571356] ERROR: 2747:mm_mini_notify:746: rcode=-330
	
---------------------------------------------------------------------------------------------------------------
20161204: 
20161205: 
20161206:
20161210:
20161211:
				Se implemento FS de Diego version 20161105
				Se corrigieron errores hasta que booteo el FS
				Se implemento MOLLIB con las llamadas al sistema de FS. ATENCION hay que hacer modificaciones

---------------------------------------------------------------------------------------------------------------
20161214:
				Se comenzo con RS en reemplazo de DEMONIZE como comando solo

---------------------------------------------------------------------------------------------------------------
20161215:
				Se termino RS utilizando LINUX message queues para comunicarse
				con demonize que es solo linea de comando.


---------------------------------------------------------------------------------------------------------------
20161216:		Se puso en marcha el demonio TTY que lee el archivo de configuracion.

---------------------------------------------------------------------------------------------------------------
20161217:
20161218:
				Se puso en marcha el demonio TTY que arranca el ciclo.
				ATENCION: Tuve que cambiar el posix_align por estructuras ESTATICAS porque 
				no funcionaba correctamente la lectura del archivo de configuracion
				danto error en malloc (que usa la libreria configfile)
				
---------------------------------------------------------------------------------------------------------------
20161220:	Se Desarrollo el driver de TTY utilizando LINUX pseudoterminals			
			Funciona el OPEN de la PSEUDO TTY
			Funciona el  IOCTL - TIOCGETCFG (lee la configuracion obtenida del archivo config) 
			y TIOCGETPSE (info sobre la pseudo terminal)
				
 test_tty.c:main:34:TEST OPEN
 test_tty.c:main:41:OPEN source=0 type=1030 m2i1=0 m2i2=21 m2i3=256 m2l1=0 m2l2=0 m2p1=(nil)
 test_tty.c:main:45:OPEN source=5 type=68 m2i1=21 m2i2=0 m2i3=0 m2l1=0 m2l2=0 m2p1=(nil)
 test_tty.c:main:50:TEST DEV_IOCTL TIOCGETPSE 
 test_tty.c:main:59:DEV_IOCTL TIOCGETPSE source=5 type=1029 m2i1=0 m2i2=21 m2i3=-2130152442 m2l1=0 m2l2=0 m2p1=0x804d000
 test_tty.c:main:63:DEV_IOCTL TIOCGETPSE source=5 type=68 m2i1=21 m2i2=0 m2i3=0 m2l1=0 m2l2=0 m2p1=(nil)
 test_tty.c:main:64:DEV_IOCTL TIOCGETPSE master=3 slave=4 sname=/dev/pts/0
 test_tty.c:main:70:TEST DEV_IOCTL TIOCGETCFG 
 test_tty.c:main:79:DEV_IOCTL TIOCGETCFG source=5 type=1029 m2i1=0 m2i2=21 m2i3=-2145356795 m2l1=0 m2l2=0 m2p1=0x804e000
 test_tty.c:main:83:DEV_IOCTL TIOCGETCFG source=5 type=68 m2i1=21 m2i2=0 m2i3=0 m2l1=0 m2l2=0 m2p1=(nil)
 test_tty.c:main:85:DEV_IOCTL TIOCGETCFG major=5 minor=0 type=1 port=0 endpoint=0

---------------------------------------------------------------------------------------------------------------
20161223/24:	Se Desarrollo el WRITE. 
				El programa TEST_TTY utiliza el slave para trabajar.
				El TTY abre una tty real para enviar la salida.

---------------------------------------------------------------------------------------------------------------
20161225:		Se desarrollo el READ 

 test_tty.c:main:95:TEST TTY WRITE 
 test_tty.c:main:98:Starting thread for minor:0
 test_tty.c:slave_thread:158:/dev/pts/0
 test_tty.c:main:111:TEST TTY WRITE source=5 type=1028 m2i1=0 m2i2=21 m2i3=11 m2l1=0 m2l2=0 m2p1=0x804b1a5
 tty.c:main:140:source=21 type=1028 m2i1=0 m2i2=21 m2i3=11 m2l1=0 m2l2=0 m2p1=0x804b1a5
 tty.c:do_write:688:tty_events=0 index=0 minor=0 tty_incount=0 tty_outleft=0 tty_outcum=0 
 tty.c:do_write:699:MTX_LOCK tp->tty_Omutex 
 sys_vircopy.c:sys_vircopy:19:SYS_VIRCOPY request to SYSTEM(21) src_proc=35534 dst_proc=11 bytes=0
 tty.c:do_write:713:COND_SIGNAL tp->tty_Obarrier
 tty.c:do_write:714:COND_WAIT tp->tty_Mbarrier tp->tty_Omutex
 tty.c:out_thread:1160:out_buf=MOL login: <<<<<<<<<<< TEXTO ENVIADO 
 count=11
 tty.c:out_thread:1164:MTX_UNLOCK tp->tty_Omutex 
 tty.c:out_thread:1171:MTX_LOCK tp->tty_Omutex 
 tty.c:out_thread:1173:COND_SIGNAL tp->tty_Mbarrier
 tty.c:out_thread:1148:tty_events=0 index=0 minor=0 tty_incount=0 tty_outleft=0 tty_outcum=0 
 tty.c:out_thread:1149:COND_WAIT tp->tty_Obarrier tp->tty_Omutex
 tty.c:tty_reply:545:source=0 type=68 m2i1=21 m2i2=-998 m2i3=0 m2l1=0 m2l2=0 m2p1=(nil)
 tty.c:main:134:Waiting to receive a request message.
 test_tty.c:slave_thread:179:bytes=11 buf_in=[MOL login:] <<<<<<<<< TEXTO RECIBIDO
 test_tty.c:slave_thread:185:bytes=5 buf_out=[PAP] <<<<<<<<<<<<<<<< TEXTO RESPUETA 
 test_tty.c:main:115:TEST TTY WRITE source=5 type=68 m2i1=21 m2i2=-998 m2i3=0 m2l1=0 m2l2=0 m2p1=(nil)
 tty.c:in_thread:1102:MTX_LOCK tp->tty_Imutex 
 tty.c:in_thread:1115:remainder=5 bytes=5 in_ptr=PAP
 tty.c:in_thread:1128:tty_incount=5 remainder=0 bytes=0
 tty.c:in_thread:1089:tty_events=0 index=0 minor=0 tty_incount=5 tty_outleft=0 tty_outcum=0 
 tty.c:in_thread:1096:tty_incount=5 remainder=251 bytes=0
 tty.c:in_thread:1100:MTX_UNLOCK tp->tty_Imutex 
 test_tty.c:main:121:TEST TTY READ 
 test_tty.c:main:131:TEST TTY READ source=5 type=1027 m2i1=0 m2i2=21 m2i3=5 m2l1=0 m2l2=0 m2p1=0x804f020
 tty.c:main:140:source=21 type=1027 m2i1=0 m2i2=21 m2i3=5 m2l1=0 m2l2=0 m2p1=0x804f020
 tty.c:do_read:633:tty_events=0 index=0 minor=0 tty_incount=5 tty_outleft=0 tty_outcum=0 
 tty.c:in_transfer:1424:tty_events=0 index=0 minor=0 tty_incount=5 tty_outleft=0 tty_outcum=0 
 tty.c:in_transfer:1426:MTX_LOCK tp->tty_Imutex 
 tty.c:in_transfer:1469:buf=PAP
 sys_vircopy.c:sys_vircopy:19:SYS_VIRCOPY request to SYSTEM(35534) src_proc=21 dst_proc=5 bytes=0
 tty.c:tty_reply:545:source=0 type=68 m2i1=21 m2i2=5 m2i3=0 m2l1=0 m2l2=0 m2p1=(nil)
 tty.c:in_transfer:1494:MTX_UNLOCK tp->tty_Imutex 
 tty.c:main:134:Waiting to receive a request message.
 test_tty.c:main:135:TEST TTY READ source=5 type=68 m2i1=21 m2i2=5 m2i3=0 m2l1=0 m2l2=0 m2p1=(nil)
 test_tty.c:main:136:TEST TTY READ  usr_entry=PAP <<<<<<<<<<<< TEXTO RECIBIDO 
 
 
---------------------------------------------------------------------------------------------------------------
20161229:

ERROR:			Los procesos de prueba tal como test_tty no estan terminando correctamente.
				quedan en estado <defunct> y permanecen sin hacer mnx_exitunbind
SOLUCIONADO PARCIALMENTE:	Error en exit de linux, se volvio atras con el codigo exit_unbind()				
				
SOLUCIONADO			Convertir en demonio al child pero tiene un PROBLEMA!!!!
					requiere un doble fork, por lo cual el padre pierde el PID de su hijo y por lo 
					tanto no puede hacer el mnx_lclbind.
					SOLUCIONADO!
					
---------------------------------------------------------------------------------------------------------------
20161230:
				La SYSTASK en do_exit se cuelga porque cuando hace el mnx_ntfy_value el destinatario DEBERIA
				ser un proceso de SISTEMA (NR_SYS_PROCS) !!
				Si el proceso MORIBUNDO no recibe el NOTIFY, entonces queda esperando que alguien lo despierte.
				por lo que la SYSTASK no puede terminar el WAIT4UNBIND.

					rcode = mnx_ntfy_value(PM_PROC_NR, proc_ep, PM_PROC_NR);
					if( rcode < 0) 	ERROR_RETURN(rcode);
					#define TO_WAIT4UNBIND	100 /* miliseconds */
					TASKDEBUG("mnx_wait4unbind_T\n");
					do { 
						rcode = mnx_wait4unbind_T(proc_ep, TO_WAIT4UNBIND)
					
				Si el proceso no esperaria ser despertado e hiciese el unbind solo, entonces
				tanto PM como SYSTASK podrian dar error de ENDPOINT porque el proceso ya estaria muerto.
				
	PREGUNTA:	De que forma el proceso  MORIBUNDO continuara para poder hacer el unbind ???

---------------------------------------------------------------------------------------------------------------
20161231:

ERROR:			No funciona correctamente el mm_exit_unbind() 
				SYSTASK: do_exit
				// mnx_ntfy_value(src_nr, dst_ep, value)
				rcode = mnx_ntfy_value(PM_PROC_NR, proc_ep, PM_PROC_NR);
	SOLUCIONADO 
				
				
ERROR:			Las comprobaciones deberian ser 
					if( update_proc >= 0 &&  update_proc < (sizeof(update_t)*8<<<<<<<<<<<) ) 
	SOLUCIONADO
				

		
TODO:			El NOTIFY REMOTO no tiene el mismo comportamiento que el notify LOCAL.
				Se MODIFICO TODO para que cuando se envia a la maquina remota no solo se envie
				el HEADER sino tambien el mensaje similar al send()
	SOLUCIONADO
				Se probo en entorno LOCAL
				
					
---------------------------------------------------------------------------------------------------------------
20161231: 
TODO:			Probar notify REMOTO.
				modificar notify para 
				BUILD_NOTIFY_MSG(c_ptr, s_ep, s_nr, d_ptr )
	RESUELTO 						
				
---------------------------------------------------------------------------------------------------------------
20170101:
				Se implemento warn_ep PARA REMOTO
				NO PROBADO

				ULTIMA VERSION CON MULTIPLES LOCKS 
				
---------------------------------------------------------------------------------------------------------------
20170110:
				HASTA EL DIA DE LA FECHA SE PROBO EL KERNEL CON LOCKS DE VM SOLAMENTE PERO TIENE PROBLEMAS
				TAMBIEN SE OPTIMIZARON FUNCIONES INTERNAS DE LA PARTE DE IPC, PERO TODO SE VOLVIO ATRAS.
				ROLLBACK DE TODO A LA VERSION  20170101
---------------------------------------------------------------------------------------------------------------
20170110-14:
				Se hizo m3urlget,  websrv y webclt 
				funciona perfectamente

				Wait4bind devuelve el endpoint del proceso bindeado como resultado en 
				cualquiera de sus posibilidades.
				
							
---------------------------------------------------------------------------------------------------------------
20170121:
				Cuando hago un test_receive y le envio un SIGUSR1 el resultado es
				[ 1141.951724] DEBUG 2722:mm_mini_receive:474: WUNLOCK_PROC ep=5 count=0
				[ 1141.951770] ERROR: 2722:mm_mini_receive:475: rcode=-512

				mm_hdw_notify:
					Es SOLO para usarse en forma LOCAL 
					En general deberia usarse solo desde otro thread
					
				Se cambio INET para que utilizara mm_hdw_notify en lugar de mnx_wakeup

				Se probo todo el conjunto ETH, INET, TEST_INET 
				La secuencia verdadera seria  ETH, INET, FS, TEST_INET
				
				Cuando INET intenta copiar desde su buffer hacia el buffer del usuario, el proceso del usuario			
				deberia estar bloqueado, pero TEST_INET no lo está dado que simula ser el FS.
				Por lo tanto falla SYS_VCOPY
								
				 sr.c:sr_put_userdata:752:fd=5 offset=0  for_ioctl=0
				 sr.c:cp_b2u:918:proc=1
				 sr.c:cp_b2u:923:src=9 (INET) dst=1 (TEST_INET) size=60
				 sr.c:cp_b2u:937:src_e=9 src_off=80F05D0 dst_e=1 dst_off=8A2B000 count=60
				 sr.c:cp_b2u:948:source=-2 type=1553 m1i1=0 m1i2=1 m1i3=1 m1p1=0x80fa2a0 m1p2=(nil) m1p3=(nil) 
				 sr.c:cp_b2u:951:source=-2 type=-324 m1i1=0 m1i2=0 m1i3=1 m1p1=0x80fa2a0 m1p2=(nil) m1p3=(nil) 
				 generic/eth.c:packet2user:994:result=-324 size=60
				 generic/eth.c:reply_thr_put:1107:ef_srfd=5, result=-324, for_ioctl=0
				 sr.c:sr_put_userdata:752:fd=5 offset=-324  for_ioctl=0
			
		ASI LO VE SYSTASK 
				 system.c:main:128:RECEIVE msg:source=9 type=1553 m4l1=0 m4l2=1 m4l3=1 m4l4=135242400 m4l5=0
				 system.c:main:137:call_nr=17 who_e=9
				 system.c:main:142:Calling vector 17
				 do_vcopy.c:do_vcopy:31:VCP_VEC_SIZE=1, VCP_VEC_ADDR=80FA2A0, VCP_NR_OK=1
				 do_vcopy.c:do_vcopy:54:src_ep=9, src_off=80F05D0, dst_ep=1, dst_off=8A2B000, count=60
				 system.c:main:153:REPLY msg:source=9 type=-324 m1i1=0 m1i2=0 m1i3=1 m1p1=0x80fa2a0 m1p2=(nil) m1p3=(nil)
				 
---------------------------------------------------------------------------------------------------------------
20170122:		Se cambio do_vcopy.c, do_copy.c y eth.c 
				Donde hay copia de datos tal como 
				TASKDEBUG("src_ep=%d, src_off=%X, dst_ep=%d, dst_off=%X, count=%d\n",		
					req->src.proc_nr_e, req->src.offset,
					req->dst.proc_nr_e, req->dst.offset,
					req->count);
				REEMPLAZAR POR
				TASKDEBUG(VIRCP_FORMAT, VIRCP_FIELDS(req));

				
---------------------------------------------------------------------------------------------------------------
20170129:
				Se hicieron pruebas de FS con molTestLib
				Hubo varias modificaciones tanto a lib/mollib como a molTestLib original de DIEGO

---------------------------------------------------------------------------------------------------------------
20170130:				
				Se probó el mknod y mkdir y funcionan bien.
				
---------------------------------------------------------------------------------------------------------------
20170201:		Se desarrollo test_fs_inet01.c
					Se Arrancar el FS
					se hace mkdir  /dev
					se hace mknod  /dev/eth
					se hace mknod  /dev/ip
					se hace link /dev/eth /dev/eth0	
					se hace link /dev/ip  /dev/ip0	
				
				ATENCION: OJO!!  todos los PATH daban error -71 EMOLADDRNOTAVAIL porque
				los string de origen eran staticos
				se cambiaron por posix_malign
				
---------------------------------------------------------------------------------------------------------------
20170212:		Pruebas de TAP local utilizando programa simpletun.c#L216
#		Topology:
#							NODE 0								
#			tap0 -----br0------eth0 
#			tap1-------|	

	CONFIGURACION
	# Bridge configuration
	brctl addbr br0
	ifconfig br0 192.168.1.20 netmask 255.255.255.0 
	ip link set dev br0 up 

	# TAP0 configuration
	mknod /dev/tap0 c 36 $[ 0 + 16 ]
	chmod 666 /dev/tap0
	ip tuntap add dev tap0 mode tap
	ip link set dev tap0 address 02:AA:BB:CC:DD:00
	ip link set dev tap0 up 
	brctl addif br0 tap0
	ifconfig tap0 192.168.1.200 netmask 255.255.255.0

	# TAP1 configuration
	mknod /dev/tap1 c 36 $[ 1 + 16 ]
	chmod 666 /dev/tap1
	ip tuntap add dev tap1 mode tap
	ip link set dev tap1 address 02:AA:BB:CC:DD:10
	ip link set dev tap1 up 
	brctl addif br0 tap1
	ifconfig tap1 192.168.1.201 netmask 255.255.255.0

	# Link ETH0 to BRIDGE
	brctl addif br0 eth0


PARA TESTEAR:
	SERVER
	./simpletun -i tap0 -s -p 4000 -a -d

		root@node0:~# ./simpletun -i tap0 -s -p 4000 -a 
		Reading data: Connection reset by peer
		TAP2NET 15925: Written 253 bytes to the network
		NET2TAP 16890: Read 253 bytes from the network
		NET2TAP 16890: Written 253 bytes to the tap interface
		TAP2NET 15926: Read 249 bytes from the tap interface
		TAP2NET 15926: Written 249 bytes to the network
		NET2TAP 16891: Read 86 bytes from the network
		NET2TAP 16891: Written 86 bytes to the tap interface
		TAP2NET 15927: Read 60 bytes from the tap interface
		TAP2NET 15927: Written 60 bytes to the network

	CLIENT
	 ./simpletun  -i tap1 -c 192.168.1.200 -p 4000 -a -d 

	 
	nc -l -p 4000 -s 192.168.1.200 << SERVER 
	nc -s 192.168.1.201 192.168.1.200 4000 << CLIENT

	
	ATENCION: No siempre funciona!! a veces queda tanto CLIENTE como SERVER a la espera
	sin intercambiar datos.
	
---------------------------------------------------------------------------------------------------------------
20170213:
			Se desarrollo la primera prueba de tapserver.c que envia un string 
			como payload[] = "esto es un string de pruebas";
			
			 tapserver.c:main:234:Starting ./tapserver
			 tapserver.c:tap_init:198:tap_lpid=3244
			 tapserver.c:low_level_probe:106:tap0
			 tapserver.c:low_level_probe:123:tap0 src_mac 02:AA:BB:CC:DD:00
			 tapserver.c:low_level_probe:132:tap1 dst_mac 02:AA:BB:CC:DD:10
			 tapserver.c:low_level_probe:139:tap0 MTU 1500
			 tapserver.c:main:255:ETH_HDR: dst=02:AA:BB:CC:DD:10 src=02:AA:BB:CC:DD:0 proto=0123
			
			
Para testear 
	tcpdump -i tap0 -N -n -e -vvv -XX > tcpdump.txt 
			
02:32:33.290230 02:aa:bb:cc:dd:00 > 02:aa:bb:cc:dd:10, ethertype Unknown (0x2301), length 42: 
	0x0000:  02aa bbcc dd10 02aa bbcc dd00 2301 6573  ............#.es
	0x0010:  746f 2065 7320 756e 2073 7472 696e 6720  to.es.un.string.
	0x0020:  6465 2070 7275 6562 6173                 de.pruebas			

---------------------------------------------------------------------------------------------------------------
20170213:
		Se volvio atras y se hizo rawtool.c
		Se pasa como parametro 
		usage: rawtool [-c|s] <rmt_node> <filename>
		rmt_node: nombre o IP del nodo destino		
			se resuelve via gethostbyname()
			se obtiene la IP del nodo remoto
			se obtiene con IOCTL la direccion MAC dada una IP.
	
SERVER EN NODE 0	
# rawtool -s  node1 eth.c
 rawtool.c:main:211:Starting ./rawtool
 rawtool.c:main:229:raw_mode=0
 rmt_name=node1 rmt_ip=192.168.1.101
 001: Mac Address of [192.168.1.3] on [eth0] is "50:e5:49:d9:8e:74"
 002: Mac Address of [192.168.1.101] on [eth0] is "00:0c:29:98:08:32"
 Destination IP:192.168.1.101 MAC:00:0C:29:98:08:32
 rawtool.c:main:282: src_mac 00:0C:29:FF:E3:2A
 rawtool.c:main:291: dst_mac 00:0C:29:98:08:32
 rawtool.c:main:333:ETH_HDR: dst=00:0C:29:98:08:32 src=00:0C:29:FF:E3:2A proto=FD
 rawtool.c:main:336:sframe_len=42 pay_len=28
 rawtool.c:main:343:sent_bytes=42
 
TCPDUMP en eth0 de NODO1
10:33:43.997217 00:0c:29:ff:e3:2a > 00:0c:29:98:08:32, ethertype Unknown (0xfd00), length 60: 
	0x0000:  6573 746f 2065 7320 756e 2073 7472 696e  esto.es.un.strin
	0x0010:  6720 6465 2070 7275 6562 6173 0000 0000  g.de.pruebas....
	0x0020:  0000 0000 0000 0000 0000 0000 0000       ..............
	
CLIENT EN NODE 1	
# rawtool -c  node0 xxxxxxx
 rawtool.c:main:211:Starting ./rawtool
 rawtool.c:main:229:raw_mode=1
rmt_name=node0 rmt_ip=192.168.1.100
001: Mac Address of [192.168.1.100] on [eth0] is "00:0c:29:ff:e3:2a"
002: Mac Address of [192.168.1.3] on [eth0] is "50:e5:49:d9:8e:74"
Destination IP:192.168.1.100 MAC:00:0C:29:FF:E3:2A
 rawtool.c:main:282: src_mac 00:0C:29:98:08:32
 rawtool.c:main:291: dst_mac 00:0C:29:FF:E3:2A
 rawtool.c:main:361:rcvd_bytes=60
 rawtool.c:main:365:ETH_HDR: dst=00:0C:29:98:08:32 src=00:0C:29:FF:E3:2A proto=FD
payload:esto es un string de pruebas
	
---------------------------------------------------------------------------------------------------------------
20170217/8:
			Comence con la creacion de m3ipc-httpd en el directorio lwip-tap
			Genere un m3ipc.h comun para todos los programas que usen lwip-tap
			ATENCION: 
				Tambien modifique lwip-tap.c para que contemple con la opcion "m" al  M3IPC_HTTPD
				modifique el Makefile.in (tambien el Makefile, pero seguramente esto se destruye al ejecutar ./configure)
				ejecute ./configure
				luego ejecuta make depend y make dep
				finalmente make all			
			OJO que las librerias no quedan MODIFICAR Makefile
				LIBS = /home/MoL_Module/mol-ipc/stub_syscall.o \
				 /home/MoL_Module/mol-ipc/lib/mollib/libmollib.so \
				 /home/MoL_Module/mol-ipc/lib/syslib/libsyslib.so 

 			Modificar en rdisk el archivo de configuracion, ponerle minor 0
			Incorpore al archivo de imagen 	 
				MoL_Module\mol-ipc\servers\diskImgs\floppy3RWX.img
				mamut.html
				mamut1.jpg
				mamut2.jpg
				
			El texto que debe presentar es:
					<h1>A very simple webpage. This is an "h1" level header.</h1>

					<h2>This is a level h2 header.</h2>

					<h6>This is a level h6 header.  Pretty small!</h6>

					<p>This is a standard paragraph.</p>
			
			PARA PROBAR	 
				lynx 172.16.0.2/mamut.html
			FUNCIONA OK!!
			
---------------------------------------------------------------------------------------------------------------
20170218/9:			
			Se  con la creacion de m3ipc-webfat en el directorio lwip-tap
			Este codigo contiene FatFs - Generic FAT File System Module
			
			Modificar en rdisk el archivo de configuracion, ponerle minor 0
			MoL_Module\mol-ipc\servers\diskImgs\floppy3FAT.img contiene estos 3 archivos (RECORDAR FAT 8.3)
				MAMUT.HTM
				MAMUT1.JPG
				MAMUT2.JPG
	
			lynx 172.16.0.2/mamut.htm  <<<<< ATENCION EL NOMBRE
			FUNCIONA OK!!!!
			
---------------------------------------------------------------------------------------------------------------
20170219:	Se comenzo con un unikernel que tiene a ETH como driver ETHERNET
			Para ello se clono el lwip-tap en el directorio lwip-mol
			Se reemplazo toda referencia a TAP por MOL.
			Compila bien.
			Ahora hay que modificar......
			
			OBJETIVO:  
				webserver: 	en principio usar httpserver (pagina fija estatica) 
							luego usar el de httpd-webfat, renombrarlo como httpd-fatimg							
				disco:		crear un falso driver que lo que hace es leer del sistema de archivos de LINUX		
							un archivo de imagen (FAT) via open, read, write, close.
				ethernet:	ETH				
			

---------------------------------------------------------------------------------------------------------------
20170227:	Se continua con lwip-mol
			No estaba habilitado el debug de LWIP y MOLDBG. Ahora loguea perfectamente.

	ETH recibe un frame, lo extraño es que src=52:44:3E:F7:13:4 que es la MAC de TAP0 y el destino es medio raro.
 eth.c:low_level_input:660:if_name=tap0 flags=0x     210
 eth.c:low_level_input:664:if_name=tap0 Frame received: length=70
 eth.c:low_level_input:673:ETH_HDR: dst=33:33:0:0:0:2 src=52:44:3E:F7:13:4 proto=DD86
 eth.c:low_level_input:676:IP_HDR: vers=60 tos=0 len=0 id=1000 flag=FF3A ttl=FE proto=80 chk=0 src=0 dst=FF3E4450
	
	Luego, ETH hace un notify a si mismo para que sea tomado por el MAIN THREAD
 eth.c:M3IPC_hdw_notify:206:MTX_LOCK m3ipc_mutex 
 eth.c:M3IPC_hdw_notify:207:vmid=0 endpoint=6
 eth.c:M3IPC_hdw_notify:209:MTX_UNLOCK m3ipc_mutex

	Luego le envía un mensaje reply = 2325 = 0x915 = DL_TASK_REPLY
 eth.c:reply:1088:source=14 type=2325 m3i1=0 m3i2=18 m3p1=(nil) m3ca1=[]

	Aqui queda colgado
VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2720    0    0   80  27342  27342  27342  27342 systask        
 0   -2     -2   2719    0    8   20  31438  27342  27342  27342 systask        
 0    0      0   2722    0    8   A0  31438  27342  27342  27342 pm             
 0    2      2   2725    0    0   20  27342  27342  27342  27342 rs             
 0    6      6   2757    0    4   20  27342     18  27342  27342 eth  <<<<< colgado en SEND            
 0   18     18   2765    0    C   20      6      6  27342  27342 lwip-mol  <<<<< colgado en SENDREC     
	
			
	Se modifico ETH para utiliza mnx_wakeup pero da error 
	ERROR: eth.c:tapif_input:737: rcode=-320: 
	#define EMOLPROCRUN 	(_SIGN 320)  /* The copy source/destination process is running */
	

	Debe recibir con mnx_receive() peticiones desde la capa de RED 
	Debe recibir alguna notificacion de que llego un frame desde el thread receiver.
	Debe responder a la capa de red con mnx_send()
	
	MODIFICAR ETH
		main thread(){
			MTX_LOCK(main_mutex)
			COND_WAIT(main_barrier,main_mutex)
			card_nr = (rw_wakeup >> 1);
			card_op	=  (rw_wakeup & ETH_CARD_WRITE);
			TASKDEBUG("woken up rw_wakeup=%X  by card_nr=%d card_op=%d\n"
					, rw_wakeup, card_nr, card_op);
			if( rw_wakeup == NO_RW_WAKEUP) {
				MTX_UNLOCK(main_mutex);
				TASKDEBUG("rw_wakeup == NO_RW_WAKEUP\n");
				continue;
			}
			ec_ptr = &ec_table[card_nr];
			TASKDEBUG(EC_FORMAT, EC_FIELDS(ec_ptr));
			rw_wakeup = NO_RW_WAKEUP;	
			if(card_op == ETH_CARD_READ){
				TASKDEBUG("Reply to upper layer about a frame received on card=%d\n", card_nr);
				ec_ptr->flags =  ECF_PACK_RECV;
			}else{
				TASKDEBUG("Reply to upper layer about a frame sent on card=%d\n", card_nr);
				ec_ptr->flags =  ECF_PACK_SEND;
			}
			reply(ec_ptr, OK, TRUE);
		}
		
					
---------------------------------------------------------------------------------------------------------------
20170228:		Modifique ETH 

 eth.c:main:1355:waiting on barrier
 eth.c:main:1356:COND_WAIT main_barrier main_mutex
 eth.c:eth_thread:1047:MTX_UNLOCK main_mutex 
 eth.c:eth_thread:1033:rcode=0
 eth.c:eth_thread:1041:source=18 type=2053 m2i1=0 m2i2=18 m2i3=1514 m2l1=-2127955398 m2l2=914 m2p1=0x90f0000
 eth.c:eth_thread:1042:MTX_LOCK main_mutex 
 eth.c:eth_thread:1045:COND_SIGNAL main_barrier
 eth.c:eth_thread:1046:COND_WAIT inet2eth_barrier main_mutex
 eth.c:main:1365:rqst_source=2
 eth.c:main:1394:source=18 type=2053 m2i1=0 m2i2=18 m2i3=1514 m2l1=-2127955398 m2l2=914 m2p1=0x90f0000
 eth.c:do_vread:1169:port=0 count=1514 client=18 vectored=0 
 eth.c:ec_recv:375:port=0 flags=210
 eth.c:ec_recv:388:i=0 rx_slot_nr=0 length=0
 eth.c:ec_recv:388:i=1 rx_slot_nr=1 length=0
 eth.c:ec_recv:388:i=2 rx_slot_nr=2 length=0
 eth.c:ec_recv:388:i=3 rx_slot_nr=3 length=0
 eth.c:ec_recv:388:i=4 rx_slot_nr=4 length=0
 eth.c:ec_recv:388:i=5 rx_slot_nr=5 length=0
 eth.c:ec_recv:388:i=6 rx_slot_nr=6 length=0
 eth.c:ec_recv:388:i=7 rx_slot_nr=7 length=0
 eth.c:ec_recv:388:i=8 rx_slot_nr=8 length=0
 eth.c:ec_recv:388:i=9 rx_slot_nr=9 length=0
 eth.c:ec_recv:388:i=10 rx_slot_nr=10 length=0
 eth.c:ec_recv:388:i=11 rx_slot_nr=11 length=0
 eth.c:ec_recv:388:i=12 rx_slot_nr=12 length=0
 eth.c:ec_recv:388:i=13 rx_slot_nr=13 length=0
 eth.c:ec_recv:388:i=14 rx_slot_nr=14 length=0
 eth.c:ec_recv:388:i=15 rx_slot_nr=15 length=0
 eth.c:reply:1119:if_name=tap0 err=0 may_block=0 flags=0x     210
 eth.c:reply:1135:clockTicks =100
 eth.c:tapif_input:733:port=0
 eth.c:tapif_input:741:MTX_LOCK main_mutex
 
 ETH queda bloqueado en reply()
	if ((now = mol_time(&now)) < 0) {
  		ERROR_EXIT((int)now);
	}

VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2690    0    0   80  27342  27342  27342  27342 systask        
 0   -2     -2   2689    0    8   20  31438  27342  27342  27342 systask        
 0    0      0   2725    0    8   A0  31438  27342  27342  27342 pm             
 0    2      2   2728    0    0   20  27342  27342  27342  27342 rs             
 0    6      6   2718    0    8  220      0  27342  27342  27342 eth      <<<<<< BLOQUEADO con main_mutex tomado
 0   18     18   2738    0    8   20      6  27342  27342  27342 lwip-mol      

 PM recibe la peticion mol_time
	 time.c:do_time:28:uptime/clockTicks=193
	 pm.c:main:70:call_nr=13 result=0
	 pm.c:setreply:312:proc_nr=6 result=0
	 pm.c:main:80:Send out all pending reply messages
 pero a diferencia de otra respuesta aparece claramente 
	 pm.c:main:80:Send out all pending reply messages
	 pm.c:main:90:Replying to 18 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
	  

---------------------------------------------------------------------------------------------------------------	
20170302:
		Se modifico el mm_rproxy.c tenia un error el notify_rmt2lcl MUY GROSERO.
		1- no comparaba el tipo de proxy de USUARIO o de KERNEL - solo kernel
		2- hacia cualquier cosa con respecto a la accion a tomar si el receptor estaba esperando o  no

 
 ERROR EN PROXY TCP CON THREADS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1

Hay un error en el proxy TCP implementado con threads, todavia no se que es.
Cambien sus scripts de pruebas 

donde decia
  read  -p "TCP THREAD PROXY Enter para continuar... "
reemplazar por
  read  -p "TCP PROXY Enter para continuar... "

donde dice 
./tcp_th_proxy node$rmt $rmt >node$rmt.txt 2>error$rmt.txt &
debe decir
./tcp_proxy node$rmt $rmt >node$rmt.txt 2>error$rmt.txt &

---------------------------------------------------------------------------------------------------------------	
20170304:			Modifique demonize porque cuando hacia getline() agrega el LF al final	
					fue reemplazado el LF por 0
					
						

VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2700    0    0   80  27342  27342  27342  27342 systask        
 0   -2     -2   2699    0    8   20  31438  27342  27342  27342 systask        
 0    0      0   2735    0    8   A0  31438  27342  27342  27342 pm             
 0    2      2   2738    0    0   20  27342  27342  27342  27342 rs             
 0    6      6   2728    0    4   20  27342     18  27342  27342 eth       <<< NO ES DEADLOCK        
 0   18     18   2745    0    C   20      6      6  27342  27342 lwip-mol  <<< NO ES DEADLOCK

 Parece ser deadlock pero en realidad lwip-mol hace 
 	rcode = mnx_sendrec(m3eth_ptr->p_endpoint, m_ptr);
		molif.c:low_level_input:256: minor=(null) name=mol0 
		molif.c:low_level_input:268: source=6 type=2325 m3i1=0 m3i2=18 m3p1=(nil) m3ca1=[]
 y 	ETH responde 
 	  eth.c:main:1357:waiting on barrier
	 eth.c:main:1358:COND_WAIT main_barrier main_mutex
	 eth.c:eth_thread:1047:MTX_UNLOCK main_mutex 
	 eth.c:eth_thread:1033:rcode=0
	 eth.c:eth_thread:1041:source=18 type=2053 m2i1=0 m2i2=18 m2i3=1514 m2l1=0 m2l2=68 m2p1=0x9468000
	 eth.c:eth_thread:1042:MTX_LOCK main_mutex 
	 eth.c:eth_thread:1045:COND_SIGNAL main_barrier
	 eth.c:eth_thread:1046:COND_WAIT inet2eth_barrier main_mutex
	 eth.c:main:1367:rqst_source=2
	 eth.c:main:1396:source=18 type=2053 m2i1=0 m2i2=18 m2i3=1514 m2l1=0 m2l2=68 m2p1=0x9468000
	......
	  eth.c:reply:1125:if_name=tap0 err=0 may_block=0 flags=0x     210
	 eth.c:reply:1137:now =68
	 eth.c:reply:1146:source=-1219301343 type=2325 m3i1=0 m3i2=18 m3p1=(nil) m3ca1=[]
	 eth.c:M3IPC_send:233:MTX_LOCK m3ipc_mutex 
	 eth.c:M3IPC_send:234:dst_ep=18
	 eth.c:M3IPC_send:236:MTX_UNLOCK m3ipc_mutex 
 
 ATENCION: Algo esta funcionando 
	tap0     Link encap:Ethernet  HWaddr 66:17:7d:0a:4f:31   <<< MAC ADDRESS DE TAP0 
	
			eth.c:low_level_probe:618:tap0
			eth.c:low_level_probe:640:tap0 MACADDR 66:17:7D:0A:4F:31: <<< MAC ADDRESS DE TAP0 
			eth.c:low_level_probe:647:eth_card0 MTU 1500
		
	Salida de tcpdump indica ARP lanzado por TAP0!!!
	09:03:38.899327 66:17:7d:0a:4f:31 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Request who-has 172.16.0.2 tell 172.16.0.3, length 28
	0x0000:  ffff ffff ffff 6617 7d0a 4f31 0806 0001  ......f.}.O1....
	0x0010:  0800 0604 0001 6617 7d0a 4f31 ac10 0003  ......f.}.O1....
	0x0020:  0000 0000 0000 ac10 0002                 ..........
	09:03:39.899579 66:17:7d:0a:4f:31 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Request who-has 172.16.0.2 tell 172.16.0.3, length 28
	0x0000:  ffff ffff ffff 6617 7d0a 4f31 0806 0001  ......f.}.O1....
	0x0010:  0800 0604 0001 6617 7d0a 4f31 ac10 0003  ......f.}.O1....
	0x0020:  0000 0000 0000 ac10 0002                 ..........
 
			ADERMAS el propio ETH lo detecta a su propio ARP
			
			 eth.c:low_level_input:670:if_name=tap0 flags=0x     210
			 eth.c:low_level_input:674:if_name=tap0 Frame received: length=42
			 eth.c:low_level_input:683:ETH_HDR: dst=FF:FF:FF:FF:FF:FF src=66:17:7D:A:4F:31 proto=608
			 eth.c:low_level_input:686:IP_HDR: vers=0 tos=1 len=8 id=406 flag=100 ttl=66 proto=17 chk=A7D src=10AC314F dst=300
			 eth.c:low_level_input:693:ARP_HDR: htype=100 ptype=8 hlen=6 plen=4 opcode=256
 
 
	QUEDAN BLOQUEADOS 
	ETH 
		 eth.c:main:1373:waiting on barrier
		 eth.c:main:1374:COND_WAIT main_barrier main_mutex
		 eth.c:low_level_input:684:if_name=tap0 flags=0x     210
		 eth.c:low_level_input:688:if_name=tap0 Frame received: length=42
		 eth.c:low_level_input:697:ETH_HDR: dst=FF:FF:FF:FF:FF:FF src=62:8A:8E:68:FA:D0 proto=608
		 eth.c:low_level_input:700:IP_HDR: vers=0 tos=1 len=8 id=406 flag=100 ttl=62 proto=8A chk=688E src=10AC D0FA dst=300
		 eth.c:low_level_input:707:ARP_HDR: htype=100 ptype=8 hlen=6 plen=4 opcode=256
		 eth.c:tapif_input:761:COND_SIGNAL main_barrier
		 eth.c:tapif_input:762:COND_WAIT receive_barrier[ec_ptr->ec_port] main_mutex
		 eth.c:eth_thread:1063:MTX_UNLOCK main_mutex 
		 eth.c:eth_thread:1049:rcode=-348
		 eth.c:eth_thread:1051:MTX_LOCK main_mutex 
		 eth.c:eth_thread:1061:COND_SIGNAL main_barrier
		 eth.c:eth_thread:1062:COND_WAIT inet2eth_barrier main_mutex
		 eth.c:main:1383:rqst_source=1
		 eth.c:main:1391:iface_rqst=1 by iface_nr=0
		 eth.c:main:1393:port_name=eth_card#0 if_name=tap0 fd=3 ifr_ifindex=0 flags=202 mode=2 ec_port=0 ec_owner=18 
		 eth.c:main:1395:Reply to upper layer about a frame received on card=0
		 eth.c:reply:1141:if_name=tap0 err=0 may_block=1 flags=0x       2
		 eth.c:reply:1153:client=18 now =85
		 eth.c:reply:1162:source=-1080161400 type=2325 m3i1=0 m3i2=18 m3p1=0x2a m3ca1=[]
		 eth.c:M3IPC_send:245:dst_ep=18
 

	LWIP 
		Deberia hacer el sendrec como aca
		molif.c:low_level_input:256: minor=(null) name=mol0 
		molif.c:low_level_input:268: source=6 type=2325 m3i1=0 m3i2=18 m3p1=(nil) m3ca1=[] <<< ESTO ES POSTERIOR A SENDREC
		Pero llega hasta el sendrec
		molif.c:molif_input:332: name=mol num=108
		molif.c:molif_input:334: minor=(null) name=mol0 
		molif.c:low_level_input:256: minor=(null) name=mol0 

	ESTO DEBE SUCEDER PORQUE AMBOS PROCESOS NO ESTAN SINCRONIZADOS CORRECTAMENTE
		Aparentemente ETH recibe un paquete ARP pero no hay un sendrec desde LWIP para hacer algo.
		De todos modos ETH hace un reply/send, justo en el momento que LWIP esta haciendo un pedido por lo que quedan colgados.
			
---------------------------------------------------------------------------------------------------------------	
20170310:	Modifique demonize y rs. 
			demonize envia la linea de argumentos como un string completo
			luego rs lo separa en tokens utilizando strtok()
			
---------------------------------------------------------------------------------------------------------------	
20170311:	El problema de molif.c era que no estaban seteadas molif y netif 
			por esa razon me daba un segfault.
			
VM p_nr -endp- -lpid- node flag misc -getf- -sndt- -wmig- -prxy- name
 0  -34    -34   2696    0    0   80  27342  27342  27342  27342 systask        
 0   -2     -2   2695    0    8   20  31438  27342  27342  27342 systask        
 0    6      6   2724    0    8   20  31438  27342  27342  27342 eth            
 0    9      9   2732    0    8   20      6  27342  27342  27342 lwip-mol    <<< LWIP ESPERANDO DE ETH
 
ETH  no hace nada al recibir un frame porque la interface no esta en READING 
 eth.c:low_level_input:697:if_name=tap0 flags=0x       0
 eth.c:low_level_input:701:if_name=tap0 Frame received: length=70
 eth.c:low_level_input:710:ETH_HDR: dst=33:33:0:0:0:2 src=E2:C8:D0:1F:CA:F5 proto=DD86
 eth.c:low_level_input:713:IP_HDR: vers=60 tos=0 len=0 id=1000 flag=FF3A ttl=FE proto=80 chk=0 src=0 dst=FFD0C8E0
 eth.c:receive_thread:897:port=0 not reading
 
LWIP no invoco a READ de ETH 
	Esto se hace con low_level_input
	
	
Evidentemente puede pasar que;
		1- LWIP no envia un READ a ETH y por lo tanto la interface no queda en READ
		2- ETH deberia notificar a LWIP de tal modo que luego invoque al READ.
		
---------------------------------------------------------------------------------------------------------------	
20170312:	Ahora tanto lwip-mol como m3eth estan funcionando.
			pero NO ANDA!!
			El HTTP server se ve que arranca y funciona pero no llegan los paquetes de ping ni nada 
			hacia la interface tap0
			
---------------------------------------------------------------------------------------------------------------
20170323/24:	Se modifico DEMONIZE y RS 

	Usage: %s -<l|r|b> <node> <vmid> <endpoint> <mpid|nodeid> \"<command> <args...>\"
	<node> means in which node the command will be executed (referred to PM) 
	\t\t l: local bind 
	\t\t r: replica bind 
	\t\t b: backup bind
	<vmid>: VM ID for the process
	<endpoint>: Endpoint to allocate for the process
	for LOCAL operation: <mpid> MINIX PID desired to allocate. 
	\t (0) means that PM allocates any one
	for REMOTE operation: <nodeid> the node where to execute
	If a <commmand> is not supplied, STDIN is read to get it
	
	Ahora se puede arrancar un proceso LOCAL (referente a PM)
		- con mnx_lclbind
		- como replica (es decir que puede haber otros procesos utilizando el mismo endpoint activos en otros nodos )
						( u otros procesos como backup)
		- como backup: es decir a la vista del nodo local el proceso debe quedar detenido y su endpoint como REMOTE
		
	Solo se puede arrancar un proceso REMOTO (referente a PM)
		- con mnx_lclbind 

	Se modificaron	com.h, 	demonize.c, mm_hyper.c , rdisk.c , rdisk.h, rs.c, rdisk.sh
	
	Ahora trabaja asi. (rdisk.c - funcion rd_init)
	fijate en el rdisk.sh que arranca en forma autonoma (sin demonize) despues de SYSTASK
	Si soy el primer RDISK que arranca, el slot RDISK_PROC_NR esta vacio, por lo tanto se asume primario. y entonces hace un mnx_replbind() y se lo informa a la SYSTASK con sys_bindproc()

	Si no soy el primero entonces el RDISK_PROC_NR no esta vacio, aunque sea de otro nodo , porque entre las SYSTASK intercambiaron info. El NODO1 lo va a ver como si fuese REMOTE. 
	Ahora el RDISK de NODE1 hace un mnx_bkupbind() por el cual convierte al slot que antes
	era REMOTE en BACKUP y queda todo OK.
	Fijate las dos impresiones sucesivas en pantalla de cat /prod/drvs/VM0/procs y veras
	que antes de ejecutar RDISK en NODO1 el lpid del slot 3 era -1 y ahora es el PID del rdisk local
	luego haces un ps-ef y lo ves arrancado
	y si miras en los logs queda
	rdisk.c:rd_init:761:primary_mbr=0 - local_nodeid=1
	rdisk.c:rd_init:763:wait until  the process will be the PRIMARY
 			
 ---------------------------------------------------------------------------------------------------------------
20170323:	se hizo m3ftpd y m3ftp y lclftp.sh
			Funciona bien FTP_GET LOCAL
			Se cambia el current directory al mismo directorio del ejecutable 
			Se incluyeron estadisticas bytes/seg

 ---------------------------------------------------------------------------------------------------------------
20170324:  m3ftpd y m3ftp se hizo FTP_PUT y FTP_GET LOCAL Y REMOTO y funciona OK
			Se incluye lectura de  -p y -g 
			reemplace sys_vcopy x mnx_vcopy 
		
	Se hizo lz4tcp_th_proxy TIPC con compresion 
		Througput     TCP:  Throuhput =  5095904.234128 [bytes/s]	Archivo TAR.GZ
		Througput LZ4 TIPC: Throuhput =  5906003.853985 [bytes/s]	Archivo TAR.GZ
		Througput LZ4 TIPC:	Throuhput =  6839477.688088 [bytes/s]	Archivo TAR
		Througput 	  TIPC: Throuhput =  6948807.806832 [bytes/s]	Archivo TAR.GZ
		Througput LZ4 TIPC: Throuhput = 11089669.572956 [bytes/s]	Archivo de TEXTO CON PATRONES 
						
---------------------------------------------------------------------------------------------------------------
20170328:
	Se inicio con  rawftpd y rawftp. 
	lo primero que hace el CLIENTE es enviar el path del archivo a recibir desde el SERVER  
	
12:37:21.580890 00:00:00:00:00:00 > 00:00:00:00:00:00, ethertype Unknown (0xfd00), length 30: 
	0x0000:  0000 0000 0000 0000 0000 0000 fd00 0c00  ................
	0x0010:  0000 6574 685f 6f75 7430 2e74 7874       ..eth_out0.txt
	
	Como se puede ver, las MAC ADDRESS se setean.
	
---------------------------------------------------------------------------------------------------------------
20170401:
		Se termino una version 2 (ayer la 1) de rawftp
		En esta version se lee un bloque completo del archivo MAXCOPYBUF y se va enviando en forma parcial
		al cliente. El cliente hace el ACK frame a frame.
		
		SERVER					CLIENTE
		      <---- GETFILE------
			  ----> GETFILE+ACK-->
			  
		Lee bloque
			  ----- DATA ------->
			  <---- DATA+ACK ---- 
			  
		Cuando llega a FIN de bloque envia un flag EOB
		Cuando llega a FIN de archivo envia un flag EOF
		
		Si hay un error en el server envia un comando RAW_EXIT en lugar de DATA.
		
		Hace retransmision en caso de error en el DATA+ACK

---------------------------------------------------------------------------------------------------------------
20170401:
			Se clarifico el codigo de rawftp y rawftpd derivando a funciones en lugar de monolitico.
			Si al receptor le llega un FRAME que NO TIENE prendido ACK y tiene ERROR 
			entonces el receptor NO ENVIA ACK y espera hasta que llegue otro FRAME con ACK para retornar error .
			Se implemento el envio RAW_NEEDACK que el emisor le pide al receptor.
			Tambien en el EOB y EOF enviar RAW_NEEDACK
			
			Se reesolvio  correctamente salidas por ERROR y retransmisiones
			Se podria hacer inyectando un falso error en rawftp
				- cuando el server pide ACK
				- cuando el server NO pide ACK
			Controlar en el receptor que send_seq = ack_seq 
			Como hacer para temporizar los RECEIVE 
				tv.tv_sec = RAW_RCV_TIMEOUT;
				if(setsockopt(raw_fd, SOL_SOCKET, SO_RCVTIMEO, &tv, sizeof(tv)) < 0){
					TASKDEBUG("%s\n",strerror(errno));
					ERROR_EXIT(errno);
				}
	
PROBLEMA A RESOLVER:
		1- Como hacer para que ESCUCHEN y ESCRIBAN al mismo tiempo (threads, select)
		
---------------------------------------------------------------------------------------------------------------
20170408:
				se hizo rawmsgq que permite
					SENDER_MSGQ---->RAW_ETHERNET
					RECEIVER_MSGQ<--RAW ETHERNET
				
			Para probar, se inserta un mensaje HELLO en la cola del emisor
			cuando se vence el TIMEOUT del RECEPTOR ETHERNET

			FUNCIONA PERFECTAMENTE - Esto es lo que recibe el node1 al vender el timeout en el receptor ethernet de node0
			12:25:06.747214 00:0c:29:ff:e3:2a > 00:0c:29:98:08:32, ethertype Unknown (0xfd00), length 104: 
	0x0000:  000c 2998 0832 000c 29ff e32a fd00 0000  ..)..2..)..*....
	0x0010:  0a00 0000 0000 0000 0000 0000 0000 0000  ................  <<<< "0x0a" mensaje tipo CMD_HELLO
	0x0020:  0000 0000 0000 0000 0000 0000 0000 0000  ................
	0x0030:  0000 0000 0000 0000 0000 0000 0000 0000  ................
	0x0040:  0000 0000 0000 0000 0000 0000 0000 0000  ................
	0x0050:  0000 0000 0000 0000 4845 4c4c 4f00 0000  ........HELLO...
	0x0060:  0000 0000 0000 0000                      ........
	
---------------------------------------------------------------------------------------------------------------
20170409:		Se hizo msgq_proxy es el proxy que utiliza las MSGQ para enviar y recibir mensajes
				Es de utilización GENERICA.
				Pruebas: transferencias de MENSAJES OK!!!
				
				Transferencias de bloques de 512 y 1024 OK
				con ERROR la de 2048 => NO ESTA ANDANDO BIEN LA FRAGMENTACION
				
El problema era el limite del kernel MAXMSG
				https://www.ibm.com/support/knowledgecenter/en/SSEPGG_10.1.0/com.ibm.db2.luw.qb.server.doc/doc/t0008238.html
				Se puede hacer #ipcs -l para ver los limites
				y modificar el archivo	/etc/sysctl.conf

						
				
TODO:			
				MODIFICAR EL ALGORITMO DE TRANSFERENCIA DE BLOQUES
					CMD_COPYIN_DATA Y CMD_COPYOUT_DATA
					Llevan el mismo header (cmd_t) excepto:
						c_len : se setea con el payload del frame, no de la transferencia total
						c_flags: se setea con flag RAW_DATA
					
					Para controlar que hayan llegado todos los frames de una transferencia se hace:
						frame de header = sizeof(cmd_t)
						N frames de datos = cmd.c_u.cu_vcopy.v_bytes
					Si llega algun frame que está fuera que rompe la cadena de datos, y no se cumple la longitud total
					entonces se decarta la transferencia completa y se envia CANCEL al remoto
				

---------------------------------------------------------------------------------------------------------------
20170417:		rawmsgq: Se agregaron ACKNOWLEDGE A NIVEL ETHERNET- No funciona bien.
				Para remover los ACK anular la constante y recompilar
				#define WITH_ACKS 1    // set this constant to enable ACKNOWLEDGES 

				Sin ACK funciona OK.
				Con ACK es extremadamente lento porque hay tiempos de espera.
				
				Cuando el SENDER envía un frame y espera por ACK haciendo COND_WAIT_TO, 
				el RECEIVER hace un COND_SIGNAL cuando llega un frame.
				Aparentemente el malfuncionamiento aparentemente se debe a que cuando hace la espera, el frame ya llego
				y el COND_SIGNAL y por lo tanto se pierde.
				
				 rawmsgq.c:SENDER_th:574:COND_WAIT_TO ret sdesc.td_cond raw_mtx
				 rawmsgq.c:SENDER_th:574:COND_WAIT_TO before tv_sec=1492442282
				 rawmsgq.c:SENDER_th:574:COND_WAIT_TO after tv_sec=1492442292 << 10 seg
				 rawmsgq.c:RECEIVER_th:766:Waiting on Ethernet sock_fd=4
				 rawmsgq.c:SENDER_th:577:wait_again=10
				 rawmsgq.c:SENDER_th:574:COND_WAIT_TO ret sdesc.td_cond raw_mtx
				 rawmsgq.c:SENDER_th:574:COND_WAIT_TO before tv_sec=1492442292
				 rawmsgq.c:SENDER_th:574:COND_WAIT_TO after tv_sec=1492442302  << 10 seg
				
				EL PROBLEMA PUEDE ESTAR EN LOS NUMEROS DE SECUENCIA 
				

---------------------------------------------------------------------------------------------------------------
20170418:		Ahora rawmsgq funciona con ACK pero solo para IPC 
				NO FUNCIONA PARA bloques de datos.
				
			
 rawmsgq.c:set_frame_hdr:453:
 rawmsgq.c:send_frame:325:lcl_snd_seq=584 lcl_ack_seq=864 sframe_len=104
 rawmsgq.c:DEBUG_hdrcmd:281:send_frame ETH_HDR: dst=00:0C:29:98:08:32 src=00:0C:29:FF:E3:2A proto=FD
 rawmsgq.c:DEBUG_hdrcmd:283:send_frame cmd=5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=512
 rawmsgq.c:DEBUG_hdrcmd:284:send_frame c_flags=10 c_snd_seq=584 c_ack_seq=864
 rawmsgq.c:SENDER_th:548:sframe_len=104 sent_bytes=104
 rawmsgq.c:SENDER_th:569:rmt_ack_seq=583 lcl_snd_seq=584
 rawmsgq.c:SENDER_th:576:COND_WAIT_TO ret sdesc.td_cond raw_mtx
 rawmsgq.c:SENDER_th:576:COND_WAIT_TO before tv_sec=1492527416
 rawmsgq.c:RECEIVER_th:768:Waiting on Ethernet sock_fd=4
 rawmsgq.c:SENDER_th:576:COND_WAIT_TO after tv_sec=1492527426 <<<<< 1RA ESPERA 10 SEGUNDOS 
 rawmsgq.c:SENDER_th:579:wait_again=10
 rawmsgq.c:SENDER_th:569:rmt_ack_seq=583 lcl_snd_seq=584
 rawmsgq.c:SENDER_th:576:COND_WAIT_TO ret sdesc.td_cond raw_mtx
 rawmsgq.c:SENDER_th:576:COND_WAIT_TO before tv_sec=1492527426
 rawmsgq.c:RECEIVER_th:768:Waiting on Ethernet sock_fd=4
 rawmsgq.c:SENDER_th:576:COND_WAIT_TO after tv_sec=1492527436 <<<<< 2DA ESPERA 10 SEGUNDOS 
 rawmsgq.c:SENDER_th:579:wait_again=5
 rawmsgq.c:SENDER_th:589:raw_mtx LOCKED  HERE
 rawmsgq.c:SENDER_th:591:rmt_ack_seq=583 lcl_snd_seq=584
 rawmsgq.c:SENDER_th:592:wait_again=0							<<<< NO ESPERA MAS, ES HORA DE REINTENTAR 
 rawmsgq.c:SENDER_th:596:send_retries=3
 rawmsgq.c:SENDER_th:615:MTX_UNLOCK raw_mtx 
 rawmsgq.c:SENDER_th:625:rlen=512
 rawmsgq.c:SENDER_th:633:MTX_LOCK raw_mtx 
 rawmsgq.c:SENDER_th:638:remain=512 lcl_snd_seq=585 lcl_ack_seq=864 send_off=d%
 rawmsgq.c:set_frame_hdr:453:
 rawmsgq.c:send_frame:325:lcl_snd_seq=585 lcl_ack_seq=864 sframe_len=616 <<<<<<<<<<<<<<<<<<<<< OJO, CAMBIO EL NRO DE SECUENCIA!! 
 rawmsgq.c:DEBUG_hdrcmd:281:send_frame ETH_HDR: dst=00:0C:29:98:08:32 src=00:0C:29:FF:E3:2A proto=FD
 rawmsgq.c:DEBUG_hdrcmd:283:send_frame cmd=5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=512
 rawmsgq.c:DEBUG_hdrcmd:284:send_frame c_flags=1D c_snd_seq=585 c_ack_seq=864  <<< 1D => RAW_RESEND RAW_EOB RAW_DATA !!!! 
 rawmsgq.c:SENDER_th:658:sframe_len=616 sent_bytes=616

			
				
				CONTROLAR EL ENVIO DE HELLO SOLO SE HAGA SI ES QUE NO HAY ACTIVIDAD ENTRANTE NI SALIENTE CON EL NODO REMOTO		
				INCLUIR EN EL PRIMER FRAME DE CMD_COPYIN_DATA Y CMD_COPYOUT_DATA TAMBIEN DATOS 
				RESOLVER EL "RAW_CANCEL" CUANDO SE CANCELA UN VCOPY 
				PROBAR CON MEMORIA COMPARTIDA
				CONTAR CUANTOS HELLO NO SE RECIBIERON EN UN TIEMPO DADO  
				VER COMO MANTENER LA SECUENCIALIDAD DE LOS ENVIOS POR ETHERNET
				EVITAR ENVIAR ACK SI ES QUE HAY UN MENSAJE EN COLA DE SENDER O EL SENDER PROXY ESTA DANDO TRATAMIENTO A UNO
			
---------------------------------------------------------------------------------------------------------------
20170422:
				visto y considerando la necesidad de realizar pruebas  de rendimiento 
				Para hacer pruebas baje en windows wget-1.11.4-1-src-setup
				
	------------------ TEST de web server en LWIP: UNIKERNEL -------------------------------
	Se pueden  realizar 6 tests
				test #1 Webserver on NODE0, MOL-FS on NODE0 and its image file
				test #2 Webserver on NODE0, MOL-FS on NODE0 and RDISK on NODE0
				test #3 Webserver on NODE0, MOL-FS on NODE0 and RDISK on NODE1
				test #4 Webserver on NODE0, MOL-FS on NODE1 and its image file
				test #5 Webserver on NODE0, MOL-FS on NODE1 and RDISK on NODE1
				test #6 Webserver on NODE0, MOL-FS on NODE1 and RDISK on NODE2
	
				
				Se creo en minix un filesystem con 4 archivos
					- index.htm
					- file10M.txt
					- file50M.txt
					- file100M.txt
					
				Se copio como archivo de imagen en /home/minix180M.img
				Para setear todo el entorno ejecutar 
					unikernel0.sh (NODE0) 
					unikernel1.sh (NODE1)
				En estos script se copia el archivo de imagen desde /home a /lib/init/rw
				para que no influyan los tiempos de acceso a disco.
								
				El MOLFS se configura para que use esa imagen
					device MY_FILE_IMG {		
						major			1;
						minor			0;
						type			FILE_IMG;
						filename 		"/lib/init/rw/minix180M.img";
						volatile		NO;	
						root_dev		YES;
						buffer_size		65536;
					#	compression 	NO;
					};
						
				para probar desde NODE0 y NODE1 
				# lynx  172.16.0.2/index.htm
				# wget  172.16.0.2/file10M.txt  <<< este da calculo de rendimiento !!
				# wget  172.16.0.2/file50M.txt  <<< este da calculo de rendimiento !!
				# wget  172.16.0.2/file100M.txt  <<< este da calculo de rendimiento !!
											
 		
		FUNCIONAN LOS TEST DE 1 A 5
		
				Se creo un nuevo server web en lwip basado en nweb-httpd que no usa M3IPC sino 
				que acceda directamente a los archivos que le ofrece LINUX (open,read,close,fstat)
				Es para usar como PATRON
				Para eso se desarrollo el script nweb-lwip.sh para ejecutar en NODE0
				the test url is http://172.16.0.2/index.htm
				
				Se probo con wget en windows y deja el resultado en 
				C:\Users\Usuario\AppData\Local\VirtualStore\Program Files (x86)\GnuWin32\bin
							
		Se hizo M3NWEB  \mol-ipc\servers\m3nweb que usa
					http://www.ibm.com/developerworks/systems/library/es-nweb/ 
		Se ejecuta con m3nweb.sh 
					M3NEWB-->MOLFS-->RDISK 							
				
		Se cambiaron TAMAÑO DE BUFFER a 65536 = MAXCOPYBUF tanto en archivos fuentes
			como en archivos de configuracion.

		
		Se hicieron prueba de contraste entre los servidores web 
					NWEB+NFS_CLIENT---->NFS_SERVER+DIR
					NWEB+NFS_CLIENT---->NFS_SERVER+NBD_CLIENT----->NBD_SERVER 
							
		
20170505: 	websrvX.sh 
	
		ERROR:	las pruebas websrvx.sh no dieron satisfatorias Solo se ejecutaba una transferencia 
				porque las siguientes transferencias del script m3nwebX.sh quedan colgadas
				Este es un web server basado en M3-IPC entre cliente y servidor (NO ES HTTP/TCP/IP)
				La systask0 queda loopeando o algo por el estilo usando 95% de CPU aparentemente es slots.c
		COMO CONSECUENCIA  En en NODE1 en tcp_proxy tira un error UNABLE to handle NULL POINTER dereference 
			mutex_lock_slowpath
			
			Si ejecuto el script websrv0.sh modificando la localizacion del CLIENTE para que tambien este en NODE0: FUNCIONA OK
		./demonize -l $lcl $vmid 21 0 "/home/MoL_Module/mol-ipc/commands/m3urlget/webclt file10M.txt"  > webcltout.txt 2> webclterr.txt &
			Si se ejecuta en NODE1 (reemplazando $lcl x $rmt ) entonces falla.
			posiblemente falle por que  cuando finaliza el cliente lo hace con exit_unbind y este manda un mensaje al PM pero solo si 
			es LOCAL 
		
			El CLIENT (q ejecuta en NODE1) , NODE0 lo ve vivo (endpoint 21) porque basicamente no hizo el unbind sobre el PM0.

	EN NODE1:  Muere el tcp_proxy 
			[  829.607791] BUG: unable to handle kernel NULL pointer dereference at (null)
			[  829.607791] IP: [<c1593587>] __mutex_lock_slowpath+0x87/0x130
			[  829.607791] *pde = 00000000 
			[  829.607791] Oops: 0002 [#1] SMP 
			[  829.607791] last sysfs file: /sys/devices/pci0000:00/0000:00:11.0/0000:02:01.0/device
			[  829.607791] Modules linked in: mol_replace
			[  829.607791] 
			[  829.607791] Pid: 2688, comm: tcp_proxy Not tainted (2.6.32 #881) VMware Virtual Platform
			[  829.607791] EIP: 0060:[<c1593587>] EFLAGS: 00010246 CPU: 0
			[  829.607791] EIP is at __mutex_lock_slowpath+0x87/0x130
			[  829.607791] EAX: 00000000 EBX: dff1e36c ECX: 00000009 EDX: de751f00
			[  829.607791] ESI: dff1e368 EDI: dff1e378 EBP: dff1e370 ESP: de751ef8
			[  829.607791]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
			[  829.607791] Process tcp_proxy (pid: 2688, ti=de750000 task=df8a7440 task.ti=de750000)
			[  829.607791] Stack:
			[  829.607791]  df8a7440 00000000 dff1e370 00000000 00000282 dff1e368 dff1e200 e099a880
			[  829.607791] <0> df1c4168 c15934dd df1c4000 dff1e200 e09907c9 e099588c 00000a80 e099a35f
			[  829.607791] <0> 000000ad fffffffd 00000000 00000000 0000200c 00000120 00000a88 00000001
			[  829.607791] Call Trace:
			[  829.607791]  [<c15934dd>] ? mutex_lock+0x1d/0x40
			[  829.607791]  [<e09907c9>] ? mm_get2rmt+0x899/0x3d80 [mol_replace]
			[  829.607791]  [<c10ade11>] ? vfs_write+0x121/0x160
			[  829.607791]  [<e098ff30>] ? mm_get2rmt+0x0/0x3d80 [mol_replace]
			[  829.607791]  [<c159474b>] ? minix_ipc_call+0x7/0xb

		TAMBIEN MUERE SLOTS 
		
20170507: 	websrvX.sh 
			Ahora funciona, no se muy bien porque.
			1- agregue al final del cliente mol_exit
			2- modifique sleep_proc3 para que espere TIMEOUT_MOLCALL 
			3- modifique los sys_vcopy por mnx_vcopy
			4- En el cliente se dejan los resultados de salida con el mismo nombre 
				this program get /lib/init/rw/nweb/file50M.txt &  file10M.txt from websvr"
				the result is wrote in file50M.txt and file10M.txt of task/systask/ on NODE1"
				performance result is wrote in task/systask/results.txt of NODE1"

		Se probo RAWMSGQ como proxy SIN ACKNOWLEDGES
			// #define WITH_ACKS 1    // set this constant to enable ACKNOWLEDGES 
		Funciono OK para IPC pero bastante lento a prioridad (VMWare)
			Throuhput = 119.615695 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 98.658931 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 98.541661 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 86.891641 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 95.115116 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 80.927945 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 55.845662 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 65.744391 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 71.066357 [SENDREC/RECEIVE-SEND/s]
			Throuhput = 108.722052 [SENDREC/RECEIVE-SEND/s]
		
		Funciono OK para VCOPY pero bastante lento a prioridad (VMWare)
			Throuhput = 2099974.994950 [bytes/s]
		
		OJO: TAMBIEN SE EJECUTO CON DEBUG !!! DE TODO

		Se probo websrvX modificando los mnx_vcopy(FUNCIONA) por sys_vcopy(NO FUNCIONA) que fueron modificados previamente.
				[  457.748295] DEBUG 2732:sleep_proc3:857: WLOCK_PROC ep=-2 count=0
				[  457.748298] DEBUG 2732:sleep_proc3:857: WLOCK_PROC ep=20 count=0
				[  457.748301] DEBUG 2732:sleep_proc3:857: WLOCK_PROC ep=21 count=0
				[  457.748304] DEBUG 2732:sleep_proc3:855: WUNLOCK_PROC ep=-2 count=0
				[  457.748307] DEBUG 2732:sleep_proc3:855: WUNLOCK_PROC ep=20 count=0
				[  457.748310] DEBUG 2732:sleep_proc3:855: WUNLOCK_PROC ep=21 count=0
		Se puede ver que se hacen 3 LOCKS y 3 UNLOCKS 
			O sea el problema esta en el sys_vircopy()
			
		El estado de los proceso es el siguiente:
		WEBCLT1-->WEBSRV0--->SYSTASK0
		
		Aparentemente en NODE0 se envia un CMD_COPYIN_DATA 
			tcp_proxy.c:ps_send_remote:394:SPROXY:cmd=5 vmid=0 src=-2 dst=21 snode=0 dnode=1 rcode=0 len=8192
		y llega de NODE1 el ACK 
			tcp_proxy.c:pr_receive_header:110:RPROXY: cmd=8197 vmid=0 src=21 dst=-2 snode=1 dnode=0 rcode=0 len=0
			
		Ahora el websrv deberia enviar un send() de reply a NODE1 pero eso no sale-		

---------------------------------------------------------------------------------------------------------------
20170509
		Se deberia implementar sys_unbind y mol_unbind para procesos servidores 
	RESUELTO: 
		Se pueden usar int _sys_exit(int proc, int nodeid)
		se hizo mol_unbind(endpoint)
			FALTA PROBAR !!!!!!!!!
		La diferencia entre mol_exit y mol_unbind es que mol_exit avisa al parent.
		

---------------------------------------------------------------------------------------------------------------
20170513
		Se modifico RS y DEMONIZE para que en lugar de usar MSGQ utilice UDP asi se puede 
			ejecutar el demonize en forma remota.
		PROBLEMA: Hay que agregar al demonize un parametro mas!!! encima son muchos que es el "nombre" de nodo
		donde se ejecuta el RS.
		
			Usage: %s -<l|r|b> <hostname> <execnodeid> <vmid> <endpoint> <mpid|rmtnodeid> \"<command> <args...>\"  
			<node> means in which node the command will be executed (referred to PM) 
			\t\t l: local bind 
			\t\t r: replica bind 
			\t\t b: backup bind 
			<hostname> of the RS process
			<execnodeid> the node where to execute
			<vmid>: VM ID for the process
			<endpoint>: Endpoint to allocate for the process
			for LOCAL operation: <mpid> MINIX PID desired to allocate. 
			\t (0) means that PM allocates any one	
			If a <commmand> is not supplied, STDIN is read to get it

		SELECCIONADA :	Ejecutar un unico RS en el mismo nodo que PM (o no, esto no fue probado)
						El demonize lo unico que hace es conectarse por UDP en forma local/remota a el.
						El resto sigue igual
						El puerto en que escucha es el rs_port = RS_BASE_PORT + (vmid * drvs_ptr->d_nr_nodes) + local_nodeid;
						RS_BASE_PORT = 2000
						
		Alternativa: 	Se podría cargar un RS por nodo	como REPLICA
						NO FUNCIONARIA dado que usa funciones mol_xxxx con lo cual se comunica
						con el PM y el PM le va a contestar a su RS LOCAL.
		
		Alternativa: 
					El RS se bindea como REPLICA
					(demonize)--msgq-->RS-->SYSTASK---->Multicast
					
					El demonize via msgq le envia el comando a RS
					El RS le hace la solicitud a su SYSTASK
					La SYSTASK envia un mensaje a todo el grupo 
					La SYSTASKx que corresponde al nodox de ejecucion, ejecuta el comando haciendo fork y bindeandolo.

					si el bind es LOCAL, entonces todas las otras SYSTASK 
						si ya tienen bindeado un proceso local como BACKUP no hacen nada
						si ya tienen bindeado como REMOTE no modifican nada 
						si no tiene bindeado nada en ese endpoint lo setean como REMOTE 
						si tiene un proceso local como LOCAL ====> ERROR 
						
					si el bind es REPLICA, entonces todas las otras SYSTASK 
						si ya tienen bindeado un proceso local como REPLICA no hacen nada
						si ya tienen bindeado a otro proceso como REMOTE no hacen nada
						si no tiene bindeado nada en ese endpoint lo setean como REMOTE 
						si tiene un proceso local como LOCAL ====> ERROR 

					si el bind es BACKUP,  las otras SYSTASK no hacen nada.	
					Luego el CHILD hace un mol_bind al PM 
				NO FUNCIONARIA: porque el multicast no asegura la respuesta de error o no por parte de la SYSTASK comprometida

		ALTERNATIVA:	Que la SYSTASKx del NODEx del RSx solicita la ejecucion a la SYSTASKy en el NODEy.
				El problema es que la SYSTASKx debe esperar una respuesta de SYSTASKy y por lo tanto puede colgarse.


---------------------------------------------------------------------------------------------------------------
20170517:
			SE INSTALO WEBMIN DESDE TAR Y FUNCIONA PERFECTO. TAMBIEN SE INSTALARON LAS APIS 

	PROBLEMA GENERICO: SOLUCIONADO Y VERIFICADO 

		Esto seguramente se debe a que la implementacion del  warnproc a quien debe reportar el unbind se ha hecho
		solo en forma local.
			En este ejemplo se hizo kill -9 de RS (Habia un error en PM que no consideraba un error de retorno de _sys_exit)
			FUNCION OK!!!!
			[  107.943540] DEBUG RS:mm_exit_unbind:156: code=0
			[  107.943546] DEBUG RS:mm_exit_unbind:160: WLOCK_TASK pid=RS count=0
			[  107.943598] DEBUG RS:mm_exit_unbind:166: RLOCK_PROC ep=2 count=0
			[  107.943659] DEBUG RS:mm_exit_unbind:167: nr=2 endp=2 vmid=0 flags=0 misc=20 lpid=RS nodeid=0 nodemap=1 name=rs 
			[  107.943713] DEBUG RS:mm_exit_unbind:200:  Exiting endpoint=2 lpid=RS
			[  107.943753] DEBUG RS:mm_exit_unbind:202: RUNLOCK_PROC ep=2 count=0
			[  107.943812] DEBUG RS:mm_exit_unbind:203: WLOCK_VM vm=0 count=0
			[  107.943857] DEBUG RS:mm_exit_unbind:204: WLOCK_PROC ep=2 count=0
			[  107.943897] DEBUG RS:mm_exit_unbind:211:  endpoint=2 lpid=RS
			[  107.943999] DEBUG RS:mm_exit_unbind:236: WUNLOCK_PROC ep=2 count=0
			[  107.944291] DEBUG RS:mm_exit_unbind:237: WLOCK_PROC ep=0 count=0
			[  107.944337] DEBUG RS:mm_exit_unbind:237: WLOCK_PROC ep=2 count=0
			[  107.944391] DEBUG RS:mm_exit_unbind:255: vm_vmid=0 vm_warn2proc=0 vm_warnmsg=1
			[  107.944470] DEBUG RS:kernel_warn2proc:22: caller_ep=2 warn_ep=0 
			[  107.944536] DEBUG RS:kernel_warn2proc:25: nr=0 endp=0 vmid=0 flags=8 misc=A0 lpid=2774 nodeid=0 nodemap=1 name=pm 
			[  107.944591] DEBUG RS:kernel_warn2proc:91: destination is waiting. Copy the message and wakeup destination
			[  107.944694] DEBUG RS:kernel_warn2proc:102: source=2 type=1 m1i1=0 m1i2=-1 m1i3=0 m1p1=(null) m1p2=(null) m1p3=(null)
			[  107.944720] DEBUG RS:inherit_cpu:357: cpuid=0
			[  107.944728] DEBUG RS:inherit_cpu:365: nr=0 endp=0 vmid=0 lpid=2774 p_cpumask=FF name=pm 
			[  107.944765] DEBUG RS:kernel_warn2proc:105: BEFORE UP lpid=2774 p_sem=-1 rcode=0
			[  107.944826] DEBUG RS:kernel_warn2proc:127: WUNLOCK_PROC ep=2 count=0
			[  107.944872] DEBUG RS:kernel_warn2proc:127: WUNLOCK_PROC ep=0 count=0
			[  107.944936] DEBUG RS:kernel_warn2proc:128: WUNLOCK_VM vm=0 count=0
			[  107.944990] DEBUG RS:kernel_warn2proc:130: WUNLOCK_TASK pid=RS count=0			
			......	
			[  107.948995] DEBUG RS:kernel_warn2proc:133: WLOCK_TASK pid=RS count=0
			[  107.949120] DEBUG RS:kernel_warn2proc:134: WLOCK_VM vm=0 count=0
			[  107.949174] DEBUG RS:kernel_warn2proc:135: WLOCK_PROC ep=0 count=0
			[  107.949221] DEBUG RS:kernel_warn2proc:135: WLOCK_PROC ep=2 count=0
			[  107.949272] DEBUG RS:kernel_warn2proc:137: nr=2 endp=2 vmid=0 flags=0 misc=124 lpid=RS nodeid=0 nodemap=1 name=rs 
			[  107.949325] DEBUG RS:mm_exit_unbind:266: WUNLOCK_PROC ep=0 count=0
			[  107.949419] DEBUG RS:do_unbind:2117: nr=2 endp=2 vmid=0 flags=0 misc=124 lpid=RS nodeid=0 nodemap=1 name=rs 	

	EL LOG DEL PM DICE LO SIGUIENTE 
		 pm.c:get_work:123:Wait for the next message and extract useful information from it.
		 pm.c:get_work:128:Request received from who_e=2, call_nr=1
		 utility.c:pm_isokendpt:40:endpoint=2 
		 utility.c:pm_isokendpt:42:*proc=2
		 utility.c:pm_isokendpt:43:kproc.p_endpoint=2
		 pm.c:get_work:145:mp_endpoint=2 mp_pid=2 mp_parent=0 mp_flags=2001 mp_nice=-10 
		 forkexit.c:do_pm_exit:240:who_p=2 who_e=2 lnx_pid=-1 status=0
		 forkexit.c:pm_exit:322:mnx_pid=2 status=0 proc_nr=2 proc_ep=2 oper=1
		 forkexit.c:pm_exit:336:parent_pid=0
		 forkexit.c:pm_exit:353:nr=2 endp=2 vmid=0 flags=8 misc=20 lpid=2777 nodeid=0 nodemap=1 name=rs 
		 utility.c:sys_procinfo:105:Sending SYS_GETINFO request 11 to SYSTEM for p_nr=2
		 utility.c:sys_procinfo:109:nr=2 endp=2 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname << flags=1 => FREESLOT
		 forkexit.c:pm_exit:364:nr=2 endp=2 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		 signal.c:sig_proc:212:sending signal 17 to minix PID 0 <<< esto ya se arreglo controlando que si el parent es igual a PM_PROC_NR
		 forkexit.c:pm_exit:413:PID=2 pgrp=0 ruid=-1 euid=-1 rgid=-1 egid=-1 flags=0
		 pm.c:main:70:call_nr=1 result=-998 <= SUSPEND
		 pm.c:main:80:Send out all pending reply messages
		 pm.c:get_work:123:Wait for the next message and extract useful information from it.				

---------------------------------------------------------------------------------------------------------------
20170518:
		 No funciona bien cuando termina un proceso REMOTO al PM. Es decir cuando hace el exit_unbind en forma remota	 
		 
		 
		 El proceso finalizado (RDISK1) envia al terminar un mensaje exit al PM0 correctamente
		 El PM0 hace sys_exit a SYSTASK1 correctamente
		 Pero SYSTASK1 ya ve al proceso unbindeado por lo que no informa al resto de las SYSTASK
			system.c:main:128:RECEIVE msg:source=0 type=1538 m4l1=3 m4l2=2 m4l3=11 m4l4=35534 m4l5=145727140
			 system.c:main:137:call_nr=2 who_e=0
			 system.c:main:142:Calling vector 2
			 do_exit.c:do_exit:31:proc_ep=3
			 do_exit.c:do_exit:43:before nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
			 system.c:main:153:REPLY msg:source=0 type=-310 m1i1=3 m1i2=2 m1i3=11 m1p1=0x8ace m1p2=0x8af9ea4 m1p3=(nil) <<<< 310 EMOLNOTBIND 
			 system.c:main:116:SYSTASK is waiting for requests
		 
		SOLUCIONADO: 
			Se basa en que SYSTASK no controla la existencia del proceso que hace exit en base a lo que el kernel tiene registrado
			que es que esta UNBIND, sino que lo hace en base a lo que la SYSTASK tiene en su tabla de SLOTS.
			Resta probar nuevamente LOCAL y REMOTO 
			Se arranco ./unikernel0.sh 3 en NODE0
			Se arranco ./unikernel1.sh 3 en NODE1
			Luego al finalizar se hace kill de rdisk(3) en NODE1 
			
			NODE0					NODE1
	1-		PM0 <------MOLEXIT------ RDISK
	2-		PM0 -----sys_exit------> SYSTASK1
	3-      SLOTS0 <--- MOLEXIT----- SYSTASK1
			
			
		RDISK1: Envia el mensaje MOLEXIT a PM0 , luego hace do_unbind. 
		[  167.570508] DEBUG 2784:kernel_warn2proc:22: caller_ep=3 warn_ep=0 
		[  167.570554] DEBUG 2784:kernel_warn2proc:25: nr=0 endp=0 vmid=0 flags=1000 misc=0 lpid=-1 nodeid=0 nodemap=1 name=pm 
		[  167.570617] DEBUG 2784:kernel_warn2proc:32: RLOCK_PROC ep=27342 count=0
		[  167.570685] DEBUG 2784:kernel_warn2proc:44: RUNLOCK_PROC ep=27342 count=0
		[  167.570807] DEBUG 2784:kernel_warn2proc:66: source=3 type=1 m1i1=569698 m1i2=-1 m1i3=0 m1p1=(null) m1p2=(null) m1p3=(null) 
		[  167.570868] DEBUG 2784:kernel_warn2proc:70: source=3 type=1 m1i1=569698 m1i2=-1 m1i3=0 m1p1=(null) m1p2=(null) m1p3=(null) 
		
			
		PM0:	Recibe el mensaje de MOLEXIT de RDISK1 Luego solicita sys_exit a SYSTASK1
		 forkexit.c:do_pm_exit:240:who_p=3 who_e=3 lnx_pid=-1 status=569698
		 forkexit.c:pm_exit:322:mnx_pid=0 status=569698 proc_nr=3 proc_ep=3 oper=1
		 forkexit.c:pm_exit:336:parent_pid=0
		 forkexit.c:pm_exit:353:nr=3 endp=3 vmid=0 flags=1000 misc=0 lpid=-1 nodeid=1 nodemap=2 name=rdisk 
		 utility.c:sys_procinfo:105:Sending SYS_GETINFO request 11 to SYSTEM for p_nr=3
		 utility.c:sys_procinfo:109:nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		 forkexit.c:pm_exit:364:nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		 signal.c:sig_proc:212:sending signal 17 to minix PID 0
		 forkexit.c:pm_exit:413:PID=0 pgrp=0 ruid=-1 euid=-1 rgid=-1 egid=-1 flags=0
		 pm.c:main:70:call_nr=1 result=-998
		 pm.c:main:80:Send out all pending reply messages
		 pm.c:get_work:123:Wait for the next message and extract useful information from it.
 
		SYSTASK1: Recibe el sys_exit de PM0 y hace multicast MOLEXIT
			system.c:main:128:RECEIVE msg:source=0 type=1538 m4l1=3 m4l2=2 m4l3=11 m4l4=35534 m4l5=158408356
			 system.c:main:137:call_nr=2 who_e=0
			 system.c:main:142:Calling vector 2
			 do_exit.c:do_exit:31:proc_ep=3
			 do_exit.c:do_exit:43:before nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
			 do_exit.c:do_exit:56:vm_nr_tasks=35 proc_nr=3 vm_nr_sysprocs=64
			 slots.c:mcast_exit_proc:226:nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
			 slots.c:slots_loop:1924:sender=#1.0#node1 Private_group=#1.0#node1 vm_name=VM0 service_type=32 SP_bytes=36 
			 slots.c:slots_loop:1937:message from #1.0#node1, of type 20, (endian 0) to 1 groups (36 bytes)
			 slots.c:slots_loop:1985:source=1 type=1 m3i1=3 m3i2=3 m3p1=(nil) m3ca1=[rdisk]
			 slots.c:sp_syscall:1592:source=1 m_type=1 p_nr=3 p_endpoint=3
			 do_exit.c:do_exit:69:s_nr=38 s_endpoint=27342 s_flags=0 s_owner=-1
			 do_exit.c:do_exit:70:proc_nr=3 $noname
			 do_exit.c:do_exit:77:UNBIND proc_ep=3
			 do_exit.c:do_exit:96:endpoint 3 unbound
			 do_exit.c:do_exit:101:proc_nr=3 free_slots=5
			 do_exit.c:do_exit:112:after nr=3 endp=3 vmid=0 flags=1 misc=100 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
			 system.c:main:153:REPLY msg:source=0 type=0 m1i1=3 m1i2=2 m1i3=11 m1p1=0x8ace m1p2=0x9711ea4 m1p3=(nil) 
 
 		SLOT0:	Recibe el multicast MOLEXIT de SYSTASK1 
		 slots.c:slots_loop:1924:sender=#1.0#node1 Private_group=#0.0#node0 vm_name=VM0 service_type=32 SP_bytes=36 
		 slots.c:slots_loop:1937:message from #1.0#node1, of type 20, (endian 0) to 1 groups (36 bytes)
		 slots.c:slots_loop:1985:source=1 type=1 m3i1=3 m3i2=3 m3p1=(nil) m3ca1=[rdisk]
		 slots.c:sp_syscall:1592:source=1 m_type=1 p_nr=3 p_endpoint=3
		 slots.c:sp_syscall:1753:MOLEXIT p_nr=3 p_endpoint=3
		 slots.c:sp_syscall:1765:nr=3 endp=3 vmid=0 flags=1000 misc=0 lpid=-1 nodeid=1 nodemap=2 name=rdisk 
		 do_getinfo.c:do_getinfo:38:caller=0 rqst=11 addr=9711F78 len=212
		 do_getinfo.c:do_getinfo:89:p_ep=3 p_nr=3 
		 do_getinfo.c:do_getinfo:106:AFTERnr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		 slots.c:sp_syscall:1798:nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 

		COMO FUNCIONA CUANDO EL PM  Y EL PROCESO QUE MUERE ESTAN EN EL MISMO NODO
		=========================================================================
		RDISK0: Informa MOLEXIT a PM0 
		[  850.541556] DEBUG 2832:kernel_warn2proc:22: caller_ep=3 warn_ep=0 
		[  850.541601] DEBUG 2832:kernel_warn2proc:25: nr=0 endp=0 vmid=0 flags=8 misc=A0 lpid=2837 nodeid=0 nodemap=1 name=pm 
		[  850.541653] DEBUG 2832:kernel_warn2proc:107: destination is waiting. Copy the message and wakeup destination
		[  850.541755] DEBUG 2832:kernel_warn2proc:118: source=3 type=1 m1i1=540784 m1i2=-1 m1i3=0 m1p1=(null) m1p2=(null) m1p3=(null) 
		[  850.541780] DEBUG 2832:inherit_cpu:357: cpuid=0
		[  850.541786] DEBUG 2832:inherit_cpu:365: nr=0 endp=0 vmid=0 lpid=2837 p_cpumask=FF name=pm 
		[  850.541823] DEBUG 2832:kernel_warn2proc:121: BEFORE UP lpid=2837 p_sem=-1 rcode=0
		[  850.541885] DEBUG 2832:kernel_warn2proc:143: WUNLOCK_PROC ep=3 count=0
		[  850.541932] DEBUG 2832:kernel_warn2proc:143: WUNLOCK_PROC ep=0 count=0
		[  850.541981] DEBUG 2832:kernel_warn2proc:144: WUNLOCK_VM vm=0 count=0
		[  850.542049] DEBUG 2832:kernel_warn2proc:146: WUNLOCK_TASK pid=2832 count=0
		
		PM0: Recibe el MOLEXIT de RDISK0 y solicita el sys_exit a SYSTASK0
		 pm.c:get_work:128:Request received from who_e=3, call_nr=1
		 utility.c:pm_isokendpt:40:endpoint=3 
		 utility.c:pm_isokendpt:42:*proc=3
		 utility.c:pm_isokendpt:43:kproc.p_endpoint=3
		 pm.c:get_work:145:mp_endpoint=0 mp_pid=0 mp_parent=0 mp_flags=1 mp_nice=0 
		 forkexit.c:do_pm_exit:240:who_p=3 who_e=3 lnx_pid=-1 status=540784
		 forkexit.c:pm_exit:322:mnx_pid=0 status=540784 proc_nr=3 proc_ep=3 oper=1
		 forkexit.c:pm_exit:336:parent_pid=0
		 forkexit.c:pm_exit:353:nr=3 endp=3 vmid=0 flags=8 misc=20 lpid=2832 nodeid=0 nodemap=1 name=rdisk 
		 utility.c:sys_procinfo:105:Sending SYS_GETINFO request 11 to SYSTEM for p_nr=3
		 utility.c:sys_procinfo:109:nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		 forkexit.c:pm_exit:364:nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		 signal.c:sig_proc:212:sending signal 17 to minix PID 0
		 forkexit.c:pm_exit:413:PID=0 pgrp=0 ruid=-1 euid=-1 rgid=-1 egid=-1 flags=0
		 pm.c:main:70:call_nr=1 result=-998	

		SYSTASK0: Recibe el sys_exit de PM0 y hace un multicast MOLEXIT al resto de los nodos
		 system.c:main:128:RECEIVE msg:source=0 type=1538 m4l1=3 m4l2=2 m4l3=11 m4l4=35534 m4l5=148848292
		 system.c:main:137:call_nr=2 who_e=0
		 system.c:main:142:Calling vector 2
		 do_exit.c:do_exit:31:proc_ep=3
		 do_exit.c:do_exit:43:before nr=3 endp=3 vmid=0 flags=8 misc=120 lpid=2832 nodeid=0 nodemap=1 name=rdisk 
		 do_exit.c:do_exit:56:vm_nr_tasks=35 proc_nr=3 vm_nr_sysprocs=64
		 slots.c:mcast_exit_proc:226:nr=3 endp=3 vmid=0 flags=8 misc=120 lpid=2832 nodeid=0 nodemap=1 name=rdisk 
		 do_exit.c:do_exit:69:s_nr=38 s_endpoint=27342 s_flags=0 s_owner=-1
		 do_exit.c:do_exit:70:proc_nr=3 rdisk
		 do_exit.c:do_exit:81:NOTIFY proc_ep=3
		 do_exit.c:do_exit:85:mnx_wait4unbind_T
		 do_exit.c:do_exit:88:mnx_wait4unbind_T  rcode=0
		 do_exit.c:do_exit:96:endpoint 3 unbound
		 do_exit.c:do_exit:101:proc_nr=3 free_slots=193
		 do_exit.c:do_exit:112:after nr=3 endp=3 vmid=0 flags=1 misc=0 lpid=-1 nodeid=-1 nodemap=0 name=$noname 
		 system.c:main:153:REPLY msg:source=0 type=0 m1i1=3 m1i2=2 m1i3=11 m1p1=0x8ace m1p2=0x8df3ea4 m1p3=(nil) 

---------------------------------------------------------------------------------------------------------------
20170521:
		Se hizo commands/simple/mnx_ifconfig que a su vez necesito de lib/ip
		Para realizar la prueba se modifico el floppy3RWX.img de tal modo que se monto como diskette en un verdadero
		MINIX y se creo el directorio /dev y los mknod para /dev/ip /dev/eth /dev/tcp /dev/udp
				
		mnx_ifconfig: realiza las siguientes operaciones en secuencia cuando se ejecuta sin parametros.
			mol_open: termina correctamente
			mol_fcntl(ip_fd, F_GETFL); termina OK
			mol_fcntl(ip_fd, F_SETFL, flags | O_NONBLOCK); termina OK-		
			result= mol_ioctl(ip_fd, NWIOGIPCONF, &ipconf);
				RS0:
				 main.c:get_work:151: LISTENING . 
				 main.c:get_work:155:source=15 type=54 MOLIOCTL m1i1=0 m1i2=0 m1i3=-2146406879 m1p1=(nil) m1p2=(nil) m1p3=0xbfe920d4 
				 main.c:get_work:156:source=15 type=54 m3i1=0 m3i2=0 m3p1=0x80106e21 m3ca1=[]
				 main.c:get_work:176:call_nr = 54 
				 filedes.c:get_filp:71:fild is=0
				 device.c:do_ioctl:358:i_num=47
				i_mode=21A4
				i_nlinks=1
				i_uid=0
				i_gid=0
				i_dev=256
				i_ndzones=7
				i_nindirs=1024
				 device.c:do_ioctl:360:I_CHAR_SPECIAL=2000 I_BLOCK_SPECIAL=6000
				 device.c:do_ioctl:365:dev=901
				 device.c:dev_io:114:op=405 dev=2305 major=9 minor=1 proc_e=15 pos=0 bytes=-2146406879 flags=802
				 device.c:d_gen_io:825:task_nr=9
				 inet.c:main:140:source=1 type=1029 m2i1=1 m2i2=15 m2i3=-2146406879 m2l1=0 m2l2=2050 m2p1=0xbfe920d4
				 inet.c:main:145:message from FS_PROC_NR
				 sr.c:sr_rec:120:sr_rec m_type=1029
				 sr.c:sr_rwio:277:source=1 type=1029 m2i1=1 m2i2=15 m2i3=-2146406879 m2l1=0 m2l2=2050 m2p1=0xbfe920d4
				 sr.c:sr_getchannel:622:minor=1
				 sr.c:sr_getchannel:632:srf_port=0 srf_flags=3 0 2 
				 main.c:reply:405: Send a reply to a user process.
				 main.c:reply:409:source=0 type=-5 EMOLIO m1i1=0 m1i2=0 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 

			STDERR DE RS0:
			inet: sr.c:635: sr_getchannel: Assertion `!(loc_fd->srf_flags & 0x01) && (loc_fd->srf_flags & 0x02)' failed.
			fs: dead driver 9
			ERROR: device.c:d_gen_io:838: rcode=-108
			/home/MoL_Module/mol-ipc/commands/simple/mnx_ifconfig: Unable to get IP configuration: Input/output error
		 

		ESTO ES CORRECTO - Comprobado con stat en minix para /dev/ip
			inode.c:get_inode:75:i_num=47
			i_mode=21A4
			i_nlinks=1
			i_uid=0
			i_gid=0
			i_dev=256
			i_ndzones=7
			i_nindirs=1024
		
---------------------------------------------------------------------------------------------------------------
20170525:
		sr_fd= &sr_fd_table[i];  // aqui el puntero apunta a un slot de la tabla de fd de SRC_MAC
		*sr_fd= sr_fd_table[minor]; // AQUI COPIA EL CONTENIDO de lo que dice la entrada minor sobre la entrada i 
		sr_fd->srf_flags= SFF_INUSE; // Luego modifica la entrada i, la entrada minor queda igual 
			
		
		sr.c:sr_open:216:minor=1 sr_fd_table[minor].srf_flags=3  <<< este es el valor de la entrada minor 	 	
		sr.c:sr_open:246:minor=1 i=4 srf_flags=2 <<< este es el valor de la entrada i

		mas adelante sr_getchannel evalua los flags de la entrada minor  y salta el error.
		sr.c:sr_getchannel:633:srf_fd=0 srf_port=0 srf_flags=3 0 2 << flags de entrada minor !!!!
		 
		LOS comandos de tipo IOCTL estan codificados como:
		#define NWIOGIPCONF	_IOR('n', 33, struct nwio_ipconf)
		#define _IOC_OUT	0x80000000
		#define _IOR(x,y,t)	((x << 8) | y | ((sizeof(t) & _IOCPARM_MASK) << 16) | _IOC_OUT)
				
		es decir:
			empienzan con 0x80000000 
			luego le sigue el tamaño de la estructura que recibira el contenido de una operacion GET 
			luego , en este caso le sigue la letra 'n' indicando que es "network". En caso de TTY es 'T'
			Luego le sigue el comando propiamente dicho q en este caso es 33 
		
		Esto es lo que recibe INET de FS 
		inet.c:main:140:source=1 type=1029 m2i1=1 m2i2=15 m2i3=-2146406879 m2l1=0 m2l2=2050 m2p1=0xbfc64c94
		Y es visto por SR 
		sr.c:sr_rwio:277:source=1 type=1029 m2i1=1 m2i2=15 m2i3=-2146406879 m2l1=0 m2l2=2050 m2p1=0xbfc64c94
		-2146406879 es equivalente a  NWIOGIPCONF=80106E21 el 0x21 final indica el comando 33 decimal
		
		FINALMENTE FUNCIONO pero no muy bien
		/dev/ip: address 48.249.137.183 mtu 5020 

---------------------------------------------------------------------------------------------------------------
20170526:
		El problema es el siguiente
		
		Cuando el endpoint de mnx_ifconfig era 15 retornaba
		device.c:dev_io:196:BEFORE return dev_mess.REP_STATUS=15 
		
		Cuando el endpoint de mnx_ifconfig era 15 retornaba
		device.c:dev_io:198:BEFORE return dev_mess.REP_STATUS=16 
		
		En realidad deberia retornar error porque no esta configurado aun las direcciones IP de las interfaces.

		MODIFICADO!! --------------------------------------------------------------
		Ahora retorna 
		 device.c:dev_io:200:BEFORE return dev_mess.REP_STATUS=-19 <<<< EMOLNODEV
		 main.c:reply:405: Send a reply to a user process.
		 main.c:reply:409:source=0 type=-19 m1i1=0 m1i2=0 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 
		 mnx_ifconfig.c:get_ipconf:228:result=-1 error=19
		
	
		FS->CANCEL->INET 
		 inet.c:main:140:source=1 type=1024 m2i1=1 m2i2=16 m2i3=135245064 m2l1=135245108 m2l2=135245112 m2p1=0x4
		 inet.c:main:145:message from FS_PROC_NR
		 sr.c:sr_rec:120:sr_rec m_type=1024
		 sr.c:sr_rec:123:source=1 type=1024 m2i1=1 m2i2=16 m2i3=135245064 m2l1=135245108 m2l2=135245112 m2p1=0x4
		 sr.c:sr_cancel:474:source=1 type=1024 m2i1=1 m2i2=16 m2i3=135245064 m2l1=135245108 m2l2=135245112 m2p1=0x4
		 sr.c:sr_getchannel:637:minor=1
		 sr.c:sr_getchannel:648:srf_fd=0 srf_port=0 srf_flags=47 0 2 
		 sr.c:sr_getchannel:657:return srf_fd=0

		 INET SE CAE LUEGO
		inet: generic/icmp.c:196: icmp_getdata: Assertion `result == 0' failed.
		fs: dead driver 9
		ERROR: device.c:d_gen_io:843: rcode=-108
		
		Para probar si el causante era mnx_ifconfig se puso en comentario 
		// WARNING SOLO PARA PROBAR - ELIMITAR COMENTARIO DE ABAJO 
		//	mol_fcntl(ip_fd, F_SETFL, flags);
		PERO ESO NO ES 
		
		RASTREAR POR AQUI
		inet: generic/icmp.c:196: icmp_getdata: Assertion `result == 0' failed.
		Quien produce la caida ??? FS o recibe algun frame que lo hace caer ?
		
		Lo ultimo visible de inet es 
			 sr.c:sr_cancel:482:srf_fd=0
			 sr.c:walk_queue:609:srf_fd=0
			 sr.c:walk_queue:617:srf_flags=47
			 generic/ip.c:ip_cancel:147:fd=0 which_operation=1 <<< SR_CANCEL_IOCTL
			generic/ip.c:ip_cancel:153:if_flags=9
			generic/ip.c:ip_cancel:156:if_flags=1	
 
 		case SR_CANCEL_IOCTL:
			SVRDEBUG("if_flags=%d\n", ip_fd->if_flags);
			assert (ip_fd->if_flags & IFF_IOCTL_IP);
			ip_fd->if_flags &= ~IFF_IOCTL_IP;
			SVRDEBUG("if_flags=%d\n", ip_fd->if_flags);
			repl_res= (*ip_fd->if_get_userdata)(ip_fd->if_srfd,  <<<<<<< ACA MUERE 
				(size_t)EMOLINTR, (size_t)0, TRUE);
			SVRDEBUG("repl_res=%d\n", repl_res);				<<<< ESTO NO SE IMPRIME 

		MODIFICADO!! --------------------------------------------------------------
			Ahora ya paso el ifconfig de visualizacion (endpoint 15)
			y este es el ifconfig (endpoint 16) de seteo de direccion IP y otros
				mnx_ifconfig -I /dev/ip -h 172.16.1.1 -n 255.255.255.0 -m 1500
			Queda mnx_ifconfig bloqueado esperando respuesta del FS 
			Lo cual esta bien dado que INET->SUSPEND->FS 
			El tema es cuando se revive!!!???? 
			
			mnx_ifconfig-->mol_ioctl-->FS---DEV_IOCTL-->INET 
				 mnx_ifconfig.c:main:122:ip_fd=0
				 mnx_ifconfig.c:set_hostaddr:178:ip_fd=0 hostaddr_s=172.16.1.1 ins=0
				 mol_ioctl.c:mol_ioctl:7:fd=0 request=1074818592 data=
				 main.c:get_work:155:source=16 type=54 m1i1=0 m1i2=0 m1i3=1074818592 m1p1=(nil) m1p2=(nil) m1p3=0xbfb0dda8 
				 main.c:get_work:156:source=16 type=54 m3i1=0 m3i2=0 m3p1=0x40106e20 m3ca1=[]
				 main.c:get_work:176:call_nr = 54 
				 filedes.c:get_filp:71:fild is=0
				 device.c:do_ioctl:364:i_num=47
				i_mode=21A4
				i_nlinks=1
				i_uid=0
				i_gid=0
				i_dev=256
				i_ndzones=7
				i_nindirs=1024
				 device.c:do_ioctl:366:I_CHAR_SPECIAL=2000 I_BLOCK_SPECIAL=6000
				 device.c:do_ioctl:371:dev=901
				 device.c:dev_io:116:op=405 dev=2305 major=9 minor=1 proc_e=16 pos=0 bytes=1074818592 flags=2
				 device.c:dev_io:171:dmap_driver=9 source=-1217142884 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfb0dda8
				 device.c:d_gen_io:836:task_nr=9 source=-1217142884 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfb0dda8
				 inet.c:main:140:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfb0dda8
				 inet.c:main:145:message from FS_PROC_NR
				 sr.c:sr_rec:120:sr_rec m_type=1029
				 sr.c:sr_rec:123:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfb0dda8
				 sr.c:sr_rwio:292:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfb0dda8
				 sr.c:sr_getchannel:663:minor=1
				 sr.c:sr_getchannel:674:srf_fd=0 srf_port=0 srf_flags=47 0 2 
				 sr.c:sr_getchannel:683:return srf_fd=0
				 sr.c:sr_rwio:315:DEV_IOCTL3 type=1029
				 sr.c:sr_rwio:326:srf_flags=47 ip_flag=4 (SFF_IOCTL_IP)  << 0x47 = 0100 0111 = (SFF_IOCTL_SUSP | SFF_IOCTL_IP | SFF_INUSE | SFF_MINOR )
				 sr.c:sr_rwio:332:q_head_ptr == NULL
				 sr.c:sr_reply_:698:proc=16 status=-998 operation=405
				 sr.c:sr_reply_:707:dest=1 source=1074818592 type=67 m2i1=16 m2i2=-998 m2i3=135248680 m2l1=135248724 m2l2=135248728 m2p1=0x4
				 inet.c:main:134:mnx_receive 
			
			ANALIZAR QUE PASA CON  srf_flags=47
			En el primer mnx_ifconfig (solo visualizacion)
				sr.c:sr_rwio:328:srf_flags=3 srf_fd=0 srf_port=0 srf_select_proc=0
				sr.c:sr_rwio:353:srf_flags=207 srf_fd=0 srf_port=0 srf_select_proc=0
			En el CANCEL
				sr.c:sr_getchannel:685:srf_fd=0 srf_port=0 srf_flags=47 0 2 
				sr.c:walk_queue:655:srf_flags=247 (el 0x200 es  SFF_IOCTL_FIRST)
			Luego en el segundo mnx_ifconfig
				sr.c:sr_open:228:minor=1 sr_fd_table[minor].srf_flags=47 
				sr.c:sr_open:258:minor=1 i=5 srf_flags=2 srf_fd=4
				sr.c:sr_getchannel:685:srf_fd=0 srf_port=0 srf_flags=47 0 2 

---------------------------------------------------------------------------------------------------------------
20170529:
			
			sr.c:sr_reply_:708:proc=16 status=-998 (SUSPEND) operation=405
			sr.c:sr_reply_:717:dest=1 source=1074818592 type=67 (MOLREVIVE) m2i1=16 m2i2=-998 (SUSPEND) 
								m2i3=135249832 m2l1=135249876 m2l2=135249880 m2p1=0x4
 
			Se supone que en  FS:device.c:d_gen_io deberia invocar a revive() (pipe.c)
		
---------------------------------------------------------------------------------------------------------------
20170530:
		 FS->IOCTL-INET 
		 device.c:do_ioctl:371:dev=901
		 device.c:dev_io:116:op=405 dev=2305 major=9 minor=1 proc_e=16 pos=0 bytes=1074818592 flags=2
		 device.c:dev_io:171:dmap_driver=9 source=-1217945700 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfec8ca8
		 device.c:d_gen_io:836:task_nr=9 proc_e=16 source=-1217945700 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfec8ca8
		 inet.c:main:140:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfec8ca8
		 inet.c:main:145:message from FS_PROC_NR
		 sr.c:sr_rec:120:sr_rec m_type=1029
		 sr.c:sr_rec:123:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfec8ca8
		 sr.c:sr_rwio:291:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfec8ca8
		 sr.c:sr_getchannel:674:minor=1
		 sr.c:sr_getchannel:685:srf_fd=0 srf_port=0 srf_flags=47 0 2 
		 sr.c:sr_getchannel:694:return srf_fd=0
		 sr.c:sr_rwio:296:srf_flags=47 srf_fd=0 srf_port=0 srf_select_proc=0
		 sr.c:sr_rwio:316:DEV_IOCTL3 type=1029
		 sr.c:sr_rwio:327:srf_flags=47 ip_flag=4
		 sr.c:sr_rwio:328:srf_flags=47 srf_fd=0 srf_port=0 srf_select_proc=0
		 sr.c:sr_rwio:335:q_head_ptr == NULL
		 sr.c:sr_reply_:709:proc=16 status=-998 (SUSPEND) operation=405 is_revive=0
		 sr.c:sr_reply_:718:dest=1 source=1074818592 type=67 (MOLREVIVE) m2i1=16 m2i2=-998(SUSPEND) m2i3=135249832 m2l1=135249876 m2l2=135249880 m2p1=0x4
		 mq.c:mq_get:35:
		 inet.c:main:134:mnx_receive
		 device.c:d_gen_io:838:source=9 type=67(MOLREVIVE) m2i1=16 m2i2=-998(SUSPEND) m2i3=135249832 m2l1=135249876 m2l2=135249880 m2p1=0x4
		 device.c:d_gen_io:856:proc_e=16  REP_ENDPT=16
		 main.c:get_work:151: LISTENING . 

		 De esta  forma, INET le responde a FS pero le dice que SUSPEND a mnx_ifconfig-.
		 COMO SE DESPIERTA???
		 
		Cuando se hace el  mnx_ifconfig para ver la configuracion se setea el flag O_NONBLOCK
		como INET bloquea el solicitante para cualquier operacion relacionada a IP
			int sr_rwio(mq_t *m)

			if (sr_fd->srf_flags & ip_flag)	{
				....
				ERROR_RETURN(SUSPEND);
			}
		Entonces el FS envia un CANCEL. 
		Este cancel provoca que se ejecute sr_cancel que invoca a walk_queue
		walk_queue despierta entonces al proceso mnx_ifconfig con el codigo de error

		En el caso de mnx_ifconfig para modificar la configuracion  NO se setea el flag O_NONBLOCK
		por lo que queda bloqueado.
		
		Segun el  codigo original 
			if (sr_fd->srf_flags & ip_flag)		{
				assert(sr_fd->srf_flags & susp_flag);
				assert(*q_head_ptr); => assert(*q_head_ptr != NULL);
				(*q_tail_ptr)->mq_next= m;
				*q_tail_ptr= m;
				return SUSPEND;
			}
		Pero en el codigo modificado
			if(*q_head_ptr == NULL){
				SVRDEBUG("q_head_ptr == NULL\n");
				*q_head_ptr = m;
			}else{
				SVRDEBUG("q_head_ptr != NULL\n");
				(*q_tail_ptr)->mq_next= m;
			}		
		y esta dando == NULL !!! 	
		 sr.c:sr_rwio:335:q_head_ptr == NULL
		 
		 
		SE PROBO LO SIGUIENTE:
			Ejecutar primero el mnx_ifconfig modificando la configuracion
			
		Esto dio como resultado 
		
		FS-IOCTL->INET 
		 inet.c:main:140:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfcbe3b8
		 inet.c:main:145:message from FS_PROC_NR
		 sr.c:sr_rec:120:sr_rec m_type=1029
		 sr.c:sr_rec:123:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfcbe3b8
		 sr.c:sr_rwio:291:source=1 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfcbe3b8
		 sr.c:sr_getchannel:674:minor=1
		 sr.c:sr_getchannel:685:srf_fd=0 srf_port=0 srf_flags=3 0 2 
		 
		 sr.c:sr_getchannel:694:return srf_fd=0
		 sr.c:sr_rwio:296:srf_flags=3 srf_fd=0 srf_port=0 srf_select_proc=0
		 sr.c:sr_rwio:316:DEV_IOCTL3 type=1029
		 sr.c:sr_rwio:327:srf_flags=3 ip_flag=4
		 sr.c:sr_rwio:328:srf_flags=3 srf_fd=0 srf_port=0 srf_select_proc=0
		 sr.c:sr_rwio:353:srf_flags=207 srf_fd=0 srf_port=0 srf_select_proc=0
		 sr.c:sr_rwio:368:DEV_IOCTL3 request=40106E20
		 sr.c:sr_rwio:395:srf_fd=0 request=40106E20
		 generic/ip_ioctl.c:ip_ioctl:50:fd=0 req=40106E20
		 generic/ip_ioctl.c:ip_ioctl:176:ip_port:0 req=40106E20 
		 generic/ip_ioctl.c:ip_ioctl:192:req=NWIOSIPCONF
		 generic/icmp.c:icmp_getdata:173:port=0 offset=0 count=16 for_ioctl=1
		 generic/ip_ioctl.c:ip_ioctl:196:req=NWIOSIPCONF acc_length=56 
			ACA DA EL ERROR 
			inet: generic/ip_ioctl.c:197: ip_ioctl: Assertion `data->acc_length == sizeof(*ipconf)' failed. <<<<<<<<<<<<<<<<<<<
		 device.c:d_gen_io:838:source=-1218936932 type=1029 m2i1=1 m2i2=16 m2i3=1074818592 m2l1=0 m2l2=2 m2p1=0xbfcbe3b8
		 main.c:reply:405: Send a reply to a user process.
		 main.c:reply:409:source=0 type=-5 m1i1=0 m1i2=0 m1i3=0 m1p1=(nil) m1p2=(nil) m1p3=(nil) 		

		ATENCION: Debe haber un conflicto de nombres de estructuras de datos MINIX-LINUX	

---------------------------------------------------------------------------------------------------------------
20170620:
		INET: Modifique varios tipos que evitaban coincidencias con LINUX
		Cambie el script para que el mnx_ifconfig -I /dev/ip
	
mnx_ifconfig -h 172.16.1.1

	generic/ip_ioctl.c:ip_ioctl:192:req=NWIOSIPCONF
 PORQUE AQUI PIDE CON ipopt  
	 generic/icmp.c:icmp_getdata:180:port=0 offset=0 count=16 for_ioctl=1 sizeof(*ipopt)=56
	 generic/icmp.c:icmp_getdata:183:icp_flags=1 icp_state=1 icp_ipport=0 icp_ipfd=0
	 buf.c:bf_memreq:236:size=56
	 buf.c:bf_memreq:322:tail->acc_length=56
	 generic/icmp.c:icmp_getdata:227:nwio_flags=15000233 nwio_tos=0 nwio_ttl=0 nwio_df=0 nwio_proto=1
	 generic/icmp.c:icmp_getdata:228:data->acc_length=56
	 buf.c:bf_packIffLess:471:min_len=16
 Y AQUI COMPARA CON ipconf 
	 generic/ip_ioctl.c:ip_ioctl:196:req=NWIOSIPCONF acc_length=56 sizeof(*ipconf)=16
 

---------------------------------------------------------------------------------------------------------------
20170903: 
		Modifique 
			ETH.C: para hacerlo andar con demonize
			MINIX.SH: Errores en la cantidad de parametros de demonize
			DEMONIZE.C: adecuacion de parametros
		
		NO PUDE VERIFICAR SI LOS PROBLEMAS DE ETH CONTINUAN
		´
		ERROR EN INET
			test_inet.c:main:75:OPEN source=0 type=1030 m2i1=1 m2i2=1 m2i3=2 m2l1=0 m2l2=0 m2p1=(nil)
			inet.c:main:144:source=10 type=1030 m2i1=1 m2i2=1 m2i3=2 m2l1=0 m2l2=0 m2p1=(nil)
			ERROR: inet.c:bad_compare:664: rcode=32768
		EN STDERR
			compare (1030) mq->mq_mess.m_type == DL_TASK_REPLY (2325) failed		
		
---------------------------------------------------------------------------------------------------------------
20170910:
		Volque el ultimo codigo de diego de FATFS a mi codigo
		modifique y probe fatfs.sh para contemplar el nuevo formato de demonize
		ademas le agregue el arranque de m3nweb que es el webserver que usa un M3IPC filesystem, en este caso FATFS
		Luego desde NODE1 hice:
		
			root@node1:/tmp# time wget http://node0:8080/100Mb.txt 
			--2017-09-10 07:27:35--  http://node0:8080/100Mb.txt
			Resolving node0... 192.168.1.100
			Connecting to node0|192.168.1.100|:8080... connected.
			HTTP request sent, awaiting response... 200 OK
			Length: 104857600 (100M) [text/txt]
			Saving to: â100Mb.txtâ

			100%[=========================================================>] 104,857,600 11.3M/s   in 8.1s    

			2017-09-10 07:27:43 (12.3 MB/s) - â100Mb.txtâsaved [104857600/104857600]

			real    0m8.159s
			user    0m0.009s
			sys     0m1.347s

		PARA VERIFICAR SI LOS ARCHIVOS SON IGUALES HACER sha1sum <archivo>

---------------------------------------------------------------------------------------------------------------
20170920:		
		Se hicieron los benchmarks en LAB5 de RAW ETHERNET PROXY pero sin considerar ACKS
		
---------------------------------------------------------------------------------------------------------------
20170921:		
		Se incorporo al codigo principal el codigo de RAW PROXY CON ACKs
		
ERROR: No hay actualizacion de offset en el buffer de envio.

hay un problema en ps_start_serving

El cual setea el payload 
	p_payload= &td_send_sframe_ptr->fmt.pay; !!!!
	
en realidad habria que pedir memoria  
Seguramente lo mismo pasa en recepcion.

	
---------------------------------------------------------------------------------------------------------------
20170922:	Se agregaron buffer p_header y p_payload via posix_malign
		
		SIN ACKSs parece que funciono OK
		
---------------------------------------------------------------------------------------------------------------
20170923:

 raw_proxy.c:ps_send_remote:669:HEADER sframe_len=104 sent_bytes=104
 raw_proxy.c:ps_send_remote:688:rmt_ack_seq=0 lcl_snd_seq=1
 raw_proxy.c:ps_send_remote:690:read before ack_read=0 
 raw_proxy.c:pr_get_message:375:rcvd_bytes=104
 raw_proxy.c:DEBUG_hdrcmd:227:pr_get_message dst=0:C:29:FF:E3:2A src=0:C:29:98:8:32 proto=FD
 raw_proxy.c:DEBUG_hdrcmd:229:pr_get_message cmd=0x20 vmid=0 src=0 dst=0 snode=0 dnode=0 rcode=0 len=0
 raw_proxy.c:DEBUG_hdrcmd:230:pr_get_message c_flags=0x0 c_snd_seq=1 c_ack_seq=1 <<<<<<<<<<<<<<<<<<<<<<<<<<<
 raw_proxy.c:pr_get_message:382:rmt_ack_seq=0 c_ack_seq=1
 raw_proxy.c:pr_get_message:385:rmt_ack_seq=1 lcl_ack_seq=0
 raw_proxy.c:pr_get_message:387:write after rmt_ack_seq=1
 raw_proxy.c:pr_get_message:422:CMD_FRAME_ACK ignored c_snd_seq=1 c_ack_seq=1
 raw_proxy.c:pr_get_message:365:Waiting on Ethernet sock_fd=4
 raw_proxy.c:ps_send_remote:692:read after  ack_read=1 rmt_ack_seq=0 
 raw_proxy.c:ps_send_remote:705:rmt_ack_seq=1 lcl_snd_seq=1
 raw_proxy.c:ps_send_remote:830:restart
 raw_proxy.c:ps_start_serving:904:SPROXY: smsg_ok=1 smsg_fail=0 
 raw_proxy.c:ps_start_serving:852:SPROXY 2693: Waiting a message
 
 raw_proxy.c:pr_get_message:375:rcvd_bytes=104
 raw_proxy.c:DEBUG_hdrcmd:227:pr_get_message dst=0:C:29:FF:E3:2A src=0:C:29:98:8:32 proto=FD
 raw_proxy.c:DEBUG_hdrcmd:229:pr_get_message cmd=0x3 vmid=0 src=1 dst=0 snode=1 dnode=0 rcode=0 len=0
 raw_proxy.c:DEBUG_hdrcmd:230:pr_get_message c_flags=0x10 c_snd_seq=1 c_ack_seq=0 <<<<<<<<<<<<<<<<<<<<<<<<<<
 raw_proxy.c:pr_get_message:382:rmt_ack_seq=1 c_ack_seq=0
 raw_proxy.c:pr_get_message:393:resent frame lcl_ack_seq=1 c_snd_seq=1
 raw_proxy.c:pr_get_message:365:Waiting on Ethernet sock_fd=4
 raw_proxy.c:pr_get_message:369:rcvd_bytes=-1
 raw_proxy.c:pr_get_message:365:Waiting on Ethernet sock_fd=4
 raw_proxy.c:pr_get_message:369:rcvd_bytes=-1
 raw_proxy.c:pr_get_message:365:Waiting on Ethernet sock_fd=4
 
 EL PROBLEMA ESTA EN QUE TENEMOS VARIABLES GLOBALES EN AMBOS PROXIES
	lcl_snd_seq, lcl_ack_seq, rmt_ack_seq
	
	rmt_ack_seq se pasa por desde RECEIVER->SENDER por PIPE
	lcl_snd_seq deberia setearlos solo SENDER
	lcl_ack_seq deberia setearlo solo RECEIVER 
		
	PROBLEMA:
		Como hago para cuando llega un FRAME al RECEIVER , en actualizar el lcl_ack_seq del SENDER ??

	OPCION 1: Utilizar SHMEM y semaforos
	OPCION 2: Utilizar THREADS y mutexes 
---------------------------------------------------------------------------------------------------------------
20170924:
			Se selecciono la opcion 2 con THREADS 
			
	raw_proxy.c:DEBUG_hdrcmd:276:pr_get_message c_flags=0x0 c_snd_seq=1 c_ack_seq=0
	raw_proxy.c:DEBUG_hdrcmd:276:pr_get_message c_flags=0x0 c_snd_seq=2 c_ack_seq=0
	raw_proxy.c:DEBUG_hdrcmd:276:pr_get_message c_flags=0x0 c_snd_seq=3 c_ack_seq=0
	raw_proxy.c:DEBUG_hdrcmd:276:pr_get_message c_flags=0x0 c_snd_seq=4 c_ack_seq=0
	raw_proxy.c:DEBUG_hdrcmd:276:pr_get_message c_flags=0x0 c_snd_seq=5 c_ack_seq=0


---------------------------------------------------------------------------------------------------------------
20170926:
		FUNCIONO EL RAW ETHERENT PROXY CON ACKNOWLEDGE
			
		Tiene una pequeña inistabilidad al arrancar.  Pero si arrancan los tests no hay problemas
		

			
	PROBLEMA:
		Puede estar en el siguiente hecho
			Supongamos que arrancamos SOLO NODO0, periodicamente se vence TIMEOUT y envia HELLO
			pero todos los HELLO no tienen ACK asi que tambien se intenta reenviar con la misma secuencia 
			
	SOLUCION: 
		Un HELLO no deberia solicitar ACK 
	
---------------------------------------------------------------------------------------------------------------
20170926:	
		
		Optimizacion
		Si el RECEIVER recibe un frame que solicita ACK ver si se puede llevar como piggybacking
		de un frame enviado por el SENDER
		para ello se hace el siguiente truco:_ 
			el sender tiene tomado el mutex la mayoria del tiempo, excepto cuando hace get2rmt
			si el receiver quiere enviar un frame de ACK y el mutex esta tomado por el sender
			entonces el receiver desiste de enviar x q sabe que el sender esta por enviar un frame
			
PROBLEMA:  Es que el receiver tiene q actualizar lcl_ack_seq y para ello necesita el mutex
			cuando adquiere el mutex quizas es porque ya el frame se envio.
		
NODE0
		
 raw_proxy.c:ps_start_serving:849:rcode=0
 raw_proxy.c:ps_start_serving:867:MTX_LOCK main_mtx 
 raw_proxy.c:ps_start_serving:869:SPROXY: cmd=0x5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=2048
 raw_proxy.c:ps_start_serving:883:SPROXY: src=2 dst=3 rqtr=2 saddr=0xb73fd000 daddr=0xb74c1000 bytes=2048 
 raw_proxy.c:ps_start_serving:894:SPROXY: cmd=0x5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=2048
 raw_proxy.c:ps_send_message:657:SPROXY:cmd=0x5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=2048
 raw_proxy.c:ps_send_message:660:mtype=3 total_bytes=2136
 raw_proxy.c:check_msgtype:361:msgtype=3 
 raw_proxy.c:set_frame_hdr:115:
 raw_proxy.c:send_frame:345:SEND lcl_snd_seq=233 lcl_ack_seq=163 sframe_len=104
 raw_proxy.c:DEBUG_hdrcmd:285:send_frame dst=0:C:29:98:8:32 src=0:C:29:FF:E3:2A proto=FD
 raw_proxy.c:DEBUG_hdrcmd:287:send_frame cmd=0x5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=2048
 raw_proxy.c:DEBUG_hdrcmd:288:send_frame c_flags=0x0 c_snd_seq=233 c_ack_seq=163
 raw_proxy.c:ps_send_message:686:HEADER sframe_len=104 sent_bytes=104
 raw_proxy.c:ps_send_message:743:remain=2048
 raw_proxy.c:ps_send_message:753:remain=2048 lcl_snd_seq=233 lcl_ack_seq=163 send_off=0
 raw_proxy.c:ps_send_message:760:frame_count=1
 raw_proxy.c:set_frame_hdr:115:
 raw_proxy.c:send_frame:345:SEND lcl_snd_seq=234 lcl_ack_seq=163 sframe_len=1514
 raw_proxy.c:DEBUG_hdrcmd:285:send_frame dst=0:C:29:98:8:32 src=0:C:29:FF:E3:2A proto=FD
 raw_proxy.c:DEBUG_hdrcmd:287:send_frame cmd=0x5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1410 CMD_COPYIN_DATA
 raw_proxy.c:DEBUG_hdrcmd:288:send_frame c_flags=0x11 c_snd_seq=234 c_ack_seq=163
 raw_proxy.c:ps_send_message:771:PAYLOAD sframe_len=1514 sent_bytes=1514
 raw_proxy.c:DEBUG_hdrcmd:285:ps_send_message dst=0:C:29:98:8:32 src=0:C:29:FF:E3:2A proto=FD
	 raw_proxy.c:pr_rcv_message:435:rcvd_bytes=104
	 raw_proxy.c:DEBUG_hdrcmd:285:pr_rcv_message dst=0:C:29:FF:E3:2A src=0:C:29:98:8:32 proto=FD
	 raw_proxy.c:DEBUG_hdrcmd:287:pr_rcv_message cmd=0x20 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1410 CMD_COPYIN_RQST <<< PORQUE COPIA 1410???
	 raw_proxy.c:DEBUG_hdrcmd:288:pr_rcv_message c_flags=0x0 c_snd_seq=164 c_ack_seq=234 <<<<<< RECIBE c_snd_seq=164 y deberia enviarlo en un frame de retorno
	 raw_proxy.c:pr_rcv_message:443:MTX_TRYLOCK main_mtx                                 <<<<<< trata de obtener el LOCK pero lo tiene SENDER 
	 raw_proxy.c:pr_rcv_message:444:rcode=16											<<<<<<< da TIMEOUT , no deberia ser necesario reenviar otro frame
	 raw_proxy.c:pr_rcv_message:473:MTX_LOCK main_mtx 									<<<<<<< PERO, hasta que obtiene el MUTEX el frame se envio con ACK viejo
 raw_proxy.c:DEBUG_hdrcmd:287:ps_send_message cmd=0x5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1410
 raw_proxy.c:DEBUG_hdrcmd:288:ps_send_message c_flags=0x11 c_snd_seq=234 c_ack_seq=163  <<<<<<<< LUEGO, el ACK correcto 234 no se envia al otro NODO !!!!        
 raw_proxy.c:ps_send_message:788:rmt_ack_seq=232 lcl_snd_seq=234
 raw_proxy.c:ps_send_message:789:COND_WAIT_TO rcode sdesc.td_cond main_mtx				<<<<<<<< EL SENDER ESPERA SU ACK 
 raw_proxy.c:ps_send_message:789:COND_WAIT_TO before tv_sec=1506832080
	 raw_proxy.c:pr_rcv_message:478:correct sequence lcl_ack_seq=164 
	 raw_proxy.c:pr_rcv_message:491:MTX_UNLOCK main_mtx 
	 raw_proxy.c:pr_rcv_message:497:CMD_FRAME_ACK ignored c_snd_seq=164 c_ack_seq=234  
	 raw_proxy.c:pr_rcv_message:425:Waiting on Ethernet raw_fd=4
 raw_proxy.c:ps_send_message:789:COND_WAIT_TO after tv_sec=1506832090
  raw_proxy.c:ps_start_serving:901:SPROXY: smsg_ok=132 smsg_fail=1 
 raw_proxy.c:ps_start_serving:845:SPROXY: Waiting a message
 
NODE 1
  raw_proxy.c:pr_rcv_message:425:Waiting on Ethernet raw_fd=4
 raw_proxy.c:pr_rcv_message:435:rcvd_bytes=1514
 raw_proxy.c:DEBUG_hdrcmd:285:pr_rcv_message dst=0:C:29:98:8:32 src=0:C:29:FF:E3:2A proto=FD
 raw_proxy.c:DEBUG_hdrcmd:287:pr_rcv_message cmd=0x5 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1410
 raw_proxy.c:DEBUG_hdrcmd:288:pr_rcv_message c_flags=0x11 c_snd_seq=234 c_ack_seq=163
 raw_proxy.c:pr_rcv_message:443:MTX_TRYLOCK main_mtx
 raw_proxy.c:pr_rcv_message:444:rcode=0
 raw_proxy.c:pr_rcv_message:464:rmt_ack_seq=163 c_ack_seq=163
 raw_proxy.c:pr_rcv_message:478:correct sequence lcl_ack_seq=234
 raw_proxy.c:pr_rcv_message:480: send ACK 
 raw_proxy.c:set_frame_hdr:115:
 raw_proxy.c:send_frame:345:SEND lcl_snd_seq=164 lcl_ack_seq=234 sframe_len=104
 raw_proxy.c:DEBUG_hdrcmd:285:send_frame dst=0:C:29:FF:E3:2A src=0:C:29:98:8:32 proto=FD
 raw_proxy.c:DEBUG_hdrcmd:287:send_frame cmd=0x20 vmid=0 src=2 dst=3 snode=0 dnode=1 rcode=0 len=1410
 raw_proxy.c:DEBUG_hdrcmd:288:send_frame c_flags=0x0 c_snd_seq=164 c_ack_seq=234
 raw_proxy.c:pr_rcv_message:491:MTX_UNLOCK main_mtx 
 raw_proxy.c:pr_rcv_message:506:c_len=1410
 raw_proxy.c:pr_rcv_message:519:total_bytes=1410
 
---------------------------------------------------------------------------------------------------------------
20171001:		RAW PROXY FUNCIONA EN VMS pero no funciona en maquinas reales (de dpto)
 
 
 
Como funciona el protocolo:
		1)	Solo los comandos (campo cmd) que tiene el bit CMD_ACKNOWLEDGE prendido van a solicitar
			ACKNOWLEDGE a nivel FRAME (flag RAW_NEEDACK).
		2)  Aquellos comandos que transportan datos (payload) solicitaran ACKNOWLEDGE a nivel FRAME
			a una tasa RAW_ACK_RATE (1 => todos los frames, 2 => la mitad de los frames, frames pares)
		3)  El último frame de payload trasporta el flag RAW_EOB
	
					
---------------------------------------------------------------------------------------------------------------
20171001:		RAW PROXY FUNCIONA EN VMS pero no funciona en maquinas reales (de dpto)

En maquina de PAP (NODE0) da el error, en maquina de INES (NODE1) no da

[  442.466641] Replacing mol_vm_init (<c104ffe0> by <f0a39440>)
[  442.467189] Replacing mol_vm_end (<c1050090> by <f0a22720>)
[  442.467189] Replacing mol_vm_dump (<c104fff0> by <f0a095c0>)
[  442.467189] Replacing mol_getvminfo (<c1050010> by <f0a15ba0>)
[  443.923679] DEBUG 3870:mm_drvs_init:653: d_nr_vms=32 d_nr_nodes=32 d_nr_procs=221 d_nr_tasks=35 d_nr_sysprocs=64 
[  444.656643] test_vm_init: page allocation failure. order:5, mode:0x40d0 
[  444.656709] Pid: 3871, comm: test_vm_init Not tainted 2.6.32 #881
[  444.656959]  [<f0a396f5>] ? mm_vm_init+0x2b5/0xf14 [mol_replace]
[  444.657094]  [<f0a39440>] ? mm_vm_init+0x0/0xf14 [mol_replace]
[  444.662172] ERROR: 3871:mm_vm_init:1042: rcode=-344 <<< EMOLALLOCMEM
[  601.812896] ERROR: 3944:mm_vm_end:1862: rcode=-303
			
RESUELTO!! el problema es que test_vm_init usa kalloc en lugar de malloc y falla si no hay memoria real suficiente.
		cambie bootcdwrite para que fije la memoria RAM en 960M
		
---------------------------------------------------------------------------------------------------------------
20180509
			Se modifico current->pid por task_pid_nr(current) para que soporte los PIDS reales en lugar de 
			los virtuales que ofrece un Container.
		
---------------------------------------------------------------------------------------------------------------
20180602
			La version de kernel 2.6.32, si bien parece que trae soporte para CGROUPs no trae el directorio
				/sys/lib/cgroup
			Baje e instale la version  2.6.39 (la mas alta de 2.6) y si lo trae.
			Compile SIN MODIFICACIONES de M3-IPC, solo para probar el funcionamiento de CGROUPS.
			
			Siguiendo los pasos de varios links de internet 
				root@node0:/sys/fs/cgroup# mkdir cpuset
				root@node0:/sys/fs/cgroup# mount -t cgroup cpuset -o cpuset /sys/fs/cgroup/cpuset/
				mount: cpuset is already mounted or /sys/fs/cgroup/cpuset busy
				root@node0:/sys/fs/cgroup# cd cpuset
				root@node0:/sys/fs/cgroup/cpuset# mkdir pap
				root@node0:/sys/fs/cgroup/cpuset# cd pap
				root@node0:/sys/fs/cgroup/cpuset/pap# /bin/echo $$ > tasks
				/bin/echo: write error: No space left on device
			
			
ULTIMO_LOG
---------------------------------------------------------------------------------------------------------------

TODO:	Probar en la version 2.6.39 
			- funcionamiento de CGROUPS
			- funcionamiento de NAMESPACES
			- Hacer diff entre las versiones de kernel 2.6.32 modificada y 2.6.39
			- Crear kernel 2.6.39 con modulo M3-IPC
			- Probar todo

			
			
ESCENARIOS DE PRUEBA DE FATFS
	CON FUSE
		COPY(NODE0)->FUSE->FATFS(NODE0)->IMAGE
		COPY(NODE0)->FUSE->FATFS(NODE0)->RDISK(NODE0)
		COPY(NODE0)->FUSE->FATFS(NODE0)->RDISK(NODE1)
	CON M3IPC 	
		M3COPY(NODE0)->FATFS(NODE0)->IMAGE
		M3COPY(NODE0)->FATFS(NODE0)->RDISK(NODE0)
		M3COPY(NODE0)->FATFS(NODE0)->RDISK(NODE1)

Cuando se ejecuta loop_vcopy_nodeX.sh con varios forks me da error
 raw_proxy.c:ps_send_message:781:MTX_UNLOCK main_mtx 
 raw_proxy.c:ps_start_serving:888:SPROXY: smsg_ok=1576 smsg_fail=3 
 raw_proxy.c:ps_start_serving:836:SPROXY: Waiting a message
 raw_proxy.c:ps_start_serving:839:rcode=-310 EMOLNOTBIND
 raw_proxy.c:ps_start_serving:851:ERROR  mnx_get2rmt -310
 raw_proxy.c:ps_start_serving:836:SPROXY: Waiting a message
 raw_proxy.c:ps_start_serving:839:rcode=-319 EMOLNOVMNODE
 raw_proxy.c:ps_start_serving:851:ERROR  mnx_get2rmt -319
 raw_proxy.c:ps_start_serving:836:SPROXY: Waiting a message
		
TODO:
		Habilitar esta linea RAW_NEEDACK
				sdesc.td_sframe->fmt.cmd.c_flags	|= (RAW_EOB); //| RAW_NEEDACK); 

		#define BIT_ACKNOWLEDGE		13 // 0010 0000 0000 0000 <<< ojo este comentario no estaba!!
		#define CMD_ACKNOWLEDGE 	(1<<BIT_ACKNOWLEDGE)		

		QUITAR
			CMD_BATCHED_CMD,	/* Payload are batched commands - ONLY USED BY PROXIES, not by kernel */
		Debe quedar 
		CMD_CHECKBIND,		/* Request remote RPROXY if a REMOTE process is binded */
		CMD_LAST_CMD		/* THIS MUST BE THE LAST COMMAND */		
				
TODO:	proxy Batched commands
		para ellos se requiere:
			1) modificar el modulo para crear una nueva API
				mnx_test2rmt(); 
				que retorne el comando sin borrarlo de la cola 
				Se podría hacer con timeout=TIMEOUT_NOWAIT;
				es decir seria:
					mnx_get2rmt_T(header, payload, TIMEOUT_NOWAIT)	
				El problema es que se SOBREENTIENDE que si hay un comando, ese comando	
				sea cual sea lo remueve de la cola, incluido los de copias.
				
			2) modificar el proxy. 
				hace mnx_get2rmt_T(header, payload, TIMEOUT_NOWAIT) y 
				si es un comando con payload DATOS, lo copia a un buffer
				y finaliza el batcheo de comandos.
				Cuando se requiere buscar un nuevo comando, primero se fija si 
				existe comando guardado en el buffer antes de invocar a mnx_get2rmt
		
		SENDER 
		while(TRUE){
			ret = mnx_get2rmt( header, payload);
			
			copy(send->fmt.cmd.c_cmd, header);
			nr_bat = 0; // numero de comandos batcheados 
			cmd_ptr = send->fmt.pl.bat[nr_bat]
			while ( cmd != DATA || nr_bat < MAX_BATCHED ){
				ret  = mnx_get2rmt_T(header, payload, TIMEOUT_NOWAIT);
				if( ret == TIMEOUT) // no hay nada encolado
					break;
					
			}
			if( nr_bat > 0){
				send->fmt.cmd.c_flags |= BIT_BATCHED;
				send->fmt.cmd.c_len = (&send->fmt.pl.bat[nr_bat] * &send->fmt.pl.bat[0]);
			}
			
		}
			
			
		Cualquier comando que no transporte datos es decir diferente de:
			CMD_COPYIN_DATA
			CMD_COPYOUT_DATA
			
	#define PAYLOAD_LEN  (ETH_FRAME_LEN - sizeof(cmd_t) - sizeof(eth_hdr_t))
	#define MAX_BATCHED	  ( (PAYLOAD_LEN-sizeof(int))%sizeof(cmd_t))
	typedef union eth_frame_u{ 
		u8_t raw[ETH_FRAME_LEN+1];
		struct {
			eth_hdr_t	hdr;
			cmd_t		cmd;
			union {
				u8_t		pay[PAYLOAD_LEN];
				cmd_t		bat[MAX_BATCHED];
			}pl;
		}fmt;
	} eth_frame_t;

	TODO:		Queda por terminar TTY, NET y ETH.
	TODO: 		M3-IPC. Probar receive con signal al thread.
		
	TODO:			INET.C
		Registra el dispositivo. Esto esta ANULADO. Hay que hacer algo?
		if (svrctl(FSSIGNON, (void *) &device) == -1) {
			printf("inet: error %d on registering ethernet devices\n",
				errno);
			pause();
		}	

	
	VERIFICA ERROR:  Porque tengo que levantar NODE1 (systask1) para que funcionen los scripts en NODE0 
				sin que intervengan procesos de NODE1
				
	
	PROBLEMA GENERICO:
		No funciona bien todo los proxies con THREADS 
	
	TODO:	Hacer proxy por PROCESOS TCP con compresion 
	
	TODO:	Hay que asignarle direccion IP a TAP para que funcione.
					
	TODO: 
			ERROR EN PROXY TCP CON THREADS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1
				
	TODO:
			Crear un Test simil ifconfig pero usando TEST->FS->INET								
			
	TODO:			Web information Server (WIS) <<<< NOOOOOOO - hacerlo con WEBMIN modificado
					
	TODO:			DATA Server (segun MHYPER)	
			

TODO:			La libreria MOLLIB , incluso el FS se basan en archivos INCLUDE de LINUX en lugar de los de MINIX.
				Los de minix se encuentran en el directorio original MINIX  /src/include
				MODIFICAR para que se tomen esos archivos y no los de LINUX
			
TODO:			Las llamadas molxxxxxx se hace usando sendrec(). Debería usarse  mnx_sendrec_T para contemplar timeout		
				rcode = mnx_sendrec_T(dst_ep, m_ptr, TIMEOUT_MOLCALL )
			
---------------------------------------------------------------------------------------------------------------				
						
	
	
TODO:			quizas lo mejor es adaptar las primitivas para que puedan operar tanto de kernel como en 
				modo usuario. Para ellos lo unico que aparentemente hay que solucionar es el problema
				de las copias de datos. 
				Para ello hay que considerar la flag miscelanea MIS_KTHREAD
				
TODO:			OPTIMIZAR EL FUNCIONAMIENTO DE TTY para que considere los tipos de modo COOKED y RAW
				Y para que considere todo el envio de caracteres.					
		
TODO:			Web information Server (WIS)
				1- utilizando sockets TCP de LINUX
				2- Hacer una libreria M3IPC equivalente a sockets 
				3- Utilizando M3IPC y haciendo un proxy M3IPC<->HTTP
				
TODO:			Modificar WEB CLIENT/BROWSER 	netrik-1.16.1 (ESTA COMO PAQUETE DEBIAN TAMBIEN)
				1- utilizando sockets TCP de LINUX 
				2- Probar con wget y con archivos segun el hacking-load.txt 
				3- Incorporar M3IPC como mecanismo 
			
---------------------------------------------------------------------------------------------------------------

TODO:			Proxima prueba de INET+FS
				Crear los i-nodos correspondientes 
				segun inet_config.c/h
				
				{	"/dev/eth",	0600,	ETH_DEV_MINOR	},
				{	"/dev/psip",0600,	PSIP_DEV_MINOR	},
				{	"/dev/ip",	0600,	IP_DEV_MINOR	},
				{	"/dev/tcp",	0666,	TCP_DEV_MINOR	},
				{	"/dev/udp",	0666,	UDP_DEV_MINOR	},
				
				1) Crear el directorio /dev
				2) Hacer MKNOD de los dispositivos.
				3) Listar directorio /dev
		El MAJOR number es entonces INET/ETH
		El MINOR number es:
		/* Offsets of the minor device numbers within a group per interface. */
				#define ETH_DEV_MINOR	0
				#define PSIP_DEV_MINOR	0
				#define IP_DEV_MINOR	1
				#define TCP_DEV_MINOR	2
				#define UDP_DEV_MINOR	3

TODO:			Hacer la libreria 				
				
TODO:			Ver optimizaciones en el uso de buffers para ver si se pueden evitar MEMCOPY
	
---------------------------------------------------------------------------------------------------------------
TODO:	Normalizar el uso de:
		mol_time_t o mnx_time_t ????
 
TODO: 			
		Este thread solo deberia encolar los paquetes recibidos, por lo que no necesitaria bindeardse a M3-IPC
		El main thread deberia sincronizarse con este thread.
		
---------------------------------------------------------------------------------------------------------------		

	SOLUCION:	Agregando a la tarea LINUX una waiqueue adicional
				
include/linux/shed.h				
====================
/* MOL-IPC proc_t pointer definded as void */
	void 		*task_proc;
	wait_queue_head_t task_wqh;			/* LINUX  wait queue head	<<<<<<<<<<<<<<<<<<<<<	*/

#if LOCK_TASK_TYPE == USE_TASK_RWLOCK
  rwlock_t   	task_rwlock;	/* LINUX spinlock to protect this task 	*/	
#elif LOCK_TASK_TYPE == USE_TASK_MUTEX
   struct mutex task_mutex;	/* LINUX mutex to protect this task	*/	
#elif LOCK_TASK_TYPE == USE_TASK_SPINLOCK
	spinlock_t task_spinlock;	/* LINUX spinlock to protect this task	*/	
#elif LOCK_TASK_TYPE == USE_TASK_RWSEM
	struct rw_semaphore task_rwsem;	/* LINUX RW semaphore to protect this task	*/	
#elif LOCK_TASK_TYPE == USE_TASK_RCU
	spinlock_t task_spinlock;	/* LINUX spinlock to protect this task	*/
#else
#warning LOCK_TASK_TYPE = ???	
#endif
		
	
asmlinkage long mm_wait4bind(int oper, int other_ep, long timeout_ms)
{

		LOCK_TASK(current);
		if (current->task_proc != NULL) {
			UNLOCK_TASK(current);
			return(OK);
		}
		init_waitqueue_head(&current->task_wqh);		/* Initialize the wait queue 	*/
		UNLOCK_TASK(current);

		/* here cames only WAIT_BIND */
		MOLDEBUG(INTERNAL,"Self process bind waiting\n");
		if( timeout_ms < 0) {
			ret = wait_event(current->task_wqh, (current->task_proc != NULL));
		} else {
			ret = wait_event_timeout(current->task_wqh, (current->task_proc != NULL), msecs_to_jiffies(timeout));
		if (ret == 0)
			ERROR_RETURN(EMOLTIMEDOUT);		/* timeout */
			
		LOCK_TASK(current);
		if (current->task_proc != NULL) {
			UNLOCK_TASK(current);
			return(OK);
		}
		UNLOCK_TASK(current)
}		
		

	EL BIND hace 
	/* Wake up the waiting process on mnx_wait4bind() */
	if( oper == LCL_BIND || oper == REPLICA_BIND || oper == BKUP_BIND){
		if( current != task_ptr ){
			/* Wakes up it as if has in wait4bind state  */
			if( waitqueue_active (&task_ptr->task_wqh);
				wake_up_interruptible(&task_ptr->task_wqh); 
		}
	}


201708023: SSH REMOTO 
	
Https://www.cyberciti.biz/faq/noninteractive-shell-script-ssh-password-provider/

bajen ese software (sshpass) y permite ejecutar comandos remotos con ssh insertando la passwd
	apt-get install sshpass

sshpass [-ffilename|-dnum|-ppassword|-e] [options] command arguments
manual en: https://linux.die.net/man/1/sshpass
sshpass(1) - Linux man page
linux.die.net
sshpass is a utility designed for running ssh using the mode referred to as "keyboard-interactive" password authentication, but in non-interactive mode.

ejemplo utilizando variable de entorno :
# SSHPASS='root'
# sshpass -e ssh node1@root  cat /proc/drvs/info 

ejemplo en la misma linea de comando:
# sshpass -p 'root'  ssh node1@root  cat /proc/drvs/info 	


201708023: WEBMIN

Generar archivos de configuracion en el nodo webmin 
	- DRVS
	- VM
	- NODES
	- PROXIES

Podemos definir los archivos de configuracion en 2 tipos:
		1- DRVS en el cual se definen los NODOS y PROXIES que los uniran
		2- VMs en la cual se definen los NODOS en la que se ejecutaran y 
				la secuencia de comandos a ejecutar 
	
Una VM no solo esta compuesta por sus parametros de configuracion sino ademas 
por su script de arranque y por el nombre del NODO donde se arranca inicialmente.

# ARCHIVO DE CONFIGURACION DE DRVS 
drvs MY_DRVS {
	d_nr_vms		32;
	d_nr_nodes  	32;
	d_nr_procs  	968;
	d_nr_tasks  	64;
	d_nr_sysprocs 	128;
	d_max_copybuf 	65536;
	d_max_copylen 	1048576;
	d_dbglvl		0;
};

# listado de nodos en los que se ejecutara el DRVS 
drvs_node node0 { /* nombre segun /etc/hosts  */	
	d_nodeid 	 0;
	d_script	"/home/webmin/drvs/scripts/drvs.sh" 	// este script levantaria el drvs, spread y los proxies  
}

drvs_node node1 { /* nombre segun /etc/hosts  */	
	d_nodeid 	 1;
	d_script	"/home/webmin/drvs/scripts/drvs.sh" 
}


# ARCHIVO DE CONFIGURACION DE VMS 

vm minix { /*  nombre de fantasia */
	vm_vmid			 0;
	vm_nr_procs		 192;
	vm_nr_tasks		 64;	
	vm_nr_sysprocs	 96;
	vm_nr_nodes		 32;
	vm_warn2proc;	 0;	/* which process to inform when a process exit  */
	vm_warnmsg		 1;	/* with this message type						*/
};

vm_node filesvr { /*  nombre de fantasia */
	vm_nodeid 	 0;
	vm_nodename  node0;	/* nombre segun /etc/hosts  */		
	vm_script	"/home/webmin/drvs/vm/scripts/minix.sh" 	
}

vm_node disksrv { /*  nombre de fantasia */
	vm_nodeid 	 1;
	vm_nodename  node1;	/* nombre segun /etc/hosts  */	
	vm_script	"/home/webmin/drvs/vm/scripts/minix.sh" 	
}

================================================================================================================
============================================
====================================================================
================================================================================================================

REPASAR FUNCIONAMIENTO DE MOL PARA INTERCEPCION DE READ/WRITE
	
	
PREGUNTA: Como hacer para crear un proceso de USUARIO REMOTO ??
	ESCENARIO:
			NODO0:					NODO1:
			KERNEL0					KERNEL1
			SYSTASK0				SYSTASK1
			PM0						-----
			INIT0 ---- replicados -- INIT1
			USER0					USER1

OBJETIVO:	Arrancar un proceso SHELL1 en NODO1 
			Hay un proceso INIT en cada NODO como REPLICATED, es decir usando el mismo endpoint.
			cuando un proceso de usuario USER0 quiere hacer rexec() de USER1 
			USER0->PM0: rexec()
			PM0->SYSTASK1: sys_exec()
			SYSTASK1: fork(), exec() USER1
			
OBJETIVO2:	Que la salida STDOUT de un proceso USER1 se muestre en NODE0
			Para ello se necesita el filesystem o hacer lo siguiente en forma temporal
			Redireccionar STDOUT a una tuberia PIPE0
			
			Crear una tarea TTYclient en NODE1 
				while(TRUE){
					bytes = read(pipe, buffer);
					msg.m_type = TTY_WRITE
					msg.buf_addr = buffer;
					msg.bytes = bytes;				
					sendrec(TTY_server, msg);
				}
			
			Crear una tarea TTYserver en NODE0
				while(TRUE){
					receive(TTYclient, msg)
					vcopy(TTYclient, msg.buf_addr, TTYserver, buffer, msg.bytes);
					bytes = write(ttyXX, buffer);
					msg.m_type = (TTY_WRITE | TTY_ACKNOWLEDGE)
					msg.rcode = bytes;				
					send(TTY_client, msg);
				}

				
		Debería haber un archivo que indique el mapeo entre una TTY virtual y la real.
		TTYxx		NODEyy,TTYzz	
					
		El  TTY task funcione similar al PROXY. 
		
		USER1-->PIPE-->FS1TTY-->=========>	
		
					
TODO:
		+ Como el PM tambien lee info de los procesos en el kernel, debería tener acceso a ellos en forma directa

		
TODO:
		+ Muchas system calls pueden sintetizarse en una, por eljemplo init_VM, end_VM, add_node, del_node
	

Web servers - File Server - Disk Server
Se requiere un proxy IP que transfiera los paquetes entre la LINUX_IP con MINIX_IP
Se podria migrar el INET Server y crear un Pseudo ethernet driver que maneje RAW Sockets
VER
http://git.savannah.gnu.org/cgit/lwip/lwip-contrib.git/tree/ports/unix/proj/unixsim
http://git.savannah.gnu.org/cgit/lwip/lwip-contrib.git/tree/ports/unix/proj/minimal/README

https://mirage.io/
ATENCION LWIP ya esta en MINIX3 
LwIP
An alternative to the inet server is represented by the lwip server. 
This server is based on the lwip stack. The stack is separated in the liblwip. 
+he server itself is an implementation of the VFS-INET protocol, the sockets.
OTRO WEB SERVER https://github.com/ankushagarwal/nweb
OTRO STACK https://github.com/cesanta/mongoose
Se ejecutaría el equivalente a una VM dentro del espacio de direcciones del proxy, pero en lugar de ser una VM es un único programa ejecutable.
Podría ser por ejemplo un server WEB(http://lwip.wikia.com/wiki/Sample_Web_Server), pero para habria que construir de abajo hacia arriba.
ontrib-1.4.1.zip\contrib-1.4.1\apps\httpserver_raw

- Un disco ram preseteado con un FS que contenga los archivos web a presentar.
- Un driver de disco RAM
- Un FS Server pero como libraria (lo mas seguro seria FAT16) http://elm-chan.org/fsw/ff/00index_e.html
- Un protocolo TCP/IP (https://en.wikipedia.org/wiki/LwIP, http://savannah.nongnu.org/projects/lwip/)
- Driver de red (http://lwip.wikia.com/wiki/Available_device_drivers)
VER VIRTUAL SQUARE
https://github.com/tass-belgium/picotcp/wiki/Setting-up-the-environment#building-picotcp
An example of application with embedded stack is the telnet and web server of a vde switch (vdetelweb).
We have created public VDE networks where users
Se pueden usar threads
http://dunkels.com/adam/pt/about.html
http://www.drdobbs.com/embedded-systems/new-threads/240164996
https://www.gnu.org/software/libmicrohttpd/
https://github.com/etr/libhttpserver
http://www.hughes.com.au/products/libhttpd/
https://code.google.com/p/tinythread/
http://code.google.com/p/utrix/source/browse/#svn%2Ftrunk threads minix
http://code.google.com/p/utrix/source/browse/trunk/
https://github.com/ManojkumarMuralidharan/User-level-Thread-library


--------------------------------------------------------------------------------------------------------------		

COMO RELACIONAR VM CON CGROUPS

	
NETWORKING:
	Parece que la mejor opcion es el INET nativo de MINIX
	Habrá que hacer un device driver tipo proxy que interprete los mensajes de INET y los envie via TAP
	Luego un virtual switch lo conectará a la eth0
		INET0(tap0)---VSWITCH---(eth0)
	
	Quizas para probar se pueda hacer un Switch virtual sencillo entre 2 VMs del mismo host 
	Un proceso veth0 está bindeado a la VM0 
	Un proceso veth1 está bindeado a la VM1
	Ambos se comunican por un "cable" que son 2 message queues
		
OTRA POSIBILIDAD:
	Usar siempre una Virtual Ethernet Interface que basicamente utiliza SPREAD para difundir los paquetes 
	El nombre del grupo SPREAD significa el nombre del DISTRIBUTED VIRTUAL SWITCH al que está conectado.
	
-------------------------------------------------------------------------------------------------------------			
TODO: systask/do_bindproc: problema con REPLICA_BIND y BKUP_BIND
		La funcion solo permite 3 parametros y el nombre del programa			
			- OPERACION: SELF_BIND, LCL_BIND, RMT_BIND, BKUP_BIND, REPLICA_BIND
			- pid/nodeid
			- endpoint
		REPLICA_BIND y BKUP_BIND necesitan 3 parametros: PID, nodeid, endpoint

TODO:
		SE PUEDE HACER PM REPLICADO
		CADA PMx SE HACE RESPONSABLE DE LOS PROCESOS LOCALES 
		DONDE UN PM LE PIDE AL OTRO PM QUE HAGA COSAS POR EL 
				
TODO:	Modificar UDP proxy para que admita copiar datos particionando la transferencia.
		si size > 32768
			primer transfer 32768 a dst_addr = XXXXX
			siguient transfer size - 32768 a dst_addr = XXXXX+32768
			
TODO: Hacer que se inhiba de enviar SYS_REQ_SLOTS si se está en medio de una transaccion.
			Mensajes que inhiben: JOIN, DISCONNECT, NET PARTITION, SYS_REQ_SLOTS
			Mensajes que liberan: SYS_PUT_PST, NET MERGE
			Broadcasts que liberan:  SYS_DON_SLOTS 
			ATENCION: Si ver que fallos pueden provocar que la inhibicion de SYS_REQ_SLOTS quede permanentemente

TODO: 	MCS locks Y Qspinlocks 												
		https://lwn.net/Articles/590243/
		RESPUESTA: No vale la pena

TODO:	Se pueden usar COMPLETIONS en lugar de WAITQUEUES					
http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dai0425/ch04s07s03.html		
		RESPUESTA: No vale la pena, el completions tiene waitqueue adentro

PROBLEMON!!: que pasa con un proceso backup, no tiene necesariamente el mismo endpoint pero si tiene el mismo p_nr.
			Esto implica cambiar todas las IPC para que internamente consideren el p_nr y no el enpoint.
			
TODO:	Ver si es necesario si un proceso servidor es arrancado por demonize asignarle al INIT como padre.

TODO:	init:	Para que arranque un ejecutable USERx con EXEC (ver demonize)
		
TODO:   Hacer un nuevo system call que permita cambiar en nombre y eventualmente algun otro atributo del proceso
			mnx_setproc()
		
PROBLEMA:	Que hacer cuando un proceso servidor LOCAL recibe una petición de un proceso CLIENTE remoto.
			- Automatic binding? Controlar contra las propiedades de SLOTS
	RESPUESTA:
			Se supone que el proceso remoto debió FORKearse via el PM LOCAL, con lo que debería estar registrado


			
			
TODO: Cambio abrupto!!! se cambia la forma de calcular los endpoints!!
		Crear nuevo tipo de datos
			#include <stdint.h>
			typedef uint32_t endpoint_t;
		Revisar tambien los bitmaps de 32 y poner este tipo de datos
		
		endpoint = 
			<2bit special>: 00 NONE
							01 NORMAL
							10 SELF
							11 ANY
			<6bit node>	:   	=> 64 nodos
			<8bit generation>: 	=> 256 Generaciones
			<16bit p_nr>		=> 65536 procesos
							
		De esa forma automaticamente un proceso que se encuentra en la misma ranura en nodos diferentes
		tiene un endpoint diferente.		

	
QUEDAN LOS SIGUIENTES PROBLEMAS A RESOLVER
	- Como cargo en el kernel los nodos donde existen replicas de un endpoint REPLICATED?
		- deberia crearse una nueva primitiva mnx_replnode(endpoint, node);
		RESPUESTA: aparentemente no es necesario saber en que nodos están las replicas (por ahora).
		
	- Como sabe un nuevo nodo, en que otros nodos de la VM existen replicas de su endpoint?? 
		- se debe incluir en la estructura slot_t un campo con el bitmap de nodos replicados
		RESPUESTA: aparentemente no es necesario saber en que nodos están las replicas (por ahora).
	
	
TODO:
=====	
Soporte MULTICAST:	
	Se soporta multicast de un grupo de endpoints replicados.
	Conformacion del nombre de grupo. "M3.<VMID>.<ENDPOINT>.<NODEID>"
	
	las primitivas son:
	ENVIO:
			mnx_MC_send(mc_type mctype, message *mptr, int len);
				mctype: es alguno de los tipos válidos de multicast segun SPREAD.
					UNRELIABLE_MESS
					RELIABLE_MESS
					FIFO_MESS
					CAUSAL_MESS
					AGREED_MESS
					SAFE_MESS
				mptr: puntero a un mensaje
				Si a continuacion del mensaje sigue un buffer con datos, entonces
				len: longitud de todo el conjunto (mensaje+datos) 
				
	RECEPCION:	
			len = mnx_MC_receive(int infotype, buffer *ptr);
			infotype puede ser:
				CMD_SEND_MSG: quiere decir que el buffer contiene SOLO un mensaje.
				CMD_COPYIN_DATA: quiere decir que el buffer contiene un mensaje mas datos
			En el m_source se encuentra el NODO origen del envio

	p_nodemap: bitmap de nodos donde el endpoint se encuentra replicado 
	bit MIS_BIT_REPLICATED, indica que el proceso esta replicado en otros nodos
	
	
	TODO: Probar sys_migr_proc()
	TODO: Proponer mecanismo de funcionamiento backup (USAR MOL-RDISK)
		- Thread0: Un thread principal crea un thread hijo Thread1 que maneja SPREAD
		- Thread0: hace mnx_bkupbind(vmid,pid,endpoint,nodeid);
		- Thread0: lock(backup);
		- Thread0: hace sys_migrproc(getpid(), endpoint);
		- Thread0: while(TRUE) de M3-IPC
		- Thread1: hace el while(TRUE) de SPREAD
		- Thread1: si detecta que es el nuevo primary_mbr hace unlock(backup);
		
	TODO: Se deberia hacer un sys_exec_proc() simplemente para cambiarle el nombre al proceso remoto (solo tareas del sistema)
			TODO: Requiere crear una nueva llamada al mnx_chg_name(object_type, id, name);

		
TODO:	SE DEBEN PENSAR EN ESTRATEGIAS DE TOLERANCIA A FALLOS PARA CUANDO SYSTASK ESTA EN FUNCIONAMIENTO.

ERROR: 		EL drvs_end() se cuelga cuando realmente esta arrancado el DRVS

PROBLEMA:
		Como convertir un nombre de host (nodeX) en una direccion IP dentro del módulo ???
		SOLUCION
			Hacer un driver de caracter por el cual se pasará como dato la direccion IP del nodo destino
			utilizando IOCTL

FS: Trabaja con buffers no alineados, esto hace que se tengan doble cantidad de copias
	PONER BUFFERS tanto de MEMORY como de FS alineados
[ 6365.445748] DEBUG FS:mol_vcopy:923: src_ep=1 dst_ep=5 bytes=4096
[ 6365.445750] DEBUG FS:check_lock_caller:77: caller_pid=FS caller_tgid=FS
[ 6365.445753] DEBUG FS:check_lock_caller:111: WLOCK_PROC ep=1 count=0
[ 6365.445755] DEBUG FS:check_lock_caller:132: caller_pid=FS 
[ 6365.445757] DEBUG FS:mol_vcopy:950: vmid=0
[ 6365.445759] DEBUG FS:mol_vcopy:953: RLOCK_VM vm=0 count=0
[ 6365.445761] DEBUG FS:mol_vcopy:955: RUNLOCK_VM vm=0 count=0
[ 6365.445763] DEBUG FS:mol_vcopy:978: LOCK PROCESSES IN ASCENDENT ORDER
[ 6365.445765] DEBUG FS:mol_vcopy:979: WUNLOCK_PROC ep=1 count=0
[ 6365.445767] DEBUG FS:mol_vcopy:984: WLOCK_PROC ep=1 count=0
[ 6365.445770] DEBUG FS:mol_vcopy:984: WLOCK_PROC ep=5 count=0
[ 6365.445772] DEBUG FS:mol_vcopy:990: CHECK FOR SOURCE/DESTINATION STATUS
[ 6365.445774] DEBUG FS:mol_vcopy:1065: CALLER vmid=0 caller_pid=FS caller_nr=1 caller_ep=1 
[ 6365.445777] DEBUG FS:mol_vcopy:1071: CALLER p_endpoint=1 
[ 6365.445779] DEBUG FS:mol_vcopy:1072: SOURCE p_endpoint=1 
[ 6365.445781] DEBUG FS:mol_vcopy:1073: DESTIN p_endpoint=5 
[ 6365.445783] DEBUG FS:mol_vcopy:1074: BYTES  bytes	=4096 
[ 6365.445785] DEBUG FS:mol_vcopy:1088: COPY_USR2USR_PROC copylen=4096
[ 6365.445788] DEBUG FS:copy_usr2usr:275: source=27342 src_pid=FS dst_pid=CLIENT bytes=4096
[ 6365.445790] DEBUG FS:copy_usr2usr:281: src_off=656 dst_off=0
[ 6365.445793] DEBUG FS:copy_usr2usr:286: src_npag=2 dst_npag=1 <<<<<<<<  


MODIFICACION!:
			Se deberia independizar el tema del BIND de los procesos servidores (NR_SYS_PROCS)
			Este tema solo deberia ser gestionado por SYSTASK.
			Cuando un proceso hace un BIND, como paràmetro va el "endpoint" y el kernel de M3-IPC mediante P_ENDPOINT(ep) obtiene el numero de ranura.
			Por lo tanto la renovacion del ENDPOINT de un proceso no debe ser responsabilidad de M3-IPC (actualizando su época)
			sino que es una responsabilidad de la SYSTASK.

MODIFICACION:
			Eliminar del RPROXY el AUTO REMOTE BIND.

PROBLEMA:	Como hacer para que un proceso REMOTO pueda comunicarse con un servidor LOCAL 
		sin que la SYSTASK REMOTA le avise a la SYSTASK LOCAL de cada fork()/exit()?

Requerimiento de la solucion:
			Este es claramente un problema de MINIX distribuido, por lo tanto no deberia involucrar a M3-IPC
			Las SYSTASK solo deberian intercambiar información respecto a SERVIDORES mediante SPREAD.	

SOLUCION 1:
		El RPROXY cuando el RPROXY recibe un mensaje de un proceso cliente REMOTO {nodeid, vmid, endpoint}
		y trata de hacer un put2lcl() de un proceso remoto que no está bindeado, recibe un EMOLUNBIND.
 
			
Posible solucion 2:
		Cuando el RPROXY recibe un mensaje de un proceso cliente REMOTO {nodeid,vmid, endpoint}
		y trata de hacer un put2lcl() de un proceso remoto que no está bindeado, recibe un EMOLUNBIND
		Entonces el RPROXY deberia FABRICAR un mensaje NOTIFY proveniente de la SYSTASK remota 
		(recordar que cada SYSTASK tiene su propio endpoint) el cual le indica que haga el  
			
		el put2lcl() hace un equivalente a un 
		sendrec() de un mensaje 
				m_source = PROXY (nuevo tipo, como ANY o NONE o SELF )
				m_type = RMTBIND
				m1_i1 = nodeid
				m1_i2 = vmid
				m1_i3 = endpoint
		El endoint del destinatario (en este caso SYSTASK) debería setearse en 
		cada PROXY struct proxies_s de proxy.h
		Si la SYSTASK no existe => retorna EMOLNOTBIND (porque el endpoint no esta registrado)
		La SYSTASK intenta hacer el rmtbind() del proceso remoto:
			- Si la SYSTASK lo rechaza por inconsistencia 
				- retornar error de SYSTASK
			- Si la SYSTASK detecta que ese slot tenia otro endpoint del mismo nodo
				- rmtunbind(old_ep)
				- lclbind(new_ep)
				- retorna OK
			- Si la SYSTASK tiene el slot libre hace
				- lclbind(new_ep)
				- retorna OK
		El put2lcl ahora sabe que el proceso esta bindeado y puede continuar.


TODO:	Quizás seria conveniente definir un tipo de endpoint REPLICA_BIND donde indicamos que ese endpoint es local
		pero que puede estar replicado.
		Luego, deberia haber un  API que permita convertir este tipo de BIND en un REMOTO para cuando deja de funcionar el local.
	
		SOLUCION:
			NO DEBERIA SER NECESARIO. Este es claramente un problema a nivel de aplicacion de M3-IPC ( en este caso SYSTASK)
			Si hay un servidor replicado (por ejemplo FS en cada uno de los nodos) las aplicaciones locales deben usar esa REPLICA
			del FS. 
			Cada SYSTASK puede informar al resto de las SYSTASK que tiene una replica de ese servidor mediante mensaje SPREAD.
			
TODO
M3=IPC.
= TERMINAR PAPER SLOTS
- TERMINAR FS Y DISK
- ARMAR UN ESQUEMA
      CLIENT ==== FILESERVER ===== DISK
= TERMINAR SYSTASKS COORDINADAS
= VER COMO INTEGRAR A CGROUPS

PROBLEMA:	Quedan vivos los procesos INIT, quedan huerfanos del init de LINUX


TODO:		Como hacer para que dos threads hijos no puedan bindearse en distintas VMs si el padre no esta
			bindeado.
				
			POSIBLE SOLUCION:
				Analizar consecuencias
				Si un thread se bindea y el padre no esta bindeado, copiar en el p_ptr el descriptor del hijo
				de esa forma los futuros hijos ven que el padre esta bindeado
				Para distinguir entre padre bindeado y padre afectado por bindeado hay que controlar
				
						task_ptr->p_ptr->p_usr->p_lpid con task_ptr->pid
				Si son distintos estan bindeados.
			OTRA POSIBLE SOLUCION:
				Si el padre no esta bindeado, asumir que el primer thread bindea al padre, no a si mismo
				y por lo tanto sera como que es parte del padre.
				


ERROR: 		Algunas veces al hacer cat /procs/drvs/VM0/procs SE CUELGA!
		
TODO:	Deshabilitar del rproxy el REMOTE BIND y REMOTE UNBIND
	
ATENCION: Esto tambien se puede necesitar en do_proxies_unbind()

PROBLEMA:	Hay tantos PMs como nodos ????
				RESPUESTA: NO NECESARIAMENTE : Prueden armarse esquemas
					* 1 PM POR VM
					* 1 PM POR VM Y MULTIPLES REPLICAS BACKUP
					* N PM POR VM administrando cada uno un set de procesos LOCALES
					* N PM por VM conformando un solo unico PM virtual
				Cada PM atiende a sus procesos locales?
					RESPUESTA: NO NECESARIAMENTE - depende el equema elegido

PROBLEMA:	Que pasa con un proceso de usuario en NODE1, denomindado USER1 que ya se registro en PM1 y SYSTASK1 Y KERNEL1
			Ahora quiere comunicarse con FS0 de NODE0
	POSIBLE SOLUCION:
			En el descriptor de proceso crear un nuevo bitmap 
unsigned long p_bindmap;		/* bitmap of a LOCAL VIEW (not complete) of nodes where this process is binded as LOCAL or REMOTE */
			Cuando el USER1 intenta enviar un mensaje a FS1 (ya bindeado correctamente en el nodo local), 
			el kernel y comparar con el bindmap ve que no esta bindeado en el nodo remoto.
			OPCION 1:
					SPROXY.NODE1: 
						* hace get2rmt() este se fija en el bindmap y utilizando el campo de cmd_t denominado rcode con alias cmd_flags setea el flag FLAG_RMTBIND. 
						* SPROXY1 envia el mensaje original con el CMD_FLASGS seado FLAG_RMTBIND
					RPROXY.NODE0: 
						* analiza el campo cmd_flags y ve que esta prendida FLAG_RMTBIND
						* RPROXY0 construye y envia un falso mensaje RMTBIND con origen SYSTASK1 a destino SYSTASK0
						* Luego envía el mensaje original de USER1 a FS1.
					SYSTASK0:
						* Hace el RMTBIND del proceso en el kernel y en sus propias tablas con get_procinfo()
						
			OPCION 2:	La SYSTASK LOCAL es informada con algun tipo de notificacion de que el proceso USER1 no esta bindeado en NODO0
						Esta envia:
							- opcion 1: un broadcast a todos los nodos via spread
							- opcion 2: un mensaje directo via M3IPC de RMTBIND solo al nodo destino.
			
			OPCION 3:	Idem OPCION 1 pero en lugar de truchar un mensaje de la SYSTASK1 -> SYSTASK0
						El RPROXY0 fabrica un mensaje RMT_BIND desde PM1 -> SYSTASK0
						PROBLEMA: La respuesta enviada de SYSTASK0->PM1 , PM1 dice ????????
						SOLUCION: IGNORARLA
						SOLUCION: SYSTASK no responde a otros PMs.
						
									
PROBLEMA:	
	Que pasa si el NUEVO miembro ya recibio la VM_INFO, la TABLE pero aun no recibio ningun slot ???
	Y SI AHORA ES EL NUEVO FIRST_MBR?
		
PROBLEMA A CONSIDERAR:	En el nuevo miembro
						1) msg JOIN:  			STS_INIT->STS_WAIT_VM_INFO
						2) msg SYS_PUT_VM_INFO: STS_WAIT_VM_INFO -> STS_WAIT_TABLE
						3) msg SYS_PUT_TABLE:	STS_WAIT_TABLE->STS_REQ_SLOTS
	El problema que se puede dar es que entre el JOIN y el SYS_PUT_TABLE el first_mbr MUERA.
	Si el first_mbr muere, entonces el nuevo first member deberá hacer el broadcast de VM_INFO y TABLE.
	Debería haber un timeout si algo de esto falla.
	Si no falla, cuanto termina de inicializarse, (initialized == TRUE) entonces hacer un broadcast para hacer un SYS_READY
	Todos los demas procesos ahora hacen un rmt_bind de la SYSTASK remota nueva.

TODO:		Se deberia deshabilitar el algoritmo de NAGLE del proxy TCP de tal forma de no demorar el envio de paquetes pequeños!!!!.
			7.2 getsockopt and setsockopt Functions 
			7.9 TCP Socket Options 
			Ver: http://www.ibase.ru/devinfo/tcp_nodelay.txt
			If set, this option disables TCP's Nagle algorithm (Section 19.4 of TCPv1 and pp. 858–859 of TCPv2). By default, this algorithm is enabled
			int flag = 1;
int result = setsockopt(sock,            /* socket affected */
                        IPPROTO_TCP,     /* set option at TCP level */
                        TCP_NODELAY,     /* name of option */
                        (char *) &flag,  /* the cast is historical cruft */
                        sizeof(int));    /* length of option value */
 if (result < 0)
    ... handle the error ..
			Something a bit cleaner would be to activate the low latency mode of TCP:
			!!!!!!!!!!!! echo 1 > /proc/sys/net/ipv4/tcp_low_latency !!!!!!!!!!!!!!!!!!!
			This will give a hint to the TCP stack as to which decisions to make latency lower 
			(I guess it's what you are trying to achieve by disabling Nagle's algorithm). 
			By default, it is set to optimize bandwidth ( "0" will be read from /proc/sys/net/ipv4/tcp_low_latency ).

TODO:	Se puede hacer un proxy con RPC !!!!
			
TODO:	Hacer loops
		- todos contra uno
		- De a pares
		- Preparar DVD con SPINLOCKS
		- Preparar DVD con RWLOCKS
		
		
TODO:	Flexibilizar los PROXIES.
			1) Que soporten threads 
				- Cambiar proxy_bind()

TODO:	Hacer todos los tests con SPINLOCKS
		LOCAL
			- loop_mol_ipc.sh 
			- loop_sr_peer.c
			- IPC udp
			- IPC tcp 
				
			
	p_lpid	p_nodeid	p_rts_flags	p_mis_flags
LOCAL	PID	local_nodeid	---		---
REMOTE	NO_PID	nodeid		REMOTE		---
BACKUP	PID	nodeid		REMOTE		BACKUP
PROXY	PID	(-1)		---		PROXY


TODO:
			KILL de task Linux:
				Si es PROXY
					do_proxies_unbind
						por cada nodo que representa
							do_node_end
								por cada proceso REMOTO de ese nodo
									do_unbind
										si es BACKUP termina 
										Por cada proceso que le quiere enviar mensaje
											si es LOCAL 
												retorna error
											si es REMOTO	 
												send_ack
								desconetar nodo de la VM
								desconetar nodo del proxy
						enviar signal al otro proxy
				Si no es proxy
					do_unbind
						si es BACKUP termina 
						Por cada proceso que le quiere enviar mensaje
							si es LOCAL 
								retorna error
							si es REMOTO	 
								send_ack

			END NODE:
				do_node_end
					por cada proceso REMOTO de ese nodo
						do_unbind
							si es BACKUP termina 
							Por cada proceso que le quiere enviar mensaje
								si es LOCAL 
									retorna error
								si es REMOTO	 
									send_ack
					desconetar nodo de la VM
					desconetar nodo del proxy
			
			PROXY PAIR END:
				do_proxies_unbind
					por cada nodo que representa
						do_node_end
							por cada proceso REMOTO de ese nodo
								do_unbind
									si es BACKUP termina 
									Por cada proceso que le quiere enviar mensaje
										si es LOCAL 
											retorna error
										si es REMOTO	 
											send_ack
								desconetar nodo de la VM
								desconetar nodo del proxy
			
			VM END:		
				Por cada proceso 
					do_unbind
				Por cada nodo
					desconectar
					


					
				
ESCENARIO DE MIGRACION:
======================
	1) El administrador de la VM ordena la migración de un proceso a su SYSTASK
			migrate(vmid, endpoint, old_node, new_node);
	2) La SYSTASK envia por MULTICAST un comando MIGRATE al resto de los nodos
	3) Todas las SYSTASK ejecutan
			mnx_migrate(MIGR_START, pid, endpoint, dst_node)
	4) La SYSTASK del OLD NODE 
			- Si el proceso esta RUNNING 	=> Marca el p_misc_flags NEED_MIGRATE
			- si el proceso esta bloqueado	=> setear p_rts_flags BIT_MIGRATE		

TODO:	Incluir algun comando que contemple la migración de un endpoint de un nodo a otro.
			Una SYSTASK envia un broadcast con {vm, endpoint, nodo_destino}.
			Todas las SYSTASKs (incluyendo origen y destino) ejecutan un comando MIGRATE(MIGR_START, endpoint, src_node, dst_node);
				SYSTASK origen: cambia el estado del proceso a (MIGRATING | REMOTE) y el p_nodoid
				SYSTASK destino: Hace el equivalente a un BIND del proceso pero cambia el estado del proceso a MIGRATING, removiendo REMOTE y cambia el p_nodeid
				OTRAS SYSTASKs: cambia el estado del proceso a MIGRATING y el p_nodoid
			La SYSTASK DESTINO: Cuando finaliza la migración hace un comando 
				entonces TODOS los NODOS cambian suprimen la bandera MIGRATE
			(el vmid se obtiene del descriptor del proceso invocante caller_ptr)

	
		ATENCION: Se puede reducir el número de los parámetros 
				mol_migrate(oper, pid, endpoint, nodeid)
			MIGRATE_START: 	nodeid = oldnode
				       	pid = (local)?p_lpid:PROC_NO_PID;
			MIGRATE_END:   	nodeid = newnode
				       	pid = (local)?p_lpid:PROC_NO_PID;
			MIGRATE_FAIL:  	nodeid = oldnode	
			pid = (local)?p_lpid:PROC_NO_PID;

ATENCION NOTIFY: Un proceso local hace un NOTIFY a otro proceso local que luego MIGRAGRA
			Que hacer con los bits de notificación??
		 Un proceso remoto hace un NOTIFY a un proceso local que luego MIGRARA
			Que hacer con los bits de notificación??
Z

/* PROXIES */
SI CAMBIO EL NODO DESTINO DE UN COMANDO, ENTONCES RETORNAR ERROR (EMOLBADNODEID) AL PROCESO EMISOR 
DE ESA FORMA EL PROCESO EMISOR PUEDE VERIFICAR

if( if ret= (EMOLBADNODEID)  && cmd->c_dnode != node[ENDPOINT_P(dst->p_usr.p_endpoint).n_nodeid )
	/* SIGNIFICA QUE HUBO UNA MIGRACION  hay que REEJECUTAR */



 		ATECION: Que hacer con los PROXIES que tienen encolados mensajes para el proceso migrante previo a MIGRATE_START?
			OLD:REMOTE, NEW:REMOTE:
				El proxy SENDER del NODO OLD lee el destination node del comando, desencola de su cola OLD 
				y encola en la del PROXY SENDER del NODO NEW.

			OLD:REMOTE, NEW:LOCAL:
				El proxy SENDER del NODO OLD lee el destination node del comando, desencola de su cola OLD 
				y encola en la de recepción del proceso MIGRADO.


			OLD:LOCAL, NEW:REMOTE:
				El mol_migrate desencola de la cola de recepción del proceso migrado y se encola 
				y encola en la del PROXY SENDER del NODO NEW.
			
		ATENCION: 	
				CON BIT_MIGRATE:
					Si el proceso receptor es migrante, el PROXY RECEIVER encola en p_mlist.
				SI EL PROCESO NO EXISTE MAS EN ESE NODO:
					Solución 1- RETORNA ERROR
					Socución 2- Encola en el PROXY SENDER del nuevo nodo del MIGRANTE.
 
		
		ATENCION:	Cuando un proceso LOCAL trata de enviarle un mensaje a un proceso DURANTE la MIGRACION, 
					es decir con la bandera MIGRATE seteada.
					El descriptor del emisor deberia encolarse en el descriptor del MIGRANTE y setear su flag WAITMIGR.
					MIGRANTE es LOCAL pero sera REMOTO: 
						Si la migración termino OK: En el NODO local deben procesarse todos los mensajes 
							,encolar los descriptores emisores en el PROXY SENDER del nuevo NODO del MIGRANTE 
							y borrar la bandera WAITMIGR de cada emisor.
						Si la migracion FALLO: borrar la bandera WAITMIGR. 
					MIGRANTE es REMOTO pero en otro NODO: 
						Si la migración termino OK: Deben desencolarse los descriptores de los emisores
							,encolar los descriptores en el PROXY SENDER del nuevo NODO del MIGRANTE 
							y borrar la bandera WAITMIGR de cada emisor.
						Si la migración FALLO: borrar la bandera WAITMIGR.
					MIGRANTE es REMOTO pero sera LOCAL: 
						Si la migración termino OK: En el NODO local se procesaran todos los mensajes de la misma forma
							que como si no hubiese estado esperando el mensaje y borrar la bandera WAITMIGR de cada emisor.	
		
		ATENCION:	Que pasa con los NOTIFY enviados al proceso MIGRANTE durante la migracion?
					Cuando se hace un NOTIFY de a un proceso MIGRANTE, se debe:
						* setear el bit notify correspondiente al emisor
						* encolar el emisor en el lista de espera 
						* Setear el BIT_WAITMIGR el descriptor del emisor.
					MIGRANTE es LOCAL pero sera REMOTO: 
						Si la migración termino OK: En el NODO local deben procesarse todos bits 
							y encolar en el PROXY SENDER del nuevo NODO del MIGRANTE

						

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	T O D O
	Que un proceso sea REMOTO implica redundacia de info
		flat BIT_REMOTE
		pid = -1
		node = not(local_nodeid)
		proc_ptr->p_taks == NULL

	Quizas para hacer BACKUP se podria poner
		flag BIT_REMOTE
		pid = (pid del backup local)
		node = not(local_nodeid)
		proc_ptr->p_taks = (task del pid local)

	Esto se haria como BACKUP BIND.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	
		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	T O D O
+ Resolver SIGNALS (problema con division x 0 )
+ Resolver EXEC (archivo de ejecucion desde otro FS)
+ Incluir en Containers
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


TODO:			Crear un FS VACIO para hacer los binding correspondientes.

				
TODO:			CONVERTIR APIs en LIBRERIAS
			test de transferencia entre 2 nodos con 2 o mas VMs
		
	

Que sucede si:
        El RPROXY recibe un mensaje CMD_SEND_MSG
		Despierta al proceso local que hace el RECEIVE
		El proceso local envia un CMD_SEND_MSG al proceso remoto (encola en el SPROXY)
		El RPROXY encola el CMD_SEND_ACK en el SPROXY
		ERRROR: La colas del SPROXY queda entonces      1) CMD_SEND_MSG
														2) CMD_SEND_ACK
														
TODO:   PROXY_UNBIND
		Si un proceso hizo un SEND por ejemplo y quedo esperando un SENDACK del proceso remoto pero el NODO CAE,
				el emisor queda colgado. (LISTO)
		Hacer lo mismo para el requester que esta esperando alguna respuesta SOURCE/DESTINATION

TODO: 	Desarrollar tests de loops remotos e instructivos.

TODO:   POWERPOINT:  Falta definir RPROXY funciones de Acks 
		Definir maquina de estados de PROXIES	
		Puede suceder que un proceso espere recibir un mensaje o comando específico pero le llega otro? 


TODO:	Cuando el sender trata de enviar algo al dest pero hay un problema, el descriptor queda bloqueado- Desbloquearlo
		
TODO:   Proxies:
		* controlar todo tipo de error
		* signal para guardar info estadistica (x tipo de header, tamaño paquete, x VM, bandwith)
		* Leer parametros de archivo de configuracion.
											
cambiar en kernel NBR2PTR por PROC2PTR

	

TODO:
	UNBIND de procesos en estado ONCOPY  (no deberia terminarse hasta finalizar la copia y en la salida terminar)
	
	UNBIND de proceso LOCAL:
		Si es el propio proceso: hacer unbind.
		Si p_rts_flags == 0: hacer unbind
		Si esta esperando algo: retornar error y despertar para que haga UNBIND !!!!!
		Si otro proceso esta esperando algo 
				DISURIA
				

2- BIFES
3- CRIOLLITAS/TRAVIATAS
1- CAJA DE TE
1- CAJA MATE COCIDO
1- QUESO NEGRA
1- DULCE BATATA
1- DESODORANTE AMBIENTE






 


		